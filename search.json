[
  {
    "objectID": "05transformers.html",
    "href": "05transformers.html",
    "title": "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
    "section": "",
    "text": "æ–‡ç« ã®åˆ†é¡ï¼šãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è©•ä¾¡ã€ã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«ã®æ¤œå‡ºã€æ–‡æ³•çš„ã«æ­£ã—ã„ã‹ã©ã†ã‹ã®åˆ¤æ–­ã€2ã¤ã®æ–‡ãŒè«–ç†çš„ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã®åˆ¤æ–­\næ–‡ã®ä¸­ã®å˜èªåˆ†é¡ï¼šå“è©ï¼ˆåè©ã€å‹•è©ã€å½¢å®¹è©ï¼‰ã‚„ã€å›ºæœ‰è¡¨ç¾ï¼ˆäººã€å ´æ‰€ã€çµ„ç¹”ï¼‰ã®è­˜åˆ¥\næ–‡ç« å†…å®¹ã®ç”Ÿæˆï¼šè‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«ã‚ˆã‚‹å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®è£œå®Œã€æ–‡ç« ã®ç©´åŸ‹ã‚\næ–‡ç« ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºï¼šè³ªå•ã¨æ–‡è„ˆãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®ã€æ–‡è„ˆã‹ã‚‰ã®æƒ…å ±ã«åŸºã¥ã„ãŸè³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆã®æŠ½å‡º\næ–‡ç« ã®å¤‰æ›ï¼šã‚ã‚‹æ–‡ç« ã®ä»–ã®è¨€èªã¸ã®ç¿»è¨³ã€æ–‡ç« ã®è¦ç´„\n\nHugging Face https://huggingface.co/ ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã£ã¦è‰²ã€…ãªNLPã®å‡¦ç†ãŒã§ãã‚‹ã€‚\n\nsentiment-analysis (æ„Ÿæƒ…åˆ†æ)\nzero-shot-classification (ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡)\ntext-generation (æ–‡ç« ç”Ÿæˆ)\nfill-mask (ç©ºæ‰€ç©´åŸ‹ã‚)\nner (named entity recognition) (å›ºæœ‰è¡¨ç¾èªè­˜)\nquestion-answering (è³ªå•å¿œç­”)\nsummarization (è¦ç´„)\ntranslation (ç¿»è¨³)\n\nåŸºæœ¬çš„ãªä½¿ã„æ–¹ã¯ç°¡å˜ã§ã‚ã‚Šã€pipelineã®taskå¼•æ•°ã«ã‚„ã‚ŠãŸã„ã“ã¨ã‚’è¡¨ã™ä¸Šã®æ–‡å­—åˆ—ã‚’å…¥ã‚Œã¦ã€ç”Ÿæˆã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«æ–‡å­—åˆ—ã‚’å…¥ã‚Œã‚‹ã ã‘ã§ã‚ã‚‹ã€‚\n\nfrom transformers import pipeline\n\n\n\nä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ãŒ POSITIVEã‹NEGATIVEã‹ã‚’è¿”ã™ã€‚\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n\n\n[{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n\n\n\n\n\nä¾‹ã‚’ç¤ºã™ã“ã¨ãªãã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‚’åˆ†é¡ã™ã‚‹ã€‚åˆ†é¡ã—ãŸã„ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ã€å¼•æ•° candidate_labelsã§ä¸ãˆã‚‹ã€‚\n\nclassifier2 = pipeline(\"zero-shot-classification\")\nclassifier2(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\nNo model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445950150489807, 0.11197729408740997, 0.0434277318418026]}\n\n\n\n\n\nä¸ãˆãŸæ–‡ç« ã®ç¶šãã‚’æ›¸ãã€‚\n\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\nNo model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n[{'generated_text': 'In this course, we will teach you how to run a database with Nginx and PHP. We first take a look at how to run PHP and Nginx together. Then we will use an example MySQL database to create a database. In the same'}]\n\n\npipelineã®ãƒ¢ãƒ‡ãƒ«å¼•æ•°modelã§ã€ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚ ãƒ¢ãƒ‡ãƒ«ã¯ã€https://huggingface.co/models ã‹ã‚‰é©å½“ãªã‚‚ã®ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\nã¾ãŸã€æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’max_lengthã€ç”Ÿæˆã™ã‚‹æ–‡ç« ã®æ•°ã‚’num_return_sequencesã§ä¸ãˆã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚\n\ngenerator = pipeline(\"text-generation\", model=\"distilgpt2\")\ngenerator(\n    \"In this course, we will teach you how to\",\n    max_length=30,\n    num_return_sequences=2,\n)\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n[{'generated_text': 'In this course, we will teach you how to make mistakes as well as avoid them all because they cost you money, and why it makes good money'},\n {'generated_text': 'In this course, we will teach you how to understand the best, most effective and most effective ways to perform the work of the American people. These'}]\n\n\n\n\n\nä¸ãˆãŸæ–‡ç« å†…ã®&lt;mask&gt;ã®éƒ¨åˆ†ã«å˜èªã§åŸ‹ã‚ã¦æ–‡ç« ã«ã™ã‚‹ã€‚å¼•æ•°top_kã§åŸ‹ã‚ã‚‹å˜èªæ•°ã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n\nunmasker = pipeline(\"fill-mask\")\n\nNo model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\n\n[{'score': 0.1961977630853653,\n  'token': 30412,\n  'token_str': ' mathematical',\n  'sequence': 'This course will teach you all about mathematical models.'},\n {'score': 0.04052729532122612,\n  'token': 38163,\n  'token_str': ' computational',\n  'sequence': 'This course will teach you all about computational models.'}]\n\n\n\n\n\nå›ºæœ‰è¡¨ç¾èªè­˜ ner (named entity recognition) ã¨ã¯ã€æ–‡ç« å†…ã® äºº(PER: persons)ã€å ´æ‰€ï¼ˆLOC: locations)ã€çµ„ç¹”(ORG: organizations)ãªã©ã‚’æŠ½å‡ºã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚\nå¼•æ•°grouped_entitiesã‚’Trueã«è¨­å®šã™ã‚‹ã¨å›ºæœ‰åè©ã‚’çµåˆã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚\n\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n\nNo model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n[{'entity_group': 'PER',\n  'score': 0.9981694,\n  'word': 'Sylvain',\n  'start': 11,\n  'end': 18},\n {'entity_group': 'ORG',\n  'score': 0.9796021,\n  'word': 'Hugging Face',\n  'start': 33,\n  'end': 45},\n {'entity_group': 'LOC',\n  'score': 0.9932106,\n  'word': 'Brooklyn',\n  'start': 49,\n  'end': 57}]\n\n\n\n\n\nè³ªå•ã‚’questionã€æ–‡ç« ã‚’contextã§ä¸ãˆã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€è³ªå•ã®ç­”ãˆã¨ã€ãã®å˜èªã®é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã‚’è¿”ã™ã€‚\n\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n\nNo model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'score': 0.6949763894081116, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n\n\n\n\n\næ–‡ç« ã®è¦ç´„ã‚’è¿”ã™ã€‚\n\nsummarizer = pipeline(\"summarization\")\nsummarizer(\n    \"\"\"\n    America has changed dramatically during recent years. Not only has the number of\n    graduates in traditional engineering disciplines such as mechanical, civil,\n    electrical, chemical, and aeronautical engineering declined, but in most of\n    the premier American universities engineering curricula now concentrate on\n    and encourage largely the study of engineering science. As a result, there\n    are declining offerings in engineering subjects dealing with infrastructure,\n    the environment, and related issues, and greater concentration on high\n    technology subjects, largely supporting increasingly complex scientific\n    developments. While the latter is important, it should not be at the expense\n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other\n    industrial countries in Europe and Asia, continue to encourage and advance\n    the teaching of engineering. Both China and India, respectively, graduate\n    six and eight times as many traditional engineers as does the United States.\n    Other industrial countries at minimum maintain their output, while America\n    suffers an increasingly serious decline in the number of engineering graduates\n    and a lack of well-educated engineers.\n\"\"\"\n)\n\nNo model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]\n\n\n\n\n\nç¿»è¨³ã—ãŸæ–‡ç« ã‚’è¿”ã™ã€‚ pipelineã®ãƒ¢ãƒ‡ãƒ«å¼•æ•°modelã«ç¿»è¨³ã‚’ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¥ã‚Œã‚‹ã€‚ ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€è‹±èªã‹ã‚‰ãƒ•ãƒ©ãƒ³ã‚¹èªã¸ã®ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ã„ã‚‹ã€‚ ï¼ˆãƒ‰ã‚¤ãƒ„èªã¸ã®ç¿»è¨³ã®å ´åˆã«ã¯ã€translation_en_to_de ã‚’taskå¼•æ•°ã¨ã™ã‚‹ã€‚ï¼‰\n\ntranslator = pipeline(\"translation_en_to_fr\")\ntranslator(\"This course is produced by Hugging Face.\")\n\nNo model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n[{'translation_text': 'Ce cours est produit par Hugging Face.'}]\n\n\nGoogle Colab.ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ç¿»è¨³ã‚’è¡Œã†å ´åˆã«ã¯ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦sentencepieceã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã‹ã‚‰ã€ ã‚«ãƒ¼ãƒãƒ«ã‚’ãƒªã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n !pip install sentencepiece\nä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯Helsinki-NLã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€æ§˜ã€…ãªè¨€èªé–“ã®ç¿»è¨³ã‚’è¡Œã†ã€‚ ä¾‹ã¨ã—ã¦ã€è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³ã‚’ç¤ºã™ã€‚\n\ndef create_translation_pipeline(source_lang, target_lang):\n    model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n    translator = pipeline(\"translation\", model=model_name)\n    return translator\n\ndef translate_text(translator, text):\n    result = translator(text, max_length=500)\n    return result[0]['translation_text']\n\n# Example usage:\nsource_lang_code = \"en\"  # English\ntarget_lang_code = \"jap\"  # Japanese\n\ntranslator = create_translation_pipeline(source_lang_code, target_lang_code)\n\nenglish_text = \"This is a pen.\"\ntranslated_text = translate_text(translator, english_text)\n\nprint(f\"{source_lang_code.capitalize()}: {english_text}\")\nprint(f\"{target_lang_code.capitalize()}: {translated_text}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn: This is a pen.\nJap: ã“ã‚Œ ã¯ ç­† ã§ ã‚ ã‚‹ .\n\n\n\n\n\npipelineã®ä¸­èº«ã¯ã€ä»¥ä¸‹ã®å‡¦ç†ã«åˆ†è§£ã•ã‚Œã‚‹ã€‚\næ–‡å­—åˆ— =&gt; ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ =&gt; ãƒ¢ãƒ‡ãƒ«ã€€=&gt; å¾Œå‡¦ç†\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom pprint import pprint\nfrom transformers import AutoModelForSequenceClassification\nimport torch\n\n\n\nã¾ãšã€å…¥åŠ›ã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªã‚„è¨˜å·ãªã©ï¼‰ã«åˆ†å‰²ã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•´æ•°ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ ã“ã‚Œã«ã¯ã€AutoTokenizer ã‚¯ãƒ©ã‚¹ã®from_pretrainedãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã€‚ å¼•æ•°ã«ã¯ã€https://huggingface.co/models ã«ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«å checkpointã‚’ä¸ãˆã‚‹ã€‚\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\npprint(tokenizer)\n\nDistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n\n\nç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒŠãƒ¼ã‚¶ãƒ¼tokenizerã«æ–‡å­—åˆ—ï¼ˆã®ãƒªã‚¹ãƒˆï¼‰ã‚’ä¸ãˆã‚‹ã¨ã€å¤‰æ›ã•ã‚ŒãŸæ•°å€¤æƒ…å ±ã‚’å«ã‚“ã è¾æ›¸ãŒç”Ÿæˆã•ã‚Œã‚‹ã€‚ è¾æ›¸ã®ã‚­ãƒ¼ã¯ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨æ„ã™ã‚‹ã‹ã‚’è¡¨ã™attention_maskã¨å…¥åŠ›ã‚’æ•°å€¤ã«å¤‰æ›ã—ãŸå¤šæ¬¡å…ƒé…åˆ—ã‚’è¡¨ã™ input_ids ã§ã‚ã‚‹ã€‚\nã“ã®éš›ã€ã©ã®æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†ã‹ã‚’è¡¨ã™return_tensorsã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ ã“ã“ã§ã¯ã€PyTorchã‚’ä½¿ã†ã®ã§ã€å¼•æ•°ã«ptã‚’æŒ‡å®šã™ã‚‹ã€‚\n\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\npprint(inputs)\n\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n             0,     0,     0,     0,     0,     0]])}\n\n\n\n\n\nç¶šã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã€‚ ã“ã“ã§ã¯ã€AutoModelã‚¯ãƒ©ã‚¹ã®from_pretrainedãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã€‚\nã“ã“ã§ç”Ÿæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åŸºæœ¬éƒ¨åˆ†ã ã‘ã‚’ã‚‚ã¡ã€å‡ºåŠ›ã¯å…¥åŠ›ã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ãŸå¤šæ¬¡å…ƒé…åˆ—ï¼ˆãƒ†ãƒ³ã‚½ãƒ«ï¼‰ã§ã‚ã‚‹ã€‚\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModel.from_pretrained(checkpoint)\n\nSome weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\npprint(model)\n\nDistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)\n\n\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ç”Ÿæˆã—ãŸè¾æ›¸ã‚’å±•é–‹ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹ã¨ã€PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ãŒå‡ºåŠ›ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ã€‚\n\noutputs = model(**inputs)\nprint(outputs.last_hidden_state.shape)\n\ntorch.Size([2, 16, 768])\n\n\n\noutputs\n\nBaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n         ...,\n         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n\n        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n         ...,\n         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), hidden_states=None, attentions=None)\n\n\nä»Šåº¦ã¯ã€å®Ÿéš›ã«æ„Ÿæƒ…åˆ†æã‚’è¡Œã†ãŸã‚ã®å±¤ã‚’å«ã‚“ã ãƒ¢ãƒ‡ãƒ«ã‚’ã€ AutoModelForSequenceClassificationã‚¯ãƒ©ã‚¹ã‚’ç”¨ã„ã¦ç”Ÿæˆã™ã‚‹ã€‚\nå‡ºåŠ›ã®logitsã«ä¿ç®¡ã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ãŒå¾—ã‚‰ã‚ŒãŸæ•°å€¤ã§ã‚ã‚‹ã€‚\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel2 = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs2 = model2(**inputs)\n\n\noutputs2\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n        [ 4.1692, -3.3464]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n\n\nå¾—ã‚‰ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯é–¢æ•°ã‚’ç”¨ã„ã¦ç¢ºç‡ã«å¤‰æ›ã™ã‚‹ã€‚ã“ã‚ŒãŒäºˆæ¸¬å€¤ã«ãªã‚‹ã€‚\n\npredictions = torch.nn.functional.softmax(outputs2.logits, dim=-1)\nprint(predictions)\n\ntensor([[4.0195e-02, 9.5981e-01],\n        [9.9946e-01, 5.4418e-04]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\næœ€åˆã®æ–‡ã®äºˆæ¸¬å€¤ã¯ [0.0402, 0.9598]ã€2ç•ªç›®ã®æ–‡ã®äºˆæ¸¬å€¤ã¯[0.9995, 0.0005]ã§ã‚ã‚‹ã€‚ ã“ã‚Œã¯æœ€åˆã®æ–‡ã¯ã€1ã§ã‚ã‚‹ç¢ºç‡ãŒé«˜ãã€2ç•ªç›®ã®æ–‡ã¯0ã§ã‚ã‚‹ç¢ºç‡ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚\nãƒ¢ãƒ‡ãƒ«ã§ç”¨ã„ã‚‰ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’å¾—ã‚‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®id2labelå±æ€§ã‚’ã¿ã‚‹ã€‚\n\nmodel2.config.id2label\n\n{0: 'NEGATIVE', 1: 'POSITIVE'}\n\n\nã—ãŸãŒã£ã¦ã€æœ€åˆã®æ–‡ç« ã¯POSITIVEã€2ç•ªç›®ã®æ–‡ç« ã¯NEGATIVEã§ã‚ã‚‹ã¨åˆ¤å®šã•ã‚Œã‚‹ã€‚",
    "crumbs": [
      "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
    ]
  },
  {
    "objectID": "05transformers.html#è‡ªç„¶è¨€èªå‡¦ç†-nlpnatural-language-processing",
    "href": "05transformers.html#è‡ªç„¶è¨€èªå‡¦ç†-nlpnatural-language-processing",
    "title": "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
    "section": "",
    "text": "æ–‡ç« ã®åˆ†é¡ï¼šãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è©•ä¾¡ã€ã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«ã®æ¤œå‡ºã€æ–‡æ³•çš„ã«æ­£ã—ã„ã‹ã©ã†ã‹ã®åˆ¤æ–­ã€2ã¤ã®æ–‡ãŒè«–ç†çš„ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã®åˆ¤æ–­\næ–‡ã®ä¸­ã®å˜èªåˆ†é¡ï¼šå“è©ï¼ˆåè©ã€å‹•è©ã€å½¢å®¹è©ï¼‰ã‚„ã€å›ºæœ‰è¡¨ç¾ï¼ˆäººã€å ´æ‰€ã€çµ„ç¹”ï¼‰ã®è­˜åˆ¥\næ–‡ç« å†…å®¹ã®ç”Ÿæˆï¼šè‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«ã‚ˆã‚‹å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®è£œå®Œã€æ–‡ç« ã®ç©´åŸ‹ã‚\næ–‡ç« ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºï¼šè³ªå•ã¨æ–‡è„ˆãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®ã€æ–‡è„ˆã‹ã‚‰ã®æƒ…å ±ã«åŸºã¥ã„ãŸè³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆã®æŠ½å‡º\næ–‡ç« ã®å¤‰æ›ï¼šã‚ã‚‹æ–‡ç« ã®ä»–ã®è¨€èªã¸ã®ç¿»è¨³ã€æ–‡ç« ã®è¦ç´„\n\nHugging Face https://huggingface.co/ ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã£ã¦è‰²ã€…ãªNLPã®å‡¦ç†ãŒã§ãã‚‹ã€‚\n\nsentiment-analysis (æ„Ÿæƒ…åˆ†æ)\nzero-shot-classification (ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡)\ntext-generation (æ–‡ç« ç”Ÿæˆ)\nfill-mask (ç©ºæ‰€ç©´åŸ‹ã‚)\nner (named entity recognition) (å›ºæœ‰è¡¨ç¾èªè­˜)\nquestion-answering (è³ªå•å¿œç­”)\nsummarization (è¦ç´„)\ntranslation (ç¿»è¨³)\n\nåŸºæœ¬çš„ãªä½¿ã„æ–¹ã¯ç°¡å˜ã§ã‚ã‚Šã€pipelineã®taskå¼•æ•°ã«ã‚„ã‚ŠãŸã„ã“ã¨ã‚’è¡¨ã™ä¸Šã®æ–‡å­—åˆ—ã‚’å…¥ã‚Œã¦ã€ç”Ÿæˆã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«æ–‡å­—åˆ—ã‚’å…¥ã‚Œã‚‹ã ã‘ã§ã‚ã‚‹ã€‚\n\nfrom transformers import pipeline\n\n\n\nä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ãŒ POSITIVEã‹NEGATIVEã‹ã‚’è¿”ã™ã€‚\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n\n\n[{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n\n\n\n\n\nä¾‹ã‚’ç¤ºã™ã“ã¨ãªãã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‚’åˆ†é¡ã™ã‚‹ã€‚åˆ†é¡ã—ãŸã„ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ã€å¼•æ•° candidate_labelsã§ä¸ãˆã‚‹ã€‚\n\nclassifier2 = pipeline(\"zero-shot-classification\")\nclassifier2(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\nNo model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445950150489807, 0.11197729408740997, 0.0434277318418026]}\n\n\n\n\n\nä¸ãˆãŸæ–‡ç« ã®ç¶šãã‚’æ›¸ãã€‚\n\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\nNo model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n[{'generated_text': 'In this course, we will teach you how to run a database with Nginx and PHP. We first take a look at how to run PHP and Nginx together. Then we will use an example MySQL database to create a database. In the same'}]\n\n\npipelineã®ãƒ¢ãƒ‡ãƒ«å¼•æ•°modelã§ã€ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚ ãƒ¢ãƒ‡ãƒ«ã¯ã€https://huggingface.co/models ã‹ã‚‰é©å½“ãªã‚‚ã®ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\nã¾ãŸã€æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’max_lengthã€ç”Ÿæˆã™ã‚‹æ–‡ç« ã®æ•°ã‚’num_return_sequencesã§ä¸ãˆã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚\n\ngenerator = pipeline(\"text-generation\", model=\"distilgpt2\")\ngenerator(\n    \"In this course, we will teach you how to\",\n    max_length=30,\n    num_return_sequences=2,\n)\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n[{'generated_text': 'In this course, we will teach you how to make mistakes as well as avoid them all because they cost you money, and why it makes good money'},\n {'generated_text': 'In this course, we will teach you how to understand the best, most effective and most effective ways to perform the work of the American people. These'}]\n\n\n\n\n\nä¸ãˆãŸæ–‡ç« å†…ã®&lt;mask&gt;ã®éƒ¨åˆ†ã«å˜èªã§åŸ‹ã‚ã¦æ–‡ç« ã«ã™ã‚‹ã€‚å¼•æ•°top_kã§åŸ‹ã‚ã‚‹å˜èªæ•°ã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n\nunmasker = pipeline(\"fill-mask\")\n\nNo model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\n\n[{'score': 0.1961977630853653,\n  'token': 30412,\n  'token_str': ' mathematical',\n  'sequence': 'This course will teach you all about mathematical models.'},\n {'score': 0.04052729532122612,\n  'token': 38163,\n  'token_str': ' computational',\n  'sequence': 'This course will teach you all about computational models.'}]\n\n\n\n\n\nå›ºæœ‰è¡¨ç¾èªè­˜ ner (named entity recognition) ã¨ã¯ã€æ–‡ç« å†…ã® äºº(PER: persons)ã€å ´æ‰€ï¼ˆLOC: locations)ã€çµ„ç¹”(ORG: organizations)ãªã©ã‚’æŠ½å‡ºã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚\nå¼•æ•°grouped_entitiesã‚’Trueã«è¨­å®šã™ã‚‹ã¨å›ºæœ‰åè©ã‚’çµåˆã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚\n\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n\nNo model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n[{'entity_group': 'PER',\n  'score': 0.9981694,\n  'word': 'Sylvain',\n  'start': 11,\n  'end': 18},\n {'entity_group': 'ORG',\n  'score': 0.9796021,\n  'word': 'Hugging Face',\n  'start': 33,\n  'end': 45},\n {'entity_group': 'LOC',\n  'score': 0.9932106,\n  'word': 'Brooklyn',\n  'start': 49,\n  'end': 57}]\n\n\n\n\n\nè³ªå•ã‚’questionã€æ–‡ç« ã‚’contextã§ä¸ãˆã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€è³ªå•ã®ç­”ãˆã¨ã€ãã®å˜èªã®é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã‚’è¿”ã™ã€‚\n\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n\nNo model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'score': 0.6949763894081116, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n\n\n\n\n\næ–‡ç« ã®è¦ç´„ã‚’è¿”ã™ã€‚\n\nsummarizer = pipeline(\"summarization\")\nsummarizer(\n    \"\"\"\n    America has changed dramatically during recent years. Not only has the number of\n    graduates in traditional engineering disciplines such as mechanical, civil,\n    electrical, chemical, and aeronautical engineering declined, but in most of\n    the premier American universities engineering curricula now concentrate on\n    and encourage largely the study of engineering science. As a result, there\n    are declining offerings in engineering subjects dealing with infrastructure,\n    the environment, and related issues, and greater concentration on high\n    technology subjects, largely supporting increasingly complex scientific\n    developments. While the latter is important, it should not be at the expense\n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other\n    industrial countries in Europe and Asia, continue to encourage and advance\n    the teaching of engineering. Both China and India, respectively, graduate\n    six and eight times as many traditional engineers as does the United States.\n    Other industrial countries at minimum maintain their output, while America\n    suffers an increasingly serious decline in the number of engineering graduates\n    and a lack of well-educated engineers.\n\"\"\"\n)\n\nNo model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]\n\n\n\n\n\nç¿»è¨³ã—ãŸæ–‡ç« ã‚’è¿”ã™ã€‚ pipelineã®ãƒ¢ãƒ‡ãƒ«å¼•æ•°modelã«ç¿»è¨³ã‚’ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¥ã‚Œã‚‹ã€‚ ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€è‹±èªã‹ã‚‰ãƒ•ãƒ©ãƒ³ã‚¹èªã¸ã®ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ã„ã‚‹ã€‚ ï¼ˆãƒ‰ã‚¤ãƒ„èªã¸ã®ç¿»è¨³ã®å ´åˆã«ã¯ã€translation_en_to_de ã‚’taskå¼•æ•°ã¨ã™ã‚‹ã€‚ï¼‰\n\ntranslator = pipeline(\"translation_en_to_fr\")\ntranslator(\"This course is produced by Hugging Face.\")\n\nNo model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n[{'translation_text': 'Ce cours est produit par Hugging Face.'}]\n\n\nGoogle Colab.ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ç¿»è¨³ã‚’è¡Œã†å ´åˆã«ã¯ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦sentencepieceã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã‹ã‚‰ã€ ã‚«ãƒ¼ãƒãƒ«ã‚’ãƒªã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n !pip install sentencepiece\nä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯Helsinki-NLã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€æ§˜ã€…ãªè¨€èªé–“ã®ç¿»è¨³ã‚’è¡Œã†ã€‚ ä¾‹ã¨ã—ã¦ã€è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³ã‚’ç¤ºã™ã€‚\n\ndef create_translation_pipeline(source_lang, target_lang):\n    model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n    translator = pipeline(\"translation\", model=model_name)\n    return translator\n\ndef translate_text(translator, text):\n    result = translator(text, max_length=500)\n    return result[0]['translation_text']\n\n# Example usage:\nsource_lang_code = \"en\"  # English\ntarget_lang_code = \"jap\"  # Japanese\n\ntranslator = create_translation_pipeline(source_lang_code, target_lang_code)\n\nenglish_text = \"This is a pen.\"\ntranslated_text = translate_text(translator, english_text)\n\nprint(f\"{source_lang_code.capitalize()}: {english_text}\")\nprint(f\"{target_lang_code.capitalize()}: {translated_text}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn: This is a pen.\nJap: ã“ã‚Œ ã¯ ç­† ã§ ã‚ ã‚‹ .\n\n\n\n\n\npipelineã®ä¸­èº«ã¯ã€ä»¥ä¸‹ã®å‡¦ç†ã«åˆ†è§£ã•ã‚Œã‚‹ã€‚\næ–‡å­—åˆ— =&gt; ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ =&gt; ãƒ¢ãƒ‡ãƒ«ã€€=&gt; å¾Œå‡¦ç†\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom pprint import pprint\nfrom transformers import AutoModelForSequenceClassification\nimport torch\n\n\n\nã¾ãšã€å…¥åŠ›ã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªã‚„è¨˜å·ãªã©ï¼‰ã«åˆ†å‰²ã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•´æ•°ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ ã“ã‚Œã«ã¯ã€AutoTokenizer ã‚¯ãƒ©ã‚¹ã®from_pretrainedãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã€‚ å¼•æ•°ã«ã¯ã€https://huggingface.co/models ã«ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«å checkpointã‚’ä¸ãˆã‚‹ã€‚\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\npprint(tokenizer)\n\nDistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n\n\nç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒŠãƒ¼ã‚¶ãƒ¼tokenizerã«æ–‡å­—åˆ—ï¼ˆã®ãƒªã‚¹ãƒˆï¼‰ã‚’ä¸ãˆã‚‹ã¨ã€å¤‰æ›ã•ã‚ŒãŸæ•°å€¤æƒ…å ±ã‚’å«ã‚“ã è¾æ›¸ãŒç”Ÿæˆã•ã‚Œã‚‹ã€‚ è¾æ›¸ã®ã‚­ãƒ¼ã¯ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨æ„ã™ã‚‹ã‹ã‚’è¡¨ã™attention_maskã¨å…¥åŠ›ã‚’æ•°å€¤ã«å¤‰æ›ã—ãŸå¤šæ¬¡å…ƒé…åˆ—ã‚’è¡¨ã™ input_ids ã§ã‚ã‚‹ã€‚\nã“ã®éš›ã€ã©ã®æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†ã‹ã‚’è¡¨ã™return_tensorsã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ ã“ã“ã§ã¯ã€PyTorchã‚’ä½¿ã†ã®ã§ã€å¼•æ•°ã«ptã‚’æŒ‡å®šã™ã‚‹ã€‚\n\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\npprint(inputs)\n\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n             0,     0,     0,     0,     0,     0]])}\n\n\n\n\n\nç¶šã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã€‚ ã“ã“ã§ã¯ã€AutoModelã‚¯ãƒ©ã‚¹ã®from_pretrainedãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã€‚\nã“ã“ã§ç”Ÿæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åŸºæœ¬éƒ¨åˆ†ã ã‘ã‚’ã‚‚ã¡ã€å‡ºåŠ›ã¯å…¥åŠ›ã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ãŸå¤šæ¬¡å…ƒé…åˆ—ï¼ˆãƒ†ãƒ³ã‚½ãƒ«ï¼‰ã§ã‚ã‚‹ã€‚\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModel.from_pretrained(checkpoint)\n\nSome weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\npprint(model)\n\nDistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)\n\n\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ç”Ÿæˆã—ãŸè¾æ›¸ã‚’å±•é–‹ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹ã¨ã€PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ãŒå‡ºåŠ›ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ã€‚\n\noutputs = model(**inputs)\nprint(outputs.last_hidden_state.shape)\n\ntorch.Size([2, 16, 768])\n\n\n\noutputs\n\nBaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n         ...,\n         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n\n        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n         ...,\n         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), hidden_states=None, attentions=None)\n\n\nä»Šåº¦ã¯ã€å®Ÿéš›ã«æ„Ÿæƒ…åˆ†æã‚’è¡Œã†ãŸã‚ã®å±¤ã‚’å«ã‚“ã ãƒ¢ãƒ‡ãƒ«ã‚’ã€ AutoModelForSequenceClassificationã‚¯ãƒ©ã‚¹ã‚’ç”¨ã„ã¦ç”Ÿæˆã™ã‚‹ã€‚\nå‡ºåŠ›ã®logitsã«ä¿ç®¡ã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ãŒå¾—ã‚‰ã‚ŒãŸæ•°å€¤ã§ã‚ã‚‹ã€‚\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel2 = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs2 = model2(**inputs)\n\n\noutputs2\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n        [ 4.1692, -3.3464]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n\n\nå¾—ã‚‰ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯é–¢æ•°ã‚’ç”¨ã„ã¦ç¢ºç‡ã«å¤‰æ›ã™ã‚‹ã€‚ã“ã‚ŒãŒäºˆæ¸¬å€¤ã«ãªã‚‹ã€‚\n\npredictions = torch.nn.functional.softmax(outputs2.logits, dim=-1)\nprint(predictions)\n\ntensor([[4.0195e-02, 9.5981e-01],\n        [9.9946e-01, 5.4418e-04]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\næœ€åˆã®æ–‡ã®äºˆæ¸¬å€¤ã¯ [0.0402, 0.9598]ã€2ç•ªç›®ã®æ–‡ã®äºˆæ¸¬å€¤ã¯[0.9995, 0.0005]ã§ã‚ã‚‹ã€‚ ã“ã‚Œã¯æœ€åˆã®æ–‡ã¯ã€1ã§ã‚ã‚‹ç¢ºç‡ãŒé«˜ãã€2ç•ªç›®ã®æ–‡ã¯0ã§ã‚ã‚‹ç¢ºç‡ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚\nãƒ¢ãƒ‡ãƒ«ã§ç”¨ã„ã‚‰ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’å¾—ã‚‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®id2labelå±æ€§ã‚’ã¿ã‚‹ã€‚\n\nmodel2.config.id2label\n\n{0: 'NEGATIVE', 1: 'POSITIVE'}\n\n\nã—ãŸãŒã£ã¦ã€æœ€åˆã®æ–‡ç« ã¯POSITIVEã€2ç•ªç›®ã®æ–‡ç« ã¯NEGATIVEã§ã‚ã‚‹ã¨åˆ¤å®šã•ã‚Œã‚‹ã€‚",
    "crumbs": [
      "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
    ]
  },
  {
    "objectID": "05transformers.html#ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³",
    "href": "05transformers.html#ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³",
    "title": "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
    "section": "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³",
    "text": "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³\n\nç”»åƒåˆ†é¡\nä»¥ä¸‹ã®ç”»åƒã‚’ä¾‹ã¨ã—ã¦ç”¨ã„ã‚‹ã€‚\n\n\nimage_example1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n\n\nvision_classifier = pipeline(task=\"image-classification\")\n\npreds = vision_classifier(images =image_example1)\npreds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\npreds\n\nNo model was supplied, defaulted to google/vit-base-patch16-224 and revision 5dca96d (https://huggingface.co/google/vit-base-patch16-224).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n\n\n\n\n\n\n\n\n\n[{'score': 0.4335, 'label': 'lynx, catamount'},\n {'score': 0.0348,\n  'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'},\n {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'},\n {'score': 0.0239, 'label': 'Egyptian cat'},\n {'score': 0.0229, 'label': 'tiger cat'}]\n\n\n\n\nç‰©ä½“æ¤œå‡º\nä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦è¿½åŠ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n!pip install timm\n\nfrom transformers import pipeline\n\ndetector = pipeline(task=\"object-detection\")\npreds = detector(image_example1)\npreds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\npreds\n\nNo model was supplied, defaulted to facebook/detr-resnet-50 and revision 2729413 (https://huggingface.co/facebook/detr-resnet-50).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nCould not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\nThe `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n\n\n\n\n\n\n\n\n[{'score': 0.9864,\n  'label': 'cat',\n  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]\n\n\n\n\nç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³\n\nsegmenter = pipeline(task=\"image-segmentation\")\npreds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\nprint(*preds, sep=\"\\n\")\n\nNo model was supplied, defaulted to facebook/detr-resnet-50-panoptic and revision fc15262 (https://huggingface.co/facebook/detr-resnet-50-panoptic).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nCould not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n`label_ids_to_fuse` unset. No instance will be fused.\n\n\n\n\n\n\n\n\n\n\n\n{'score': 0.9879, 'label': 'LABEL_184'}\n{'score': 0.9973, 'label': 'snow'}\n{'score': 0.9972, 'label': 'cat'}\n\n\n\n\næ·±ã•æ¨å®š\n\nestimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\nresult = estimator(images=image_example1)\nresult\n\nSome weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nCould not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n\n\n{'predicted_depth': tensor([[[ 0.7999,  0.8382,  0.8483,  ...,  2.3091,  2.3669,  2.3291],\n          [ 0.8054,  0.8101,  0.8106,  ...,  2.3390,  2.3357,  2.3307],\n          [ 0.8580,  0.8359,  0.8457,  ...,  2.3557,  2.3509,  2.3599],\n          ...,\n          [26.3410, 26.4059, 26.3881,  ..., 17.5088, 17.4768, 17.4148],\n          [26.4727, 26.4515, 26.5042,  ..., 17.4223, 17.3911, 17.4052],\n          [26.5116, 26.5452, 26.5301,  ..., 17.4719, 17.4700, 17.4025]]]),\n 'depth': &lt;PIL.Image.Image image mode=L size=960x686&gt;}",
    "crumbs": [
      "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
    ]
  },
  {
    "objectID": "05transformers.html#éŸ³å£°",
    "href": "05transformers.html#éŸ³å£°",
    "title": "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
    "section": "éŸ³å£°",
    "text": "éŸ³å£°\nä»¥ä¸‹ã®æ¼”èª¬ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨ã„ã‚‹ã€‚\n\n\n\naudio_example1 = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n\n\néŸ³å£°åˆ†é¡\nãƒ¢ãƒ‡ãƒ«ã«superb/hubert-base-superb-erã‚’ä½¿ã†ã¨éŸ³å£°ã®æ„Ÿæƒ…ã‚’åˆ†é¡ã—ã€ MIT/ast-finetuned-audioset-10-10-0.4593ã‚’ä½¿ã†ã¨éŸ³å£°ã®ç¨®é¡ã‚’åˆ†é¡ã™ã‚‹ã€‚\n\n# #classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\nclassifier = pipeline(task=\"audio-classification\", model=\"MIT/ast-finetuned-audioset-10-10-0.4593\")\npreds = classifier(audio_example1)\npreds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\npreds\n\n/usr/local/lib/python3.10/dist-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py:96: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  waveform = torch.from_numpy(waveform).unsqueeze(0)\n\n\n[{'score': 0.4208, 'label': 'Speech'},\n {'score': 0.1793, 'label': 'Rain on surface'},\n {'score': 0.1301, 'label': 'Rain'},\n {'score': 0.096, 'label': 'Raindrop'},\n {'score': 0.0578, 'label': 'Music'}]\n\n\n\n\néŸ³å£°èªè­˜\n\ntranscriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\ntranscriber(audio_example1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n\n\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}",
    "crumbs": [
      "Hugging Faceã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç·´ç¿’å•é¡Œé›† II",
    "section": "",
    "text": "ã“ã®ãƒšãƒ¼ã‚¸ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç·´ç¿’å•é¡Œé›† https://scmopt.github.io/analytics ã®ç¶šç·¨ã§ã‚ã‚Šã€ä¸»ã«æ·±å±¤å­¦ç¿’ã¨æœ€é©åŒ–ã«ã¤ã„ã¦ã®æœ€æ–°ã®è©±é¡Œã‚’ç´¹ä»‹ã™ã‚‹ã€‚",
    "crumbs": [
      "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç·´ç¿’å•é¡Œé›† II"
    ]
  },
  {
    "objectID": "30chatgpt.html",
    "href": "30chatgpt.html",
    "title": "ChatGPTã‚’ç”¨ã„ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°",
    "section": "",
    "text": "openai, python-dotenv (python 3.8.1ä»¥ä¸Šï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nã€Œcommandã€+ã€Œshiftã€+ã€Œ.ã€ ã§ã€éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º\nhttps://platform.openai.com/account/api-keys ã§ã‚­ãƒ¼ã‚’å¾—ã‚‹ï¼ 3ãƒ¶æœˆã§180$ã‚’ç„¡æ–™ã§ä½¿ç”¨ã§ãã‚‹ï¼ .env ã«OPENAI_API_KEY = &lt;ã‚­ãƒ¼&gt; ã‚’è¿½åŠ \n.gitignore ã« .env ã‚’è¿½åŠ \n\nimport openai\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\n\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\n\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0, # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]\n\n\n# text = f\"\"\"\n# You should express what you want a model to do by \\ \n# providing instructions that are as clear and \\ \n# specific as you can possibly make them. \\ \n# This will guide the model towards the desired output, \\ \n# and reduce the chances of receiving irrelevant \\ \n# or incorrect responses. Don't confuse writing a \\ \n# clear prompt with writing a short prompt. \\ \n# In many cases, longer prompts provide more clarity \\ \n# and context for the model, which can lead to \\ \n# more detailed and relevant outputs.\n# \"\"\"\n# prompt = f\"\"\"\n# Summarize the text delimited by triple backticks \\ \n# into a single sentence.\n# ```{text}```\n# \"\"\"\n# response = get_completion(prompt)\n# print(response)",
    "crumbs": [
      "ChatGPTã‚’ç”¨ã„ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
    ]
  },
  {
    "objectID": "30chatgpt.html#æº–å‚™",
    "href": "30chatgpt.html#æº–å‚™",
    "title": "ChatGPTã‚’ç”¨ã„ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°",
    "section": "",
    "text": "openai, python-dotenv (python 3.8.1ä»¥ä¸Šï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nã€Œcommandã€+ã€Œshiftã€+ã€Œ.ã€ ã§ã€éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º\nhttps://platform.openai.com/account/api-keys ã§ã‚­ãƒ¼ã‚’å¾—ã‚‹ï¼ 3ãƒ¶æœˆã§180$ã‚’ç„¡æ–™ã§ä½¿ç”¨ã§ãã‚‹ï¼ .env ã«OPENAI_API_KEY = &lt;ã‚­ãƒ¼&gt; ã‚’è¿½åŠ \n.gitignore ã« .env ã‚’è¿½åŠ \n\nimport openai\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\n\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\n\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0, # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]\n\n\n# text = f\"\"\"\n# You should express what you want a model to do by \\ \n# providing instructions that are as clear and \\ \n# specific as you can possibly make them. \\ \n# This will guide the model towards the desired output, \\ \n# and reduce the chances of receiving irrelevant \\ \n# or incorrect responses. Don't confuse writing a \\ \n# clear prompt with writing a short prompt. \\ \n# In many cases, longer prompts provide more clarity \\ \n# and context for the model, which can lead to \\ \n# more detailed and relevant outputs.\n# \"\"\"\n# prompt = f\"\"\"\n# Summarize the text delimited by triple backticks \\ \n# into a single sentence.\n# ```{text}```\n# \"\"\"\n# response = get_completion(prompt)\n# print(response)",
    "crumbs": [
      "ChatGPTã‚’ç”¨ã„ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
    ]
  },
  {
    "objectID": "32torchrl.html",
    "href": "32torchrl.html",
    "title": "TorchRLã¨RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’",
    "section": "",
    "text": "import torch\nfrom tensordict.nn import TensorDictModule\n\nimport gymnasium as gym\nfrom torchrl.envs.libs.gym import GymEnv, GymWrapper\n\n\n\nPendulum-v1\n\nè¦³æ¸¬ observation: \\(x,y,\\theta\\)\nè¡Œå‹• action: ãƒˆãƒ«ã‚¯ï¼ˆå›è»¢åŠ›ï¼‰\n\nç’°å¢ƒ env ã‚’ç”Ÿæˆã—ï¼Œç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ï¼ ä»¥ä¸‹ã®è¦³æ¸¬å€¤ã‚’å«ã‚€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚‚ã¤åˆæœŸçŠ¶æ…‹ã‚’è¡¨ã™ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ãŒå¾—ã‚‰ã‚Œã‚‹ï¼\n\ndone:\nobservation: è¦³æ¸¬å€¤\nterminated:\ntruncated:\n\n\ngym_env = gym.make(\"Pendulum-v1\")\nenv = GymWrapper(gym_env, device=\"cpu\")\n\nreset = env.reset()\nprint(reset)\n\nTensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\n\n\nãƒªã‚»ãƒƒãƒˆã«æ¬¡ã„ã§ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•ã‚’ rand_action ã§å¾—ã‚‹ï¼\nã“ã‚Œã‚‚ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ã§ï¼Œè¡Œå‹• action ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ï¼\n\nreset_with_action = env.rand_action(reset)\nprint(reset_with_action)\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\nprint(reset_with_action[\"observation\"], reset_with_action[\"action\"])\n\ntensor([ 0.5448, -0.8386,  0.8938]) tensor([0.0090])\n\n\n\n\n\nè¡Œå‹•ã‚’stepã«å…¥ã‚Œã¦æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€ï¼ è¿”å€¤ã®ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ã«ã¯ï¼Œ nextã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ï¼ nextã«ã¯æ¬¡ã«ç§»ã‚‹çŠ¶æ…‹ã‚’è¡¨ã™ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ï¼\n\nstepped_data = env.step(reset_with_action)\nprint(stepped_data)\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n            batch_size=torch.Size([]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\nprint(stepped_data[\"action\"], stepped_data[\"next\"])\n\ntensor([0.4661]) TensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\n\n\nè¡Œå‹•ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã«å…¥ã‚ŒãŸã‚‚ã®ã‚’step_mdpé–¢æ•°ã«å…¥ã‚Œã‚‹ã¨æ¬¡ã®çŠ¶æ…‹ã«ç§»ã‚‹ï¼ nextãŒè¤‡æ•°ã‚ã‚‹å ´åˆã«ã¯ï¼Œãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ (MDP: Markov Decision Process)ã«ã—ãŸãŒã„ï¼Œæ¬¡ã®çŠ¶æ…‹ãŒé¸æŠã•ã‚Œã‚‹ï¼\n\nfrom torchrl.envs import step_mdp\n\ndata = step_mdp(stepped_data)\nprint(data)\n\nTensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\n\n\nrolloutã‚’ç”¨ã„ã‚‹ã¨ï¼Œä¸Šã®ä¸€é€£ã®æ“ä½œã‚’é€£ç¶šã§è¡Œã†ã“ã¨ãŒã§ãã‚‹ï¼ max_stepsã®åå¾©ã®æƒ…å ±ã‚’ä¿ç®¡ã—ãŸãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ãŒè¿”ã•ã‚Œã‚‹ï¼\n\nrollout = env.rollout(max_steps=10)\nprint(rollout, \"\\n\", rollout[\"observation\"])\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n            batch_size=torch.Size([10]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=cpu,\n    is_shared=False) \n tensor([[-0.3323,  0.9432, -0.3913],\n        [-0.3438,  0.9390,  0.2434],\n        [-0.3796,  0.9251,  0.7687],\n        [-0.4574,  0.8893,  1.7127],\n        [-0.5562,  0.8310,  2.2960],\n        [-0.6708,  0.7417,  2.9086],\n        [-0.7798,  0.6261,  3.1803],\n        [-0.8825,  0.4704,  3.7365],\n        [-0.9629,  0.2699,  4.3276],\n        [-0.9993,  0.0383,  4.7006]])\n\n\n\n\n\nç°¡å˜ãª1å±¤ã®ç·šå½¢å±¤ã‚’ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ã‚’ç”¨ã„ã¦ç”Ÿæˆã—ï¼Œ ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’è¡Œã†ï¼\n\nfrom tensordict.nn import TensorDictModule\n\nmodule = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\npolicy = TensorDictModule(\n    module,\n    in_keys=[\"observation\"],\n    out_keys=[\"action\"],\n)\n\n\nrollout = env.rollout(max_steps=10, policy=policy)\nprint(rollout)\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n            batch_size=torch.Size([10]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=cpu,\n    is_shared=False)",
    "crumbs": [
      "TorchRLã¨RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’"
    ]
  },
  {
    "objectID": "32torchrl.html#torchrlã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã®åŸºç¤",
    "href": "32torchrl.html#torchrlã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã®åŸºç¤",
    "title": "TorchRLã¨RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’",
    "section": "",
    "text": "import torch\nfrom tensordict.nn import TensorDictModule\n\nimport gymnasium as gym\nfrom torchrl.envs.libs.gym import GymEnv, GymWrapper\n\n\n\nPendulum-v1\n\nè¦³æ¸¬ observation: \\(x,y,\\theta\\)\nè¡Œå‹• action: ãƒˆãƒ«ã‚¯ï¼ˆå›è»¢åŠ›ï¼‰\n\nç’°å¢ƒ env ã‚’ç”Ÿæˆã—ï¼Œç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ï¼ ä»¥ä¸‹ã®è¦³æ¸¬å€¤ã‚’å«ã‚€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚‚ã¤åˆæœŸçŠ¶æ…‹ã‚’è¡¨ã™ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ãŒå¾—ã‚‰ã‚Œã‚‹ï¼\n\ndone:\nobservation: è¦³æ¸¬å€¤\nterminated:\ntruncated:\n\n\ngym_env = gym.make(\"Pendulum-v1\")\nenv = GymWrapper(gym_env, device=\"cpu\")\n\nreset = env.reset()\nprint(reset)\n\nTensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\n\n\nãƒªã‚»ãƒƒãƒˆã«æ¬¡ã„ã§ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•ã‚’ rand_action ã§å¾—ã‚‹ï¼\nã“ã‚Œã‚‚ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ã§ï¼Œè¡Œå‹• action ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ï¼\n\nreset_with_action = env.rand_action(reset)\nprint(reset_with_action)\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\nprint(reset_with_action[\"observation\"], reset_with_action[\"action\"])\n\ntensor([ 0.5448, -0.8386,  0.8938]) tensor([0.0090])\n\n\n\n\n\nè¡Œå‹•ã‚’stepã«å…¥ã‚Œã¦æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€ï¼ è¿”å€¤ã®ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ã«ã¯ï¼Œ nextã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ï¼ nextã«ã¯æ¬¡ã«ç§»ã‚‹çŠ¶æ…‹ã‚’è¡¨ã™ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ï¼\n\nstepped_data = env.step(reset_with_action)\nprint(stepped_data)\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n            batch_size=torch.Size([]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\nprint(stepped_data[\"action\"], stepped_data[\"next\"])\n\ntensor([0.4661]) TensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\n\n\nè¡Œå‹•ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã«å…¥ã‚ŒãŸã‚‚ã®ã‚’step_mdpé–¢æ•°ã«å…¥ã‚Œã‚‹ã¨æ¬¡ã®çŠ¶æ…‹ã«ç§»ã‚‹ï¼ nextãŒè¤‡æ•°ã‚ã‚‹å ´åˆã«ã¯ï¼Œãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ (MDP: Markov Decision Process)ã«ã—ãŸãŒã„ï¼Œæ¬¡ã®çŠ¶æ…‹ãŒé¸æŠã•ã‚Œã‚‹ï¼\n\nfrom torchrl.envs import step_mdp\n\ndata = step_mdp(stepped_data)\nprint(data)\n\nTensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n\n\n\n\n\nrolloutã‚’ç”¨ã„ã‚‹ã¨ï¼Œä¸Šã®ä¸€é€£ã®æ“ä½œã‚’é€£ç¶šã§è¡Œã†ã“ã¨ãŒã§ãã‚‹ï¼ max_stepsã®åå¾©ã®æƒ…å ±ã‚’ä¿ç®¡ã—ãŸãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ãŒè¿”ã•ã‚Œã‚‹ï¼\n\nrollout = env.rollout(max_steps=10)\nprint(rollout, \"\\n\", rollout[\"observation\"])\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n            batch_size=torch.Size([10]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=cpu,\n    is_shared=False) \n tensor([[-0.3323,  0.9432, -0.3913],\n        [-0.3438,  0.9390,  0.2434],\n        [-0.3796,  0.9251,  0.7687],\n        [-0.4574,  0.8893,  1.7127],\n        [-0.5562,  0.8310,  2.2960],\n        [-0.6708,  0.7417,  2.9086],\n        [-0.7798,  0.6261,  3.1803],\n        [-0.8825,  0.4704,  3.7365],\n        [-0.9629,  0.2699,  4.3276],\n        [-0.9993,  0.0383,  4.7006]])\n\n\n\n\n\nç°¡å˜ãª1å±¤ã®ç·šå½¢å±¤ã‚’ãƒ†ãƒ³ã‚½ãƒ«è¾æ›¸ã‚’ç”¨ã„ã¦ç”Ÿæˆã—ï¼Œ ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’è¡Œã†ï¼\n\nfrom tensordict.nn import TensorDictModule\n\nmodule = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\npolicy = TensorDictModule(\n    module,\n    in_keys=[\"observation\"],\n    out_keys=[\"action\"],\n)\n\n\nrollout = env.rollout(max_steps=10, policy=policy)\nprint(rollout)\n\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n            batch_size=torch.Size([10]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=cpu,\n    is_shared=False)",
    "crumbs": [
      "TorchRLã¨RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’"
    ]
  },
  {
    "objectID": "32torchrl.html#rl4coã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã§çµ„åˆã›æœ€é©åŒ–å•é¡Œã®æ±‚è§£",
    "href": "32torchrl.html#rl4coã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã§çµ„åˆã›æœ€é©åŒ–å•é¡Œã®æ±‚è§£",
    "title": "TorchRLã¨RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’",
    "section": "RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã§çµ„åˆã›æœ€é©åŒ–å•é¡Œã®æ±‚è§£",
    "text": "RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã§çµ„åˆã›æœ€é©åŒ–å•é¡Œã®æ±‚è§£\nå·¡å›ã‚»ãƒ¼ãƒ«ã‚¹ãƒãƒ³å•é¡Œã®ç’°å¢ƒ TSPEnvã‚’æº–å‚™ã™ã‚‹ï¼ã€€ç‚¹æ•°ã¯ \\(50\\) ã®å•é¡Œä¾‹ã‚’ç”Ÿæˆã™ã‚‹ï¼\næ–¹ç­–ã¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”¨ã„ã‚‹ï¼ã“ã‚Œã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ï¼\nè¨“ç·´ã«ã¯ REINFORCE ã‚’ç”¨ã„ã‚‹ï¼\n\nimport torch\n\nfrom rl4co.envs import TSPEnv\nfrom rl4co.models import AttentionModelPolicy, REINFORCE\nfrom rl4co.utils.trainer import RL4COTrainer\n\n\n# RL4CO env based on TorchRL\nenv = TSPEnv(generator_params={'num_loc': 50})\n\n# Policy: neural network, in this case with encoder-decoder architecture\npolicy = AttentionModelPolicy(env_name=env.name,\n                              embed_dim=128,\n                              num_encoder_layers=3,\n                              num_heads=8,\n                            )\n\n# RL Model: REINFORCE and greedy rollout baseline\nmodel = REINFORCE(env,\n                    policy,\n                    baseline=\"rollout\",\n                    batch_size=512,\n                    train_data_size=100_000,\n                    val_data_size=10_000,\n                    optimizer_kwargs={\"lr\": 1e-4},\n                    )\n\n/Users/mikiokubo/miniconda3/envs/jupyterlab/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/Users/mikiokubo/miniconda3/envs/jupyterlab/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n\n\n\nè¨“ç·´ã—ã¦ã„ãªã„æ–¹ç­–ã§ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ\n\n# Greedy rollouts over untrained policy\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntd_init = env.reset(batch_size=[3]).to(device)\npolicy = policy.to(device)\nout = policy(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions_untrained = out['actions'].cpu().detach()\nrewards_untrained = out['reward'].cpu().detach()\n\nfor i in range(3):\n    print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")\n    env.render(td_init[i], actions_untrained[i])\n\nProblem 1 | Cost: 28.789\nProblem 2 | Cost: 21.648\nProblem 3 | Cost: 23.582\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nè¨“ç·´ã‚¯ãƒ©ã‚¹ã®ç”Ÿæˆ\nRL4COTrainerã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æº–å‚™ã—ï¼Œ ã‚¨ãƒãƒƒã‚¯æ•° \\(3\\) ã§è¨“ç·´ã‚’è¡Œã†ï¼\n\ntrainer = RL4COTrainer(\n    max_epochs=3,\n    accelerator=\"gpu\",\n    devices=1,\n    logger=None,\n)\n\nUsing 16bit Automatic Mixed Precision (AMP)\n/Users/mikiokubo/miniconda3/envs/jupyterlab/lib/python3.10/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/Users/mikiokubo/miniconda3/envs/jupyterlab/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n\n\n\ntrainer.fit(model)\n\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\n\n  | Name     | Type                 | Params | Mode \n----------------------------------------------------------\n0 | env      | TSPEnv               | 0      | train\n1 | policy   | AttentionModelPolicy | 710 K  | train\n2 | baseline | WarmupBaseline       | 710 K  | train\n----------------------------------------------------------\n1.4 M     Trainable params\n0         Non-trainable params\n1.4 M     Total params\n5.681     Total estimated model params size (MB)\n/Users/mikiokubo/miniconda3/envs/jupyterlab/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n/Users/mikiokubo/miniconda3/envs/jupyterlab/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n/Users/mikiokubo/miniconda3/envs/jupyterlab/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\nSanity Checking DataLoader 0:   0%|                                                                               | 0/2 [00:00&lt;?, ?it/s]                                                                                                                                        Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [12:31&lt;00:00,  0.26it/s, v_num=59, train/reward=-7.30, train/loss=-0.694]\nValidation: |                                                                                                     | 0/? [00:00&lt;?, ?it/s]\nValidation:   0%|                                                                                                | 0/20 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:   0%|                                                                                   | 0/20 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:   5%|â–ˆâ–ˆâ–ˆâ–Š                                                                       | 1/20 [00:01&lt;00:34,  0.54it/s]\nValidation DataLoader 0:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                   | 2/20 [00:04&lt;00:36,  0.49it/s]\nValidation DataLoader 0:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 3/20 [00:06&lt;00:36,  0.47it/s]\nValidation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 4/20 [00:08&lt;00:34,  0.46it/s]\nValidation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                        | 5/20 [00:11&lt;00:33,  0.45it/s]\nValidation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 6/20 [00:13&lt;00:31,  0.44it/s]\nValidation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 7/20 [00:15&lt;00:29,  0.44it/s]\nValidation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 8/20 [00:18&lt;00:27,  0.44it/s]\nValidation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 9/20 [00:20&lt;00:25,  0.44it/s]\nValidation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 10/20 [00:22&lt;00:22,  0.44it/s]\nValidation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 11/20 [00:25&lt;00:20,  0.44it/s]\nValidation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 12/20 [00:27&lt;00:18,  0.43it/s]\nValidation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 13/20 [00:30&lt;00:16,  0.43it/s]\nValidation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 14/20 [00:32&lt;00:13,  0.43it/s]\nValidation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 15/20 [00:34&lt;00:11,  0.43it/s]\nValidation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 16/20 [00:37&lt;00:09,  0.43it/s]\nValidation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 17/20 [00:39&lt;00:06,  0.43it/s]\nValidation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 18/20 [00:41&lt;00:04,  0.43it/s]\nValidation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/20 [00:44&lt;00:02,  0.43it/s]\nValidation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46&lt;00:00,  0.43it/s]\nEpoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [12:19&lt;00:00,  0.27it/s, v_num=59, train/reward=-6.72, train/loss=-1.64, val/reward=-6.63]\nValidation: |                                                                                                     | 0/? [00:00&lt;?, ?it/s]\nValidation:   0%|                                                                                                | 0/20 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:   0%|                                                                                   | 0/20 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:   5%|â–ˆâ–ˆâ–ˆâ–Š                                                                       | 1/20 [00:01&lt;00:35,  0.54it/s]\nValidation DataLoader 0:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                   | 2/20 [00:04&lt;00:37,  0.48it/s]\nValidation DataLoader 0:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 3/20 [00:07&lt;00:44,  0.38it/s]\nValidation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 4/20 [00:10&lt;00:40,  0.39it/s]\nValidation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                        | 5/20 [00:12&lt;00:37,  0.40it/s]\nValidation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 6/20 [00:14&lt;00:34,  0.40it/s]\nValidation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 7/20 [00:17&lt;00:31,  0.41it/s]\nValidation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 8/20 [00:19&lt;00:29,  0.41it/s]\nValidation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 9/20 [00:21&lt;00:26,  0.42it/s]\nValidation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 10/20 [00:23&lt;00:23,  0.42it/s]\nValidation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 11/20 [00:26&lt;00:21,  0.42it/s]\nValidation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 12/20 [00:28&lt;00:19,  0.42it/s]\nValidation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 13/20 [00:30&lt;00:16,  0.42it/s]\nValidation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 14/20 [00:33&lt;00:14,  0.42it/s]\nValidation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 15/20 [00:35&lt;00:11,  0.42it/s]\nValidation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 16/20 [00:37&lt;00:09,  0.42it/s]\nValidation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 17/20 [00:40&lt;00:07,  0.42it/s]\nValidation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 18/20 [00:42&lt;00:04,  0.42it/s]\nValidation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/20 [00:44&lt;00:02,  0.43it/s]\nValidation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46&lt;00:00,  0.43it/s]\nEpoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [12:15&lt;00:00,  0.27it/s, v_num=59, train/reward=-6.60, train/loss=-3.36, val/reward=-6.46]\nValidation: |                                                                                                     | 0/? [00:00&lt;?, ?it/s]\nValidation:   0%|                                                                                                | 0/20 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:   0%|                                                                                   | 0/20 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:   5%|â–ˆâ–ˆâ–ˆâ–Š                                                                       | 1/20 [00:01&lt;00:34,  0.54it/s]\nValidation DataLoader 0:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                   | 2/20 [00:04&lt;00:37,  0.48it/s]\nValidation DataLoader 0:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 3/20 [00:06&lt;00:37,  0.46it/s]\nValidation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 4/20 [00:08&lt;00:35,  0.45it/s]\nValidation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                        | 5/20 [00:11&lt;00:33,  0.45it/s]\nValidation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 6/20 [00:13&lt;00:31,  0.44it/s]\nValidation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 7/20 [00:15&lt;00:29,  0.44it/s]\nValidation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 8/20 [00:18&lt;00:27,  0.44it/s]\nValidation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 9/20 [00:20&lt;00:25,  0.44it/s]\nValidation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 10/20 [00:22&lt;00:22,  0.44it/s]\nValidation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 11/20 [00:25&lt;00:20,  0.43it/s]\nValidation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 12/20 [00:27&lt;00:18,  0.43it/s]\nValidation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 13/20 [00:29&lt;00:16,  0.43it/s]\nValidation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 14/20 [00:32&lt;00:13,  0.43it/s]\nValidation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 15/20 [00:34&lt;00:11,  0.43it/s]\nValidation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 16/20 [00:36&lt;00:09,  0.43it/s]\nValidation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 17/20 [00:39&lt;00:06,  0.43it/s]\nValidation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 18/20 [00:41&lt;00:04,  0.43it/s]\nValidation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/20 [00:43&lt;00:02,  0.43it/s]\nValidation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46&lt;00:00,  0.43it/s]\nEpoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [13:01&lt;00:00,  0.25it/s, v_num=59, train/reward=-6.60, train/loss=-3.36, val/reward=-6.40]Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [14:34&lt;00:00,  0.22it/s, v_num=59, train/reward=-6.60, train/loss=-3.36, val/reward=-6.40]\n\n\n\n\næœ€é©åŒ–\nè¨“ç·´ã—ãŸæ–¹ç­–ã§æ±‚è§£ã—ï¼Œå¯è¦–åŒ–ã™ã‚‹ï¼\n\n# Greedy rollouts over trained model (same states as previous plot)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions_trained = out['actions'].cpu().detach()\n\n# Plotting\nimport matplotlib.pyplot as plt\nfor i, td in enumerate(td_init):\n    fig, axs = plt.subplots(1,2, figsize=(11,5))\n    env.render(td, actions_untrained[i], ax=axs[0])\n    env.render(td, actions_trained[i], ax=axs[1])\n    axs[0].set_title(f\"Untrained | Cost = {-rewards_untrained[i].item():.3f}\")\n    axs[1].set_title(r\"Trained $\\pi_\\theta$\" + f\"| Cost = {-out['reward'][i].item():.3f}\")",
    "crumbs": [
      "TorchRLã¨RL4COã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’"
    ]
  },
  {
    "objectID": "17neuralprophet.html",
    "href": "17neuralprophet.html",
    "title": "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬",
    "section": "",
    "text": "NeuralProphet ã¯éœ€è¦äºˆæ¸¬ã®ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚ã‚‹ï¼\nBayesæ¨è«–ã«åŸºã¥ãäºˆæ¸¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ prophet ã¨æ©Ÿæ¢°ï¼ˆæ·±å±¤ï¼‰å­¦ç¿’ã®æ©‹æ¸¡ã—ã®ãŸã‚ã«æ–°ãŸã«é–‹ç™ºã•ã‚ŒãŸã‚‚ã®ã§ã€æ·±å±¤å­¦ç¿’ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ PyTorch ã‚’ç”¨ã„ã¦ã„ã‚‹ã€‚\nvega_datasetsã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã®ã§ï¼Œã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠãï¼\n\n# Google Colabã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãŠãã€‚\n# !pip uninstall -y torch notebook notebook_shim tensorflow tensorflow-datasets prophet torchaudio torchdata torchtext torchvision\n# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã«ã¯ã€ä»¥ä¸‹ã‚’ç”Ÿã‹ã™ã€‚\n# !pip install -U neuralprophet\n# !pip install -U vega_datasets\n# !pip install -U ipywidgets",
    "crumbs": [
      "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬"
    ]
  },
  {
    "objectID": "17neuralprophet.html#neuralprophetã¨ã¯",
    "href": "17neuralprophet.html#neuralprophetã¨ã¯",
    "title": "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬",
    "section": "",
    "text": "NeuralProphet ã¯éœ€è¦äºˆæ¸¬ã®ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚ã‚‹ï¼\nBayesæ¨è«–ã«åŸºã¥ãäºˆæ¸¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ prophet ã¨æ©Ÿæ¢°ï¼ˆæ·±å±¤ï¼‰å­¦ç¿’ã®æ©‹æ¸¡ã—ã®ãŸã‚ã«æ–°ãŸã«é–‹ç™ºã•ã‚ŒãŸã‚‚ã®ã§ã€æ·±å±¤å­¦ç¿’ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ PyTorch ã‚’ç”¨ã„ã¦ã„ã‚‹ã€‚\nvega_datasetsã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã®ã§ï¼Œã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠãï¼\n\n# Google Colabã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãŠãã€‚\n# !pip uninstall -y torch notebook notebook_shim tensorflow tensorflow-datasets prophet torchaudio torchdata torchtext torchvision\n# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã«ã¯ã€ä»¥ä¸‹ã‚’ç”Ÿã‹ã™ã€‚\n# !pip install -U neuralprophet\n# !pip install -U vega_datasets\n# !pip install -U ipywidgets",
    "crumbs": [
      "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬"
    ]
  },
  {
    "objectID": "17neuralprophet.html#è«¸ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
    "href": "17neuralprophet.html#è«¸ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
    "title": "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬",
    "section": "è«¸ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
    "text": "è«¸ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nNeuralProphetã§äºˆæ¸¬ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãŠãï¼\n\nimport pandas as pd\nfrom neuralprophet import NeuralProphet, set_log_level\n\n# ã‚¨ãƒ©ãƒ¼ä»¥å¤–ã¯ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ‘åˆ¶\nset_log_level(\"ERROR\")\n\nfrom vega_datasets import data\nimport plotly.express as px\nimport plotly",
    "crumbs": [
      "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬"
    ]
  },
  {
    "objectID": "17neuralprophet.html#neuralprophetã®åŸºæœ¬",
    "href": "17neuralprophet.html#neuralprophetã®åŸºæœ¬",
    "title": "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬",
    "section": "NeuralProphetã®åŸºæœ¬",
    "text": "NeuralProphetã®åŸºæœ¬\nNeuralProphetã‚’Pythonã‹ã‚‰å‘¼ã³å‡ºã—ã¦ä½¿ã†æ–¹æ³•ã¯ï¼Œæ©Ÿæ¢°å­¦ç¿’ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸scikit-learnã¨åŒã˜ã§ã‚ã‚‹ã€‚\n\nNeuralProphetã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹modelã‚’ç”Ÿæˆ\nfitãƒ¡ã‚½ãƒƒãƒ‰ã§å­¦ç¿’ï¼ˆå¼•æ•°ã¯ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€è¿”å€¤ã¯è©•ä¾¡å°ºåº¦ï¼‰\npredictãƒ¡ã‚½ãƒƒãƒ‰ã§äºˆæ¸¬ï¼ˆå¼•æ•°ã¯äºˆæ¸¬ã—ãŸã„æœŸé–“ã‚’å«ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰\n\n\nä¾‹é¡Œï¼šWikiã‚¢ã‚¯ã‚»ã‚¹æ•°\nä¾‹ã¨ã—ã¦ã‚¢ãƒ¡ãƒªã‚«ãƒ³ãƒ•ãƒƒãƒˆãƒœãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ¤ã®Payton Manningã®Wikiã‚¢ã‚¯ã‚»ã‚¹æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã€‚\n\ndf = pd.read_csv(\"http://logopt.com/data/peyton_manning.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n2007-12-10\n9.590761\n\n\n1\n2007-12-11\n8.519590\n\n\n2\n2007-12-12\n8.183677\n\n\n3\n2007-12-13\n8.072467\n\n\n4\n2007-12-14\n7.893572\n\n\n\n\n\n\n\nProphetãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ï¼Œfitãƒ¡ã‚½ãƒƒãƒ‰ã§å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–ï¼‰ã‚’è¡Œã†ï¼fitãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã™ã®ã¯ï¼Œä¸Šã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã‚ã‚‹ï¼ã“ã®ã¨ãã€ds(datestamp)åˆ—ã«æ—¥ä»˜ï¼ˆæ™‚åˆ»ï¼‰ã‚’ã€yåˆ—ã«äºˆæ¸¬ã—ãŸã„æ•°å€¤ã‚’å…¥ã‚Œã¦ãŠãå¿…è¦ãŒã‚ã‚‹ ï¼ˆã“ã®ä¾‹é¡Œã§ã¯ï¼Œã‚ã‚‰ã‹ã˜ã‚ãã®ã‚ˆã†ã«å¤‰æ›´ã•ã‚Œã¦ã„ã‚‹ï¼‰ï¼\n\nmodel = NeuralProphet()\nmetrics = model.fit(df)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\nmake_future_dataframeãƒ¡ã‚½ãƒƒãƒ‰ã§æœªæ¥ã®æ™‚åˆ»ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã€‚ æ—¢å®šå€¤ã§ã¯äºˆæ¸¬ã§ç”¨ã„ãŸéå»ã®æ™‚åˆ»ã‚‚å«ã¾ãªã„ã®ã§ã€n_historic_predictionså¼•æ•°ã‚’Trueã«ã—ã¦ã€éå»ã®æ™‚åˆ»ã‚‚å…¥ã‚Œã‚‹ã€‚ å¼•æ•°ã¯äºˆæ¸¬ã‚’ã—ãŸã„æœŸé–“æ•°periodsã§ã‚ã‚Šï¼Œã“ã“ã§ã¯ã€ï¼‘å¹´å¾Œï¼ˆ365æ—¥åˆ†ï¼‰ã¾ã§äºˆæ¸¬ã™ã‚‹ã“ã¨ã«ã™ã‚‹ã€‚\n\ndf_future = model.make_future_dataframe(df, n_historic_predictions=True, periods=365)\nforecast = model.predict(df_future)\nmodel.set_plotting_backend(\"matplotlib\") #matplotlibã§æç”»ã™ã‚‹ï¼ˆæ—¢å®šå€¤ã¯plotlyï¼‰\nmodel.plot(forecast)\n\n\n\n\n\n\n\n\n\n\n\npredict ãƒ¡ã‚½ãƒƒãƒ‰ã«äºˆæ¸¬ã—ãŸã„æ™‚åˆ»ã‚’å«ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ future ã‚’æ¸¡ã™ã¨ã€äºˆæ¸¬å€¤ã‚’å…¥ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ forecastã‚’è¿”ã™ã€‚\n\nforecast.tail()\n\n\n\n\n\n\n\n\nds\ny\nyhat1\ntrend\nseason_yearly\nseason_weekly\n\n\n\n\n3265\n2017-01-15\nNaN\n8.216602\n7.137280\n1.036658\n0.042665\n\n\n3266\n2017-01-16\nNaN\n8.550267\n7.136175\n1.049828\n0.364264\n\n\n3267\n2017-01-17\nNaN\n8.316690\n7.135070\n1.060175\n0.121446\n\n\n3268\n2017-01-18\nNaN\n8.133187\n7.133965\n1.067564\n-0.068342\n\n\n3269\n2017-01-19\nNaN\n8.133450\n7.132861\n1.071885\n-0.071296\n\n\n\n\n\n\n\n\n\nä¸€èˆ¬åŒ–åŠ æ³•ãƒ¢ãƒ‡ãƒ«\nNeuralProphetã«ãŠã‘ã‚‹äºˆæ¸¬ã¯ä¸€èˆ¬åŒ–åŠ æ³•ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦è¡Œã‚ã‚Œã‚‹ï¼ ã“ã‚Œã¯ï¼Œå‚¾å‘å¤‰å‹•ï¼Œå­£ç¯€å¤‰å‹•ï¼Œã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãªã©ã®æ§˜ã€…ãªå› å­ã®å’Œã¨ã—ã¦äºˆæ¸¬ã‚’è¡Œã†æ–¹æ³•ã§ã‚ã‚‹ï¼\n\\[\ny_t =g_t + s_t + h_t + f_t + a_t + \\ell_t + \\epsilon_t\n\\]\n\n\\(y_t\\) : äºˆæ¸¬å€¤\n\\(g_t\\) : å‚¾å‘å¤‰å‹•(trend)ï¼›å‚¾å‘å¤‰åŒ–ç‚¹ã‚ã‚Šã®åŒºåˆ†çš„ç·šå½¢ï¼ˆã‚¢ãƒ•ã‚£ãƒ³ï¼‰é–¢æ•°\n\\(s_t\\) : å­£ç¯€å¤‰å‹•ï¼›å¹´æ¬¡ï¼Œé€±æ¬¡ï¼Œæ—¥æ¬¡ã®å­£ç¯€å¤‰å‹•ã‚’sin, cosã®çµ„ã¿åˆã‚ã›ï¼ˆãƒ•ãƒ¼ãƒªã‚¨ç´šæ•°ï¼‰ã§è¡¨ç¾\n\\(h_t\\) : ä¼‘æ—¥ãªã©ã®ã‚¤ãƒ™ãƒ³ãƒˆé …\n\\(f_t\\): å¤–ç”Ÿå¤‰æ•°ã«å¯¾ã™ã‚‹å›å¸°é …\n\\(a_t\\): è‡ªå·±å›å¸°é …ï¼ˆæ™‚é–“é…ã‚Œï¼ˆãƒ©ã‚°ï¼‰ã‚’æŒ‡å®šã™ã‚‹ï¼‰\n\\(\\ell_t\\): æ™‚é–“é…ã‚Œï¼ˆãƒ©ã‚°ï¼‰ä»˜ãã®å¤–ç”Ÿå¤‰æ•°ã«å¯¾ã™ã‚‹å›å¸°é …\n\\(\\epsilon_t\\) : èª¤å·®é …\n\nå› å­ã”ã¨ã«äºˆæ¸¬å€¤ã®æç”»ã‚’è¡Œã†ã«ã¯ï¼Œplot_componentsãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼æ—¢å®šã§ã¯ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«ï¼Œä¸Šã‹ã‚‰é †ã«å‚¾å‘å¤‰å‹•ï¼Œå¹´æ¬¡ã®å­£ç¯€å¤‰å‹•ã€é€±æ¬¡ã®å­£ç¯€å¤‰å‹•ãŒæç”»ã•ã‚Œã‚‹ï¼ã¾ãŸï¼Œå‚¾å‘å¤‰å‹•ã®å›³ï¼ˆä¸€ç•ªä¸Šï¼‰ã«ã¯ï¼Œäºˆæ¸¬ã®èª¤å·®ç¯„å›²ãŒç¤ºã•ã‚Œã‚‹ï¼å­£ç¯€å¤‰å‹•ã®èª¤å·®ç¯„å›²ã‚’å¾—ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã¯ï¼Œå¾Œè¿°ã™ã‚‹ï¼\n\nmodel.plot_components(forecast)\n\n\n\n\n\n\n\n\nå¯¾è©±å½¢å¼ã«ï¼Œæ‹¡å¤§ç¸®å°ã‚„ç¯„å›²æŒ‡å®šãŒã§ãã‚‹å‹•çš„ãªå›³ã‚‚ï¼ŒPlotlyãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n\nmodel.set_plotting_backend(\"plotly\") \nfig = model.plot(forecast)\n#plotly.offline.plot(fig);\n\n\n\n\n\n\n\n\n\n\n\n\nä¾‹é¡Œï¼š \\(CO_2\\) æ’å‡ºé‡ã®ãƒ‡ãƒ¼ã‚¿\nãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰äºŒé…¸åŒ–ç‚­ç´ æ’å‡ºé‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ŒPlotly Expressã§æç”»ã™ã‚‹ï¼\n\nco2 = data.co2_concentration()\nco2.head()\n\n\n\n\n\n\n\n\nDate\nCO2\n\n\n\n\n0\n1958-03-01\n315.70\n\n\n1\n1958-04-01\n317.46\n\n\n2\n1958-05-01\n317.51\n\n\n3\n1958-07-01\n315.86\n\n\n4\n1958-08-01\n314.93\n\n\n\n\n\n\n\n\nfig = px.line(co2,x=\"Date\",y=\"CO2\")\n#plotly.offline.plot(fig);\n\n\n\n\n\n\n\n\n\n\nåˆ—åã®å¤‰æ›´ã«ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®renameãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼å¼•æ•°ã¯columnsã§ï¼Œå…ƒã®åˆ—åã‚’ã‚­ãƒ¼ã¨ã—ï¼Œå¤‰æ›´å¾Œã®åˆ—åã‚’å€¤ã¨ã—ãŸè¾æ›¸ã‚’ä¸ãˆã‚‹ï¼ã¾ãŸï¼Œå…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ä¸Šæ›¸ãã™ã‚‹ãŸã‚ã«ï¼Œinplaceå¼•æ•°ã‚’Trueã«è¨­å®šã—ã¦ãŠãï¼\n\nco2.rename(columns={\"Date\":\"ds\",\"CO2\":\"y\"},inplace=True)\nco2.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1958-03-01\n315.70\n\n\n1\n1958-04-01\n317.46\n\n\n2\n1958-05-01\n317.51\n\n\n3\n1958-07-01\n315.86\n\n\n4\n1958-08-01\n314.93\n\n\n\n\n\n\n\nmake_future_dataframeãƒ¡ã‚½ãƒƒãƒ‰ã§æœªæ¥ã®æ™‚åˆ»ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã€‚æ—¢å®šå€¤ã§ã¯ã€ï¼ˆäºˆæ¸¬ã§ç”¨ã„ãŸï¼‰éå»ã®æ™‚åˆ»ã‚‚å«ã‚€ã€‚ ã“ã“ã§ã¯ã€200ãƒ¶æœˆå…ˆã¾ã§äºˆæ¸¬ã™ã‚‹ã“ã¨ã«ã™ã‚‹ã€‚\nãã®ãŸã‚ã«ï¼Œå¼•æ•° periods ã‚’200ã«è¨­å®šã—ã¦ãŠã\npredict ãƒ¡ã‚½ãƒƒãƒ‰ã«äºˆæ¸¬ã—ãŸã„æ™‚åˆ»ã‚’å«ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ future ã‚’æ¸¡ã™ã¨ã€äºˆæ¸¬å€¤ã‚’å…¥ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ forecastã‚’è¿”ã™ã€‚\næœ€å¾Œã«plotãƒ¡ã‚½ãƒƒãƒ‰ã§è¡¨ç¤ºã™ã‚‹ï¼\n\nmodel = NeuralProphet()\nmetrics = model.fit(co2)\nfuture = model.make_future_dataframe(co2, n_historic_predictions=True, periods=200)\nforecast = model.predict(future)\nmodel.set_plotting_backend(\"plotly\")\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\näºˆæ¸¬ã¯ä¸€èˆ¬åŒ–åŠ æ³•ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦è¡Œã‚ã‚Œã‚‹ï¼\nã“ã‚Œã¯ï¼Œå‚¾å‘å¤‰å‹•ï¼Œå­£ç¯€å¤‰å‹•ï¼Œã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãªã©ã®æ§˜ã€…ãªå› å­ã®å’Œã¨ã—ã¦äºˆæ¸¬ã‚’è¡Œã†æ–¹æ³•ã§ã‚ã‚‹ï¼\nä¸Šã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ï¼Œé€±æ¬¡ã¨æ—¥æ¬¡ã®å­£ç¯€å¤‰å‹•ã¯ç„¡è¦–ã•ã‚Œï¼Œå¹´æ¬¡ã®å­£ç¯€å¤‰å‹•ã®ã¿è€ƒæ…®ã—ã¦äºˆæ¸¬ã—ã¦ã„ã‚‹ï¼\nå› å­ã”ã¨ã«äºˆæ¸¬å€¤ã®æç”»ã‚’è¡Œã†ã«ã¯ï¼Œplot_componentsãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼æ—¢å®šã§ã¯ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«ï¼Œä¸Šã‹ã‚‰é †ã«å‚¾å‘å¤‰å‹•ï¼Œå¹´æ¬¡ã®å­£ç¯€å¤‰å‹•ãŒæç”»ã•ã‚Œã‚‹ï¼\n\nmodel.plot_components(forecast)\n\n\n\n\n\n\n\n\n\n\nä¾‹é¡Œï¼šèˆªç©ºæ©Ÿä¹—å®¢æ•°ã®ãƒ‡ãƒ¼ã‚¿\nProphetã®æ—¢å®šå€¤ã§ã¯å­£ç¯€å¤‰å‹•ã¯åŠ æ³•çš„ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŒã€å•é¡Œã«ã‚ˆã£ã¦ã¯ä¹—æ³•çš„å­£ç¯€å¤‰å‹•ã®æ–¹ãŒè‰¯ã„å ´åˆã‚‚ã‚ã‚‹ã€‚ ä¾‹ã¨ã—ã¦ã€èˆªç©ºæ©Ÿã®ä¹—å®¢æ•°ã‚’äºˆæ¸¬ã—ã¦ã¿ã‚ˆã†ã€‚æœ€åˆã«æ—¢å®šå€¤ã®åŠ æ³•çš„å­£ç¯€å¤‰å‹•ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã—ï¼Œæ¬¡ã„ã§ä¹—æ³•çš„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã™ã‚‹ï¼\n\npassengers = pd.read_csv(\"http://logopt.com/data/AirPassengers.csv\")\npassengers.head()\n\n\n\n\n\n\n\n\nMonth\n#Passengers\n\n\n\n\n0\n1949-01\n112\n\n\n1\n1949-02\n118\n\n\n2\n1949-03\n132\n\n\n3\n1949-04\n129\n\n\n4\n1949-05\n121\n\n\n\n\n\n\n\n\nfig = px.line(passengers,x=\"Month\",y=\"#Passengers\")\n#plotly.offline.plot(fig);\n\n\n\n\n\n\n\n\n\n\n\npassengers.rename(inplace=True,columns={\"Month\":\"ds\",\"#Passengers\":\"y\"})\npassengers.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1949-01\n112\n\n\n1\n1949-02\n118\n\n\n2\n1949-03\n132\n\n\n3\n1949-04\n129\n\n\n4\n1949-05\n121\n\n\n\n\n\n\n\nå­£ç¯€å¤‰å‹•ã‚’ä¹—æ³•çš„ã«å¤‰æ›´ã™ã‚‹ã«ã¯ï¼Œ ãƒ¢ãƒ‡ãƒ«ã® seasonality_mode å¼•æ•°ã‚’ä¹—æ³•çš„ã‚’è¡¨ã™ multiplicative ã«è¨­å®šã™ã‚‹ï¼\nã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã¨ãã«ã€å¼•æ•°quantilesã§ä¸ç¢ºå®Ÿæ€§ã®å¹…ã‚’è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚ä»¥ä¸‹ã§ã¯ã€5%ã¨95%ã®å¹…ã‚’è¡¨ç¤ºã•ã›ã¦ã„ã‚‹ã€‚\n\nmodel = NeuralProphet(quantiles=[0.05, 0.95])\nmetrics = model.fit(passengers)\nfuture = model.make_future_dataframe(passengers, periods=20, n_historic_predictions=True)\nforecast = model.predict(future)\nmodel.set_plotting_backend(\"plotly\")\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\nmodel = NeuralProphet(quantiles=[0.05, 0.95], seasonality_mode=\"multiplicative\")\nmetrics = model.fit(passengers)\nfuture = model.make_future_dataframe(passengers, periods=20, n_historic_predictions=True)\nforecast = model.predict(future)\nmodel.set_plotting_backend(\"plotly\")\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\nçµæœã‹ã‚‰ï¼Œä¹—æ³•çš„å­£ç¯€å¤‰å‹•ã®æ–¹ãŒï¼Œè‰¯ã„äºˆæ¸¬ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ï¼\n\n\nå•é¡Œï¼ˆå°å£²ã‚Šã®éœ€è¦ãƒ‡ãƒ¼ã‚¿ï¼‰\nä»¥ä¸‹ã®ï¼Œå°å£²ã‚Šã®éœ€è¦ãƒ‡ãƒ¼ã‚¿ã‚’æç”»ã—ï¼Œäºˆæ¸¬ã‚’è¡Œãˆï¼ ãŸã ã—ï¼Œãƒ¢ãƒ‡ãƒ«ã¯ä¹—æ³•çš„å­£ç¯€å¤‰å‹•ã§ï¼Œæœˆæ¬¡ã§äºˆæ¸¬ã›ã‚ˆï¼\n\nretail = pd.read_csv(`http://logopt.com/data/retail_sales.csv`)\nretail.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1992-01-01\n146376\n\n\n1\n1992-02-01\n147079\n\n\n2\n1992-03-01\n159336\n\n\n3\n1992-04-01\n163669\n\n\n4\n1992-05-01\n170068\n\n\n\n\n\n\n\n\n\nä¾‹é¡Œï¼š 1æ™‚é–“ã”ã¨ã®æ°—æ¸©ãƒ‡ãƒ¼ã‚¿\nã“ã“ã§ã¯ã‚·ã‚¢ãƒˆãƒ«ã®æ°—æ¸©ã®äºˆæ¸¬ã‚’è¡Œã†ï¼\n\nclimate = data.seattle_temps()\nclimate.head()\n\n\n\n\n\n\n\n\ndate\ntemp\n\n\n\n\n0\n2010-01-01 00:00:00\n39.4\n\n\n1\n2010-01-01 01:00:00\n39.2\n\n\n2\n2010-01-01 02:00:00\n39.0\n\n\n3\n2010-01-01 03:00:00\n38.9\n\n\n4\n2010-01-01 04:00:00\n38.8\n\n\n\n\n\n\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ï¼Œ date åˆ—ã«æ—¥ä»˜ã¨1æ™‚é–“ã”ã¨ã®æ™‚åˆ»ãŒï¼Œ temp åˆ—ã«æ°—æ¸©ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ã„ã‚‹ï¼\nNeuralProphetã¯ï¼Œ æ—¥åˆ¥ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚‚æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã€‚ dateåˆ—ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã¯ã€æ—¥ä»˜ã‚’è¡¨ã™YYYY-MM-DDã®å¾Œã«æ™‚åˆ»ã‚’è¡¨ã™HH:MM:SSãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã€‚ æœªæ¥ã®æ™‚åˆ»ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ã€make_future_dataframeãƒ¡ã‚½ãƒƒãƒ‰ã§ç”Ÿæˆã™ã‚‹ã€‚\n\nclimate[\"Date\"] = pd.to_datetime(climate.date)\n\n\nclimate.rename(columns={\"Date\":\"ds\",\"temp\":\"y\"},inplace=True)\n\n\nclimate = climate[[\"ds\",\"y\"]] #ä½™åˆ†ãªåˆ—ã‚’å‰Šé™¤ã™ã‚‹\n\n\nmodel = NeuralProphet(daily_seasonality=True)\nmetrics = model.fit(climate)\nfuture = model.make_future_dataframe(climate, periods=200, n_historic_predictions=True)\nforecast = model.predict(future)\nmodel.set_plotting_backend(\"plotly\")\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\nå› å­ã”ã¨ã«äºˆæ¸¬å€¤ã‚’æç”»ã™ã‚‹ã¨ï¼Œå‚¾å‘å¤‰å‹•ã¨é€±æ¬¡ã®å­£ç¯€å¤‰å‹•ã®ä»–ã«ï¼Œæ—¥æ¬¡ã®å­£ç¯€å¤‰å‹•ï¼ˆ1æ—¥ã®æ°—æ¸©ã®å¤‰åŒ–ï¼‰ã‚‚å‡ºåŠ›ã•ã‚Œã‚‹ï¼\n\nmodel.plot_components(forecast)\n\n                                                \n\n\n\n\nå•é¡Œï¼ˆã‚µãƒ³ãƒ•ãƒ©ãƒ³ã‚·ã‚¹ã‚³ã®æ°—æ¸©ãƒ‡ãƒ¼ã‚¿ï¼‰\nä»¥ä¸‹ã®ã‚µãƒ³ãƒ•ãƒ©ãƒ³ã‚·ã‚¹ã‚³ã®æ°—æ¸©ãƒ‡ãƒ¼ã‚¿ã‚’æç”»ã—ï¼Œæ™‚é–“å˜ä½ã§äºˆæ¸¬ã‚’è¡Œãˆï¼\n\nsf = data.sf_temps()\nsf.head()\n\n\n\n\n\n\n\n\ntemp\ndate\n\n\n\n\n0\n47.8\n2010-01-01 00:00:00\n\n\n1\n47.4\n2010-01-01 01:00:00\n\n\n2\n46.9\n2010-01-01 02:00:00\n\n\n3\n46.5\n2010-01-01 03:00:00\n\n\n4\n46.0\n2010-01-01 04:00:00\n\n\n\n\n\n\n\n\n\nå‚¾å‘å¤‰åŒ–ç‚¹\nã€Œä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ ªä¾¡ãŒï¼Œä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ã«ç§»ã£ãŸã€ã¨ã„ã†ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚ˆãè€³ã«ã™ã‚‹ã ã‚ã†ï¼ã“ã®ã‚ˆã†ã«ï¼Œå‚¾å‘å¤‰å‹•ã¯ï¼Œæ™‚ã€…å¤‰åŒ–ã™ã‚‹ã¨ä»®å®šã—ãŸæ–¹ãŒè‡ªç„¶ãªã®ã ï¼NeuralProphetã§ã¯ï¼Œã“ã‚Œã‚’å‚¾å‘å¤‰åŒ–ç‚¹ã¨ã—ã¦å‡¦ç†ã™ã‚‹ï¼å†ã³ï¼ŒPeyton Manningã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ï¼\nå‚¾å‘å¤‰åŒ–ç‚¹ã®æ•°ã¯n_changepointsã§æŒ‡å®šã™ã‚‹ã€‚æ—¢å®šå€¤ã¯ \\(10\\) ã§ã‚ã‚‹ã€‚ä»¥ä¸‹ã§ã¯ã€å‚¾å‘å¤‰åŒ–ç‚¹ã‚’ \\(5\\) ã«è¨­å®šã™ã‚‹ã€‚\n\ndf = pd.read_csv(\"http://logopt.com/data/peyton_manning.csv\")\nmodel = NeuralProphet(n_changepoints=5)\nmodel.fit(df)\nfuture = model.make_future_dataframe(df, periods=365, n_historic_predictions=True)\nforecast = model.predict(future)\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n  rank_zero_warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.plot_components(forecast)\n\n\n\n\nå‚¾å‘å¤‰åŒ–ç‚¹ã®ãƒªã‚¹ãƒˆã‚’changepointså¼•æ•°ã§ä¸ãˆã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€1ã¤ã®æ—¥ã ã‘ã§å¤‰åŒ–ã™ã‚‹ã‚ˆã†ã«è¨­å®šã—ã¦ã„ã‚‹ã€‚\n\nmodel = NeuralProphet(changepoints=[\"2014-01-01\"])\nmodel.fit(df)\nfuture = model.make_future_dataframe(df, periods=365, n_historic_predictions=True)\nforecast = model.predict(future)\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.plot_components(forecast)\n\n\n\n\n\n\nä¾‹é¡Œ SP500ãƒ‡ãƒ¼ã‚¿\næ ªä¾¡ã®äºˆæ¸¬ã‚’è¡Œã†ï¼\nå‚¾å‘å¤‰åŒ–ç‚¹ã®å€™è£œã¯è‡ªå‹•çš„ã«è¨­å®šã•ã‚Œã‚‹ã€‚æ—¢å®šå€¤ã§ã¯æ™‚ç³»åˆ—ã®æœ€åˆã®80%ã®éƒ¨åˆ†ã«å‡ç­‰ã«è¨­å®šã•ã‚Œã‚‹ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ã®changepoint_rangeå¼•æ•°ã§è¨­å®šã™ã‚‹ï¼ ã“ã®ä¾‹ã§ã¯ï¼ŒæœŸé–“ã®çµ‚ã‚ã‚Šã§å¤‰åŒ–ç‚¹ã‚’è¨­å®šã—ãŸã„ã®ã§ï¼Œ0.95ã«å¤‰æ›´ã™ã‚‹ï¼\nå¹´æ¬¡ã®å­£ç¯€å¤‰å‹•ã®å¤‰åŒ–ã®åº¦åˆã„ã¯ã€yearly_seasonalityï¼ˆæ—¢å®šå€¤ã¯ \\(10\\)) ã§åˆ¶å¾¡ã§ãã‚‹ã€‚ã“ã®ä¾‹ã§ã¯ï¼Œã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ \\(5\\) ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å¹´é–“ã®å­£ç¯€å¤‰å‹•ã‚’æŠ‘åˆ¶ã—ã¦äºˆæ¸¬ã‚’è¡Œã†ï¼\n\nsp500 = data.sp500()\nsp500.tail()\n\n\n\n\n\n\n\n\ndate\nprice\n\n\n\n\n118\n2009-11-01\n1095.63\n\n\n119\n2009-12-01\n1115.10\n\n\n120\n2010-01-01\n1073.87\n\n\n121\n2010-02-01\n1104.49\n\n\n122\n2010-03-01\n1140.45\n\n\n\n\n\n\n\n\nsp500.rename(inplace=True,columns={\"date\":\"ds\",\"price\":\"y\"})\n\n\nmodel = NeuralProphet(changepoints_range=0.95, yearly_seasonality=5)\nmodel.fit(sp500)\nfuture = model.make_future_dataframe(sp500, periods=20, n_historic_predictions=True)\nforecast = model.predict(future)\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.plot_components(forecast)\n\n\n\n\n\n\nä¾‹é¡Œï¼š å€‹åˆ¥éŠ˜æŸ„ã®æ ªä¾¡ã®äºˆæ¸¬\nstocksãƒ‡ãƒ¼ã‚¿ã§ã¯ï¼Œsymbolåˆ—ã«ä¼æ¥­ã‚³ãƒ¼ãƒ‰ãŒå…¥ã£ã¦ã„ã‚‹ï¼\n\nAAPL ã‚¢ãƒƒãƒ—ãƒ«\nAMZN ã‚¢ãƒã‚¾ãƒ³\nIBM IBM\nGOOG ã‚°ãƒ¼ã‚°ãƒ«\nMSFT ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆ\n\nã¾ãšã¯å¯è¦–åŒ–ã‚’è¡Œã†ã€‚\n\nstocks = data.stocks()\nstocks.tail()\n\n\n\n\n\n\n\n\nsymbol\ndate\nprice\n\n\n\n\n555\nAAPL\n2009-11-01\n199.91\n\n\n556\nAAPL\n2009-12-01\n210.73\n\n\n557\nAAPL\n2010-01-01\n192.06\n\n\n558\nAAPL\n2010-02-01\n204.62\n\n\n559\nAAPL\n2010-03-01\n223.02\n\n\n\n\n\n\n\n\nfig = px.line(stocks,x=\"date\",y=\"price\",color=\"symbol\")\n#plotly.offline.plot(fig);\n\n\n\n\n\n\n\n\n\n\nä»¥ä¸‹ã§ã¯ï¼Œãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã®æ ªä¾¡ã‚’äºˆæ¸¬ã—ã¦ã¿ã‚‹ï¼\n\nmsft = stocks[stocks.symbol == \"MSFT\"]\nmsft.head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nprice\n\n\n\n\n0\nMSFT\n2000-01-01\n39.81\n\n\n1\nMSFT\n2000-02-01\n36.35\n\n\n2\nMSFT\n2000-03-01\n43.22\n\n\n3\nMSFT\n2000-04-01\n28.37\n\n\n4\nMSFT\n2000-05-01\n25.45\n\n\n\n\n\n\n\n\nmsft = msft.rename(columns={\"date\":\"ds\",\"price\":\"y\"})\nmsft = msft[[\"ds\",\"y\"]]\nmsft.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n2000-01-01\n39.81\n\n\n1\n2000-02-01\n36.35\n\n\n2\n2000-03-01\n43.22\n\n\n3\n2000-04-01\n28.37\n\n\n4\n2000-05-01\n25.45\n\n\n\n\n\n\n\n\nmodel = NeuralProphet(changepoints_range=0.95,yearly_seasonality=5)\nmodel.fit(msft)\nfuture = model.make_future_dataframe(msft, periods=20, n_historic_predictions=True)\nforecast = model.predict(future)\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#model.plot_components(forecast)\nmodel.plot_parameters(forecast)\n\n\n\n\n\n\nå•é¡Œï¼ˆæ ªä¾¡ï¼‰\nä¸Šã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆä»¥å¤–ã®éŠ˜æŸ„ã‚’1ã¤é¸æŠã—ï¼Œäºˆæ¸¬ã‚’è¡Œãˆï¼\n\nstocks = data.stocks()\nstocks.head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nprice\n\n\n\n\n0\nMSFT\n2000-01-01\n39.81\n\n\n1\nMSFT\n2000-02-01\n36.35\n\n\n2\nMSFT\n2000-03-01\n43.22\n\n\n3\nMSFT\n2000-04-01\n28.37\n\n\n4\nMSFT\n2000-05-01\n25.45",
    "crumbs": [
      "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬"
    ]
  },
  {
    "objectID": "17neuralprophet.html#ç™ºå±•ç·¨",
    "href": "17neuralprophet.html#ç™ºå±•ç·¨",
    "title": "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬",
    "section": "ç™ºå±•ç·¨",
    "text": "ç™ºå±•ç·¨\nä»¥ä¸‹ã§ã¯ï¼ŒNeuralProphetã®é«˜åº¦ãªä½¿ç”¨æ³•ã‚’è§£èª¬ã™ã‚‹ï¼\n\nè‡ªå·±å›å¸°ã«ã‚ˆã‚‹äºˆæ¸¬\nä»¥ä¸‹ã®ä¾‹ã¯ã€ã‚¹ãƒšã‚¤ãƒ³ã®4å¹´é–“ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¾¡æ ¼ã§ã‚ã‚‹ã€‚ ç›´è¿‘ã® \\(p\\) æ—¥å‰ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ä»˜ãã®å’Œã®é …ã‚’è¿½åŠ ã™ã‚‹ã®ãŒã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚ \\(p\\) ã‚’è¡¨ã™n_legså¼•æ•°ã‚’è¨­å®šã™ã‚‹ã¨ã€è‡ªå·±å›å¸°é …ãŒè¿½åŠ ã•ã‚Œã‚‹ã€‚\n\ndf = pd.read_csv(\"https://github.com/ourownstory/neuralprophet-data/raw/main/kaggle-energy/datasets/tutorial01.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n2014-12-31\n65.41\n\n\n1\n2015-01-01\n62.09\n\n\n2\n2015-01-02\n69.44\n\n\n3\n2015-01-03\n65.22\n\n\n4\n2015-01-04\n58.91\n\n\n\n\n\n\n\n\nmodel = NeuralProphet(\n    yearly_seasonality=True,\n    weekly_seasonality=True,\n    daily_seasonality=True,\n    n_lags=10\n)\nmodel.set_plotting_backend(\"matplotlib\")\nmetrics = model.fit(df)\nforecast = model.predict(df)\nmodel.plot(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.plot_parameters()\n\n\n\n\n\n\n\n\n\n\nä¼‘æ—¥ï¼ˆç‰¹åˆ¥ãªã‚¤ãƒ™ãƒ³ãƒˆï¼‰ã‚’è€ƒæ…®ã—ãŸäºˆæ¸¬\nä¼‘æ—¥ã‚„ç‰¹åˆ¥ãªã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã€‚\nãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®add_country_holidaysãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã¦ï¼Œå„å›½ï¼ˆå·ï¼‰ã®ä¼‘æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ å¼•æ•°ã¯ python-holidays ã«ã‚ã‚‹æ–‡å­—åˆ—ã§ã€ ãŸã¨ãˆã°ç±³å›½ã®å ´åˆã«ã¯USã€ã‚¹ãƒšã‚¤ãƒ³ã®å ´åˆã«ã¯ESã‚’æŒ‡å®šã™ã‚‹ã€‚\n\ndf = pd.read_csv(\"https://github.com/ourownstory/neuralprophet-data/raw/main/kaggle-energy/datasets/tutorial01.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n2014-12-31\n65.41\n\n\n1\n2015-01-01\n62.09\n\n\n2\n2015-01-02\n69.44\n\n\n3\n2015-01-03\n65.22\n\n\n4\n2015-01-04\n58.91\n\n\n\n\n\n\n\n\nmodel = NeuralProphet()\nmodel.set_plotting_backend(\"matplotlib\")\nmodel = model.add_country_holidays(\"US\")\nmetrics = model.fit(df)\nforecast = model.predict(df)\nmodel.plot(forecast)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.plot_components(forecast)\n\n\n\n\n\n\n\n\næ‚ªå¤©å€™(extreme_weather)ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸ãˆã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ç‰¹åˆ¥ãªã‚¤ãƒ™ãƒ³ãƒˆã‚’è€ƒæ…®ã—ãŸäºˆæ¸¬ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¯eventã¨dsã®åˆ—ã‚’ã‚‚ã¡ã€ãã‚Œã‚’add_eventsãƒ¡ã‚½ãƒƒãƒ‰ã§è¿½åŠ ã™ã‚‹ã€‚ ãã®å¾Œã€create_df_with_eventsã§ã‚¤ãƒ™ãƒ³ãƒˆãŒè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã€‚\n\ndf_events = pd.DataFrame(\n    {\n        \"event\": \"extreme_weather\",\n        \"ds\": pd.to_datetime(\n            [\n                \"2018-11-23\",\n                \"2018-11-17\",\n                \"2018-10-28\",\n                \"2018-10-18\",\n                \"2018-10-14\",\n            ]\n        ),\n    }\n)\ndf_events.head()\n\n\n\n\n\n\n\n\nevent\nds\n\n\n\n\n0\nextreme_weather\n2018-11-23\n\n\n1\nextreme_weather\n2018-11-17\n\n\n2\nextreme_weather\n2018-10-28\n\n\n3\nextreme_weather\n2018-10-18\n\n\n4\nextreme_weather\n2018-10-14\n\n\n\n\n\n\n\n\nmodel = NeuralProphet()\nmodel.set_plotting_backend(\"matplotlib\")\nmodel.add_events(\"extreme_weather\")\ndf_all = model.create_df_with_events(df, df_events)\ndf_all.head()\n\n\n\n\n\n\n\n\nds\ny\nextreme_weather\n\n\n\n\n0\n2014-12-31\n65.41\n0.0\n\n\n1\n2015-01-01\n62.09\n0.0\n\n\n2\n2015-01-02\n69.44\n0.0\n\n\n3\n2015-01-03\n65.22\n0.0\n\n\n4\n2015-01-04\n58.91\n0.0\n\n\n\n\n\n\n\n\nmetrics = model.fit(df_all)\nforecast = model.predict(df_all)\nmodel.plot(forecast)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nå¤–ç”Ÿå¤‰æ•°ã®è¿½åŠ \nå¤–ç”Ÿå¤‰æ•°ã§ã‚ã‚‹æ°—æ¸©temperatureã‚’åˆ©ç”¨ã—ã¦ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¾¡æ ¼ã®äºˆæ¸¬ã‚’è¡Œã†ã€‚\nã“ã“ã§ã¯ã€\\(q\\) æ—¥å‰ã¾ã§ã®æ°—æ¸©ãŒã‚¨ãƒãƒ«ã‚®ãƒ¼ä¾¡æ ¼ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹æ™‚é–“é…ã‚Œï¼ˆãƒ©ã‚°ï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã‚‹ã€‚ ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®addd_lagged_regressorãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ã€‚ å¼•æ•°ã¯åˆ—åã‚‚ã—ãã¯åˆ—åã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã€‚æ™‚é–“é…ã‚Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \\(q\\) ã¯å¼•æ•°n_lagsã§ä¸ãˆã‚‹ã€‚\nã¾ãŸã€æœªæ¥ã®æ°—æ¸©ãŒæ—¢çŸ¥ã§ã‚ã‚‹ã¨ä»®å®šã—ãŸå ´åˆã«ã¯ã€æ°—æ¸©ã‚’å¤–ç”Ÿå¤‰æ•°ã¨ã—ãŸå›å¸°é …ã‚’è¿½åŠ ã™ã‚‹ã€‚ ã“ã‚Œã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã® add_future_regressorã‚’ç”¨ã„ã‚‹ã€‚ å¼•æ•°ã¯åˆ—åã‚‚ã—ãã¯åˆ—åã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã€‚\n\ndf = pd.read_csv(\"https://github.com/ourownstory/neuralprophet-data/raw/main/kaggle-energy/datasets/tutorial04.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\ntemperature\n\n\n\n\n0\n2015-01-01\n64.92\n277.00\n\n\n1\n2015-01-02\n58.46\n277.95\n\n\n2\n2015-01-03\n63.35\n278.83\n\n\n3\n2015-01-04\n50.54\n279.64\n\n\n4\n2015-01-05\n64.89\n279.05\n\n\n\n\n\n\n\n\nmodel = NeuralProphet(\n    yearly_seasonality=True,\n    weekly_seasonality=True,\n    daily_seasonality=True,\n    n_lags=10\n)\nmodel.set_plotting_backend(\"matplotlib\")\n#model.add_lagged_regressor(\"temperature\", n_lags=5)\nmodel.add_future_regressor(\"temperature\")\nmetrics = model.fit(df)\nforecast = model.predict(df)\nmodel.plot(forecast)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.plot_parameters()\n\n\n\n\n\n\n\n\n\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨­å®šã—ãŸå­£ç¯€å¤‰å‹•\nProphetã§ã¯æ—¢å®šå€¤ã®å¹´æ¬¡(yearly)ã‚„é€±æ¬¡(weekly)ã‚„æ—¥æ¬¡(daily)ã®å­£ç¯€å¤‰å‹•ã ã‘ã§ãªãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è‡ªèº«ã§å­£ç¯€å¤‰å‹•ã‚’å®šç¾©ãƒ»è¿½åŠ ã§ãã‚‹ã€‚ ä»¥ä¸‹ã§ã¯ã€é€±æ¬¡ã®å­£ç¯€å¤‰å‹•ã‚’é™¤ãï¼Œã‹ã‚ã‚Šã«å‘¨æœŸãŒ30.5æ—¥ã®æœˆæ¬¡å¤‰å‹•ã‚’ãƒ•ãƒ¼ãƒªã‚¨æ¬¡æ•°ï¼ˆseasonalityã®åˆ¥åï¼‰5ã¨ã—ã¦è¿½åŠ ã—ã¦ã„ã‚‹ã€‚\n\ndf = pd.read_csv(\"http://logopt.com/data/peyton_manning.csv\")\n\nmodel = NeuralProphet(weekly_seasonality=False)\nmodel.add_seasonality(name=\"monthly\", period=30.5, fourier_order=5)\nmodel.fit(df)\nfuture = model.make_future_dataframe(df, n_historic_predictions=True, periods=365)\nforecast = model.predict(future)\nmodel.plot_components(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nä»–ã®è¦å› ã«ä¾å­˜ã—ãŸå­£ç¯€å¤‰å‹•\nä»–ã®è¦å› ã«ä¾å­˜ã—ãŸå­£ç¯€å¤‰å‹•ã‚‚å®šç¾©ãƒ»è¿½åŠ ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€ã‚ªãƒ³ã‚·ãƒ¼ã‚ºãƒ³ã¨ã‚ªãƒ•ã‚·ãƒ¼ã‚ºãƒ³ã”ã¨é€±æ¬¡å¤‰å‹•ã‚’å®šç¾©ã—ã€è¿½åŠ ã—ã¦ã¿ã‚‹ã€‚\n\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])\ndf[\"on_season\"] = df[\"ds\"].apply(lambda x: x.month in [9, 10, 11, 12, 1])\ndf[\"off_season\"] = df[\"ds\"].apply(lambda x: x.month not in [9, 10, 11, 12, 1])\n\n\nmodel = NeuralProphet(weekly_seasonality=False)\nmodel.add_seasonality(name=\"weekly_on_season\", period=7, fourier_order=3, condition_name=\"on_season\")\nmodel.add_seasonality(name=\"weekly_off_season\", period=7, fourier_order=3, condition_name=\"off_season\")\nmetrics = model.fit(df, freq=\"D\")\nfuture = model.make_future_dataframe(df, n_historic_predictions=True, periods=365)\nforecast = model.predict(future)\nmodel.plot_components(forecast)\n\nWARNING - (py.warnings._showwarnmsg) - /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning:\n\nMPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.",
    "crumbs": [
      "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬"
    ]
  },
  {
    "objectID": "17neuralprophet.html#ä¸»ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ—¢å®šå€¤",
    "href": "17neuralprophet.html#ä¸»ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ—¢å®šå€¤",
    "title": "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬",
    "section": "ä¸»ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ—¢å®šå€¤",
    "text": "ä¸»ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ—¢å®šå€¤\nä»¥ä¸‹ã«NeuralProphetã®ä¸»è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¼•æ•°ï¼‰ã¨ãã®æ—¢å®šå€¤ã‚’ç¤ºã™ï¼\n\nchangepoints=None : å‚¾å‘å¤‰æ›´ç‚¹ã®ãƒªã‚¹ãƒˆ\nchangepoint_range = \\(0.8\\) : å‚¾å‘å¤‰åŒ–ç‚¹ã®å€™è£œã®å¹…ï¼ˆå…ˆé ­ã‹ã‚‰ä½•å‰²ã‚’å€™è£œã¨ã™ã‚‹ã‹ï¼‰\nn_changepoints= \\(10\\) : å‚¾å‘å¤‰æ›´ç‚¹ã®æ•°\ntrend_reg = \\(0\\) : å‚¾å‘å¤‰å‹•ã®æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nyearly_seasonality=auto : å¹´æ¬¡ã®å­£ç¯€å¤‰å‹•ã‚’è€ƒæ…®ã™ã‚‹ã‹å¦ã‹\nweekly_seasonality=auto : é€±æ¬¡ã®å­£ç¯€å¤‰å‹•ã‚’è€ƒæ…®ã™ã‚‹ã‹å¦ã‹\ndaily_seasonality=auto : æ—¥æ¬¡ã®å­£ç¯€å¤‰å‹•ã‚’è€ƒæ…®ã™ã‚‹ã‹å¦ã‹\nseasonality_mode = additive : å­£ç¯€å¤‰å‹•ãŒåŠ æ³•çš„(additive)ã‹ä¹—æ³•çš„(multiplicative)ã‹\nseasonality_reg= \\(0\\) : å­£ç¯€å¤‰å‹•ã®æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nn_forecasts = \\(1\\) : äºˆæ¸¬æ•°\nn_lags = \\(0\\): æ™‚é–“ãšã‚Œï¼ˆãƒ©ã‚°ï¼‰\nar_reg = \\(0\\): è‡ªå·±å›å¸°ã®æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nquantile= \\([]\\) : ä¸ç¢ºå®Ÿæ€§ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®åˆ†ä½æ•°ã®ãƒªã‚¹ãƒˆ",
    "crumbs": [
      "NeuralProphetã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬"
    ]
  },
  {
    "objectID": "16fastai.html#æ·±å±¤å­¦ç¿’ã¨ã¯",
    "href": "16fastai.html#æ·±å±¤å­¦ç¿’ã¨ã¯",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "æ·±å±¤å­¦ç¿’ã¨ã¯",
    "text": "æ·±å±¤å­¦ç¿’ã¨ã¯\næ·±å±¤å­¦ç¿’ã¨ã¯å¤šãã®éš ã‚Œå±¤ï¼ˆå¾Œã§èª¬æ˜ã™ã‚‹ï¼‰ã‚’ã‚‚ã¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã‚ã‚‹ï¼ ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¨ã¯ï¼ˆæ©Ÿæ¢°å­¦ç¿’ã®ã¨ã“ã‚ã§ç°¡å˜ã«è§¦ã‚ŒãŸã‚ˆã†ã«ï¼‰ï¼Œ\n\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã®éšå±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ï¼Œ\nä¸Šå±¤ã‹ã‚‰ã®é‡ã¿ä»˜ãå’Œï¼ˆç·šå½¢å¤‰æ›ï¼‰ã«æ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨ã—ã¦ï¼Œä¸‹å±¤ã«æµã™ï¼Œ\næœ€ä¸‹å±¤ã§ã¯æå‡ºé–¢æ•°ã«ã‚ˆã£ã¦èª¤å·®ã‚’è©•ä¾¡ï¼Œ\nèª¤å·®ã®æƒ…å ±ã‹ã‚‰å‹¾é…ã‚’è¨ˆç®—ï¼Œ\nå‹¾é…ã®æƒ…å ±ã‚’ç”¨ã„ã¦ï¼Œé‡ã¿ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã®æ›´æ–°ï¼Œ\n\nã‚’ç¹°ã‚Šè¿”ã™ã ã‘ã§ã‚ã‚‹ï¼ã“ã‚Œã¯å˜ã«ï¼Œå…¥åŠ›ã‚’å‡ºåŠ›ã«å¤‰æ›ã™ã‚‹é–¢æ•°ã¨ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ãŒï¼Œ ã§ãã‚‹ã ã‘ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«é©åˆã™ã‚‹ã‚ˆã†ã«è¿‘ä¼¼ã™ã‚‹ã“ã¨ã‚’ç›®æ¨™ã¨ã—ã¦ã„ã‚‹ç‚¹ãŒç‰¹å¾´ã§ã‚ã‚‹ï¼\n1ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯å˜ãªã‚‹å¤å…¸çš„ãªç·šå½¢å›å¸°ï¼ˆã‚‚ã—ãã¯åˆ†é¡å•é¡Œã®å ´åˆã«ã¯ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰ã§ã‚ã‚‹ï¼\nä»¥ä¸‹ã§ç”¨ã„ã‚‹ç”¨èªã‚’æ•´ç†ã—ã¦ãŠã“ã†ï¼\n\näººå·¥çŸ¥èƒ½: æ©Ÿæ¢°ã«çŸ¥èƒ½ã‚’æŒãŸã›ã‚‹ãŸã‚ã®æŠ€è¡“ï¼\næ©Ÿæ¢°å­¦ç¿’ï¼šï¼ˆæ•™å¸«ã‚ã‚Šã«é™å®šã ãŒï¼‰å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¨å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼Œãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹æ–¹æ³•ï¼\nãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆï¼šå˜ãªã‚‹é–¢æ•°è¿‘ä¼¼å™¨ï¼\næ·±å±¤å­¦ç¿’ï¼šå˜ãªã‚‹å¤šæ¬¡å…ƒåˆ†æ•£å‹é–¢æ•°è¿‘ä¼¼å™¨ï¼\nfastaiï¼šPyTorchã®ãƒ©ãƒƒãƒ‘ãƒ¼",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#æ·±å±¤å­¦ç¿’ã®æ­´å²",
    "href": "16fastai.html#æ·±å±¤å­¦ç¿’ã®æ­´å²",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "æ·±å±¤å­¦ç¿’ã®æ­´å²",
    "text": "æ·±å±¤å­¦ç¿’ã®æ­´å²\nã„ã¾æµè¡Œã®æ·±å±¤å­¦ç¿’(deep learning)ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‹ã‚‰ç”Ÿã¾ã‚Œï¼Œãã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã‹ã‚‰ç”Ÿã¾ã‚ŒãŸï¼èµ·æºã§ã‚ã‚‹ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã¾ã§é¡ã‚ã†ï¼\n1958å¹´ã«ï¼Œã‚³ãƒ¼ãƒãƒ«å¤§å­¦ã®å¿ƒç†å­¦è€…ã§ã‚ã£ãŸFrank RosenblattãŒãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã®æ¦‚å¿µã‚’ææ¡ˆã—ãŸï¼ã“ã‚Œã¯1å±¤ã‹ã‚‰ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã‚ã‚Šï¼Œæ¥µã‚ã¦å˜ç´”ãªæ§‹æˆã‚’ã‚‚ã¤ãŒï¼Œå½“æ™‚ã¯éƒ¨å±‹ã„ã£ã±ã„ã®ãƒ‘ãƒ³ãƒã‚«ãƒ¼ãƒ‰å¼ã®è¨ˆç®—æ©ŸãŒå¿…è¦ã§ã‚ã£ãŸï¼\néš ã‚Œå±¤ã®ãªã„2å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã®å‡ºåŠ›èª¤å·®ã‹ã‚‰ã®ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ã¯1960å¹´ã«B. Widrow ã¨ M.E. Hoff, Jr.Â ã‚‰ãŒ Widrow-Hoff æ³•ï¼ˆãƒ‡ãƒ«ã‚¿ãƒ«ãƒ¼ãƒ«ï¼‰ã¨ã„ã†åç§°ã§ç™ºè¡¨ã—ãŸï¼ éš ã‚Œå±¤ã®ã‚ã‚‹3å±¤ä»¥ä¸Šã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ã€1967å¹´ã«ç”˜åˆ©ä¿Šä¸€ãŒç™ºè¡¨ã—ãŸï¼\n1969å¹´ã«ï¼ŒMITã®Marvin Minskyï¼ˆäººå·¥çŸ¥èƒ½ã®å·¨äººã¨ã—ã¦çŸ¥ã‚‰ã‚Œã‚‹ï¼‰ãŒï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®é™ç•Œã«ã¤ã„ã¦ã®è«–æ–‡ã‚’ç™ºè¡¨ã—ãŸï¼å½¼ã®åå£°ã«ã‚ˆã‚‹å½±éŸ¿ã®ãŸã‚ã‹ï¼Œãã®å¾Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ç ”ç©¶ã¯å¾ã€…ã«ä¸‹ç«ã«ãªã£ã¦ã„ãï¼\n2006å¹´ã«ï¼Œãƒˆãƒ­ãƒ³ãƒˆå¤§å­¦ã®Geoffrey Hintonï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®çˆ¶ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã‚‹ï¼‰ã¯ï¼Œå¤šéšå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã‚‚åŠ¹ç‡ã‚ˆãå­¦ç¿’ã§ãã‚‹ã‚ˆã†ãªæ–¹æ³•ã«é–¢ã™ã‚‹è«–æ–‡ã‚’ç™ºè¡¨ã™ã‚‹ï¼ ã“ã®æ‰‹æ³•ã¯ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨å‘¼ã°ã‚Œï¼Œãã®å¾Œã®æ·±å±¤å­¦ç¿’ã®çˆ†ç™ºçš„ãªç ”ç©¶ã®ã‚‚ã¨ã«ãªã£ãŸã‚‚ã®ã§ã‚ã‚‹ï¼\n2011å¹´ã«ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆç¤¾ã¯ï¼Œè¨€èªèªè­˜ã®ãŸã‚ã«ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ä½¿ã†ã‚ˆã†ã«ãªã‚‹ï¼ãã®å¾Œã‚‚è¨€èªèªè­˜ã‚„æ©Ÿæ¢°ç¿»è¨³ã¯ï¼Œç”»åƒèªè­˜ã¨ã¨ã¨ã‚‚ã«ï¼Œæ·±å±¤å­¦ç¿’ã®å¿œç”¨åˆ†é‡ã¨ã—ã¦å®šç€ã—ã¦ã„ã‚‹ï¼\n2012å¹´ã®7æœˆã«Googleç¤¾ã¯çŒ«ã‚’èªè­˜ã™ã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã‚ã‚‹Google Brainã‚’é–‹å§‹ã—ï¼Œ8æœˆã«ã¯è¨€èªèªè­˜ã«ç”¨ã„ã‚‹ã‚ˆã†ã«ãªã‚‹ï¼åŒå¹´ã®10æœˆã«ã¯ï¼ŒHintonã®2äººã®å­¦ç”ŸãŒï¼ŒImageNetã‚³ãƒ³ãƒ†ã‚¹ãƒˆã§æ–­ãƒˆãƒ„ã®æˆç¸¾ã§1ä½ã«ãªã‚‹ï¼ã“ã‚Œã‚’ãã£ã‹ã‘ã«ï¼Œæ·±å±¤å­¦ç¿’ãŒæ§˜ã€…ãªå¿œç”¨ã«ä½¿ã‚ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹ï¼\n2015å¹´ã®12æœˆã«ã¯ï¼Œãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆç¤¾ã®ãƒãƒ¼ãƒ ãŒï¼ŒImageNetã‚³ãƒ³ãƒ†ã‚¹ãƒˆã§äººé–“ã‚’è¶…ãˆã‚‹çµæœã‚’å‡ºã—ï¼Œ2016å¹´3æœˆã«ã¯ï¼ŒAlphaGoãŒç¢ã®ä¸–ç•Œãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã§Lee Sedolã‚’æ‰“ã¡è² ã‹ã™ï¼ˆãŸã ã—ã“ã‚Œã¯æ·±å±¤å­¦ç¿’ã¨ã„ã†ã‚ˆã‚Šå¼·åŒ–å­¦ç¿’ã®æˆæœã¨ã‚‚è¨€ãˆã‚‹ï¼‰ï¼\næœ€è¿‘ã§ã¯ï¼Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«(diffusion model)ã‚’ç”¨ã„ãŸé«˜ç²¾åº¦ã®ç”»åƒã®ç”Ÿæˆã‚„ï¼Œ ChatGPT(Generative Pre-trained Transformer)ã«ä»£è¡¨ã•ã‚Œã‚‹è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³(self attention)ã‚’ç”¨ã„ãŸè‡ªç„¶è¨€èªå‡¦ç†ã¸ã®å¿œç”¨ãŒé€²ã¿ï¼ŒæŠ€è¡“ã®æ°‘ä¸»åŒ–ãŒé€²ã‚“ã§ã„ã‚‹ï¼",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#ãªãœæ·±å±¤å­¦ç¿’ãŒã†ã¾ãã„ãã‚ˆã†ã«ãªã£ãŸã®ã‹",
    "href": "16fastai.html#ãªãœæ·±å±¤å­¦ç¿’ãŒã†ã¾ãã„ãã‚ˆã†ã«ãªã£ãŸã®ã‹",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "ãªãœæ·±å±¤å­¦ç¿’ãŒã†ã¾ãã„ãã‚ˆã†ã«ãªã£ãŸã®ã‹ï¼Ÿ",
    "text": "ãªãœæ·±å±¤å­¦ç¿’ãŒã†ã¾ãã„ãã‚ˆã†ã«ãªã£ãŸã®ã‹ï¼Ÿ\nãƒ‡ãƒ¼ã‚¿é‡ã®å¢—å¤§ã«ä¼´ã„ï¼Œãã‚Œã‚’ã†ã¾ãåˆ©ç”¨ã§ãã‚‹æ‰‹æ³•ã§ã‚ã‚‹æ·±å±¤å­¦ç¿’ãŒæœ‰åŠ¹ã«ãªã£ã¦ãã¦ã„ã‚‹ï¼å±¤ã®æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‚å¤§ä¸ˆå¤«ãªã‚ˆã†ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒ¢ãƒ‡ãƒ«ï¼‰ãŒé–‹ç™ºã•ã‚ŒãŸã“ã¨ã‚‚ï¼Œé‡è¦ãªè¦å› ã§ã‚ã‚‹ï¼ã¤ã¾ã‚Šï¼Œãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãŒä¸¡è¼ªã¨ãªã£ã¦ï¼Œæ§˜ã€…ãªåˆ†é‡ã¸ã®å¿œç”¨ã‚’å¾ŒæŠ¼ã—ã—ã¦ã„ã‚‹ã®ã§ã‚ã‚‹ï¼å°ã•ãªãƒ‡ãƒ¼ã‚¿ã—ã‹ãªã„ã¨ãã«ã¯ï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ç·šå½¢å›å¸°ã‚„ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«æ©Ÿæ¢°(SVM)ã¨åŒã˜ç¨‹åº¦ã®æ€§èƒ½ã§ã‚ã‚‹ï¼ã—ã‹ã—ï¼Œãƒ‡ãƒ¼ã‚¿ãŒå¤§è¦æ¨¡ã«ãªã‚‹ã¨ï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯SVMã‚ˆã‚Šé«˜æ€§èƒ½ã«ãªã‚Šï¼Œå°è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®æ–¹ãŒè‰¯ã„æ€§èƒ½ã‚’å‡ºã™ã‚ˆã†ã«ãªã‚‹ï¼\nã•ã‚‰ã«ã¯ï¼ŒGPUã®ä½ä¾¡æ ¼åŒ–ã«ã‚ˆã£ã¦å˜ç´”ãªè¨ˆç®—ã®åå¾©ãŒå¿…è¦ãªæ·±å±¤å­¦ç¿’ãŒé«˜é€Ÿã«å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã“ã¨ã‚‚æ™®åŠã‚’å¾ŒæŠ¼ã—ã—ã¦ã„ã‚‹ï¼æ·±å±¤å­¦ç¿’ãŒã†ã¾ãå‹•ãã“ã¨ãŒçŸ¥ã‚‰ã‚Œã‚‹ã«ã¤ã‚Œã¦ï¼Œç ”ç©¶ã‚‚åŠ é€Ÿã—ã¦ã„ã‚‹ï¼å¤å…¸çš„ãªã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‹ã‚‰ReLUï¼ˆãªã‚‰ã³ã«ãã®äºœç¨®ï¼‰ã¸ã®ç§»è¡Œï¼Œãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆï¼Œãƒãƒƒãƒæ­£è¦åŒ–ãªã©ï¼Œå®Ÿéš›ã«ã†ã¾ãå‹•ãã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é–‹ç™ºã‚‚é‡è¦ãªè¦å› ã§ã‚ã‚‹ï¼ã•ã‚‰ã«ï¼Œå¿œç”¨ã«å¿œã˜ãŸæ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ãŒææ¡ˆã•ã‚Œï¼Œå•é¡Œã«å¿œã˜ã¦é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ããŸã®ã‚‚ï¼Œç†ç”±ã®1ã¤ã§ã‚ã‚‹ï¼\nå¤šãã®äººæãŒæ·±å±¤å­¦ç¿’ã®åˆ†é‡ã«å‚å…¥ã—ãŸã“ã¨ã‚‚é‡è¦ãªè¦å› ã§ã‚ã‚‹ã‚ˆã†ã«æ„Ÿã˜ã¦ã„ã‚‹ï¼ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é©æ­£åŒ–ã¯ï¼Œæœ€é©åŒ–ã«ãŠã‘ã‚‹å®Ÿé¨“çš„è§£æã¨åŒæ§˜ã«ï¼Œè†¨å¤§ãªç³»çµ±çš„ãªå®Ÿé¨“ã¨ï¼Œãã‚Œã‚’è§£æã™ã‚‹ãƒãƒ³ãƒ‘ãƒ¯ãƒ¼ãŒå¿…è¦ã¨ãªã‚‹ï¼ãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹ã—ï¼Œé–‹ç™ºã—ãŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã«ã—ã¦é…å¸ƒã™ã‚‹ã¨ã„ã£ãŸã“ã®åˆ†é‡ã®é¢¨åœŸã‚‚ç ”ç©¶ã‚’åŠ é€Ÿã—ã¦ã„ã‚‹ï¼\nãƒ‡ãƒ¼ã‚¿ã‚„ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’éå…¬é–‹ã«ã™ã‚‹é¢¨åœŸã‚’ã‚‚ã¤ä»–ã®ç ”ç©¶åˆ†é‡ã¯ï¼Œæ·±å±¤å­¦ç¿’ã‚’ãŠæ‰‹æœ¬ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã ã‚ã†ï¼ç‰¹ã«ï¼Œæ—¥æœ¬ã®ä¼æ¥­ã¨ã®å…±åŒç ”ç©¶ã§ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ã‚„é–‹ç™ºã—ãŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯éå…¬é–‹ã«ã—ãŒã¡ã§ã‚ã‚‹ï¼æ·±å±¤å­¦ç¿’ã‚’ç‰½å¼•ã™ã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ¼ã®ãƒ‘ãƒ¯ãƒ¼ã¯ï¼Œãã†ã„ã£ãŸç§˜å¯†ä¸»ç¾©ãŒãªã„ã“ã¨ã«èµ·å› ã—ã¦ã„ã‚‹ï¼",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#fastaiã¨ã¯",
    "href": "16fastai.html#fastaiã¨ã¯",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "fastaiã¨ã¯",
    "text": "fastaiã¨ã¯\næ·±å±¤å­¦ç¿’ã®ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦ã¯ï¼Œ tensorflow (+Keras), PyTorchãªã©ãŒæœ‰åã§ã‚ã‚‹ãŒï¼Œã“ã“ã§ã¯fastai https://www.fast.ai ã‚’ç”¨ã„ã‚‹ï¼\nfastaiã¯ã€æœ€å…ˆç«¯ã®æ·±å±¤å­¦ç¿’ã‚’å®Ÿå‹™å®¶ãŒæ°—è»½ã«é©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚ã‚‹ï¼\né–‹ç™ºè€…ãŒã€ŒAIã‚’ã‚‚ã†ä¸€åº¦uncoolã«ã€ã‚’æ¨™èªã«ã—ã¦ã„ã‚‹ã‚ˆã†ã«ï¼Œå°‚é–€å®¶ã§ãªãã¦ã‚‚ï¼ˆPythonã‚’çŸ¥ã£ã¦ã„ã‚Œã°ï¼‰ã‚ã‚‹ç¨‹åº¦ï¼ˆã¨ã„ã†ã‹æ•°å¹´å‰ã®ä¸–ç•Œæ–°è¨˜éŒ²ç¨‹åº¦ï¼‰ã®æ·±å±¤å­¦ç¿’ã‚’ä½¿ã†ã“ã¨ãŒã§ãã‚‹ï¼\nç‰¹å¾´ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚\n\nã‚³ãƒ¼ãƒ‰ãŒçŸ­ãã‹ã‘ã‚‹ï¼ˆKerasã‚ˆã‚Šã‚‚çŸ­ã„ï¼‰ï¼\né€Ÿã„ï¼\næœ€æ–°ã®å·¥å¤«ãŒå–ã‚Šå…¥ã‚Œã‚‰ã‚Œã¦ã„ã‚‹ï¼\nPyTorchã®è¶³ã‚Šãªã„éƒ¨åˆ†ã‚’è£œå®Œã—ã¦ãã‚Œã‚‹ï¼\nç„¡æ–™ã®ï¼ˆåºƒå‘Šãªã—ã®ï¼‰è¬›ç¾©ãƒ“ãƒ‡ã‚ªãŒã‚ã‚‹ï¼\nãƒ†ã‚­ã‚¹ãƒˆã®ã‚½ãƒ¼ã‚¹ã‚‚ç„¡æ–™å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ï¼ https://github.com/fastai/fastbook",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#fastaiã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
    "href": "16fastai.html#fastaiã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "fastaiã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
    "text": "fastaiã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nè‡ªåˆ†ã®ãƒã‚·ãƒ³ã¸ã®fastaiã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯æœ¬å®¶ã‚µã‚¤ãƒˆã‚’å‚ç…§ã•ã‚ŒãŸã„ï¼\nGoogle Colabä¸Šã«ã¯ã™ã§ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã®ã§ï¼Œä»¥ä¸‹ã®æ“ä½œã ã‘ã‚’è¡Œãˆã°è‰¯ã„ï¼\n\nä¸Šéƒ¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ /ãƒ©ãƒ³ã‚¿ã‚¤ãƒ—ã®ç¨®é¡ã‚’å¤‰æ›´ã§GPUã‚’ã‚ªãƒ³ã«ã™ã‚‹ï¼\n\nå‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸGPUã‚’ï¼Œä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ç¢ºèªã—ã¦ãŠãï¼\n\n!nvidia-smi\n\nTue Aug 23 01:10:22 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   65C    P0    31W /  70W |   2084MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#mnist_sample",
    "href": "16fastai.html#mnist_sample",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "MNIST_SAMPLE",
    "text": "MNIST_SAMPLE\næ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹ â€Hello Worldâ€ ã¯ã€MNISTã®æ‰‹æ›¸ãæ–‡å­—èªè­˜ã§ã‚ã‚‹ã€‚ã“ã“ã§ã¯ã€ã•ã‚‰ã«ç°¡å˜ãªMNISTã®ä¸€éƒ¨ï¼ˆ\\(ï¼“\\)ã¨\\(ï¼—\\)ã ã‘ã®ç”»åƒï¼‰ã‚’èªè­˜ã™ã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã€‚ ã“ã‚Œã¯ã€2å€¤åˆ†é¡å•é¡Œã¨å‘¼ã°ã‚Œã€ä¼¼ãŸä¾‹ã‚’ã‚ã’ã‚‹ã¨ï¼Œä¸ãˆã‚‰ã‚ŒãŸå†™çœŸã«çŒ«ãŒå†™ã£ã¦ã„ã‚‹ã‹å¦ã‹ï¼Œå—ã‘å–ã£ãŸãƒ¡ã‚¤ãƒ«ãŒã‚¹ãƒ‘ãƒ ã‹å¦ã‹ï¼Œãªã©ã‚’åˆ¤å®šã™ã‚‹ã“ã¨ãŒã‚ã’ã‚‰ã‚Œã‚‹ã€‚ 2å€¤åˆ†é¡å•é¡Œã¯ã€ç‹¬ç«‹å¤‰æ•°ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å…¥åŠ›ï¼Œç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«å¯¾ã™ã‚‹å¾“å±å¤‰æ•°ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰ãŒ \\(0\\)ã‹\\(1\\)ã®å€¤ã‚’ã¨ã‚‹å•é¡Œã§ã‚ã‚‹ã¨è¨€ãˆã‚‹ï¼\nã“ã®ç°¡å˜ãªä¾‹ã‚’ç”¨ã„ã¦ã€fastaiã‚’ç”¨ã„ãŸè¨“ç·´ (training) ã®ã‚³ãƒ„ã‚’ä¼æˆã™ã‚‹ï¼\nã¾ãšã€fastaiã§æº–å‚™ã•ã‚Œã¦ã„ã‚‹MNIST_SAMPLEã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼\npathã¯ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰åã§ã‚ã‚Šã€dlsã¯ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã¨åä»˜ã‘ã‚‰ã‚ŒãŸç”»åƒç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ImageDataLoader)ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚ã‚‹ã€‚\nãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã«ã¯ï¼Œæ§˜ã€…ãªãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰ãŒã‚ã‚‹ï¼ã“ã“ã§ã¯ï¼Œãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç”Ÿæˆã™ã‚‹from_folderãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST_SAMPLE)\ndls = ImageDataLoaders.from_folder(path)\n\ndoc()ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã¿ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n\ndoc(ImageDataLoaders)\n\n\nImageDataLoaders\nImageDataLoaders(*loaders, path:str|Path='.', device=None)Basic wrapper around several `DataLoader`s with factory methods for computer vision problems\nShow in docs\n\n\nèª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®1ãƒãƒƒãƒåˆ†ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®show_batchãƒ¡ã‚½ãƒƒãƒ‰ã§ã¿ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n\ndls.show_batch()\n\n\n\n\n\n\n\n\nç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ï¼Œãƒ‡ãƒ¼ã‚¿ã¨ã‚ã‚ã›ã¦å­¦ç¿’å™¨ learn ã‚’ç”Ÿæˆã™ã‚‹ï¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆè©•ä¾¡å°ºåº¦ï¼‰ã¯error_rateã‚’æŒ‡å®šã—ã¦ãŠãï¼\nå­¦ç¿’å™¨ã¯resnet34ã‚’ç”¨ã„ï¼Œå­¦ç¿’æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸè»¢ç§»å­¦ç¿’ã‚’è¡Œã†ï¼\n\nResNet\nResNetã¯æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã¨ã‚ˆã°ã‚Œã‚‹å±¤ã®å›ºã¾ã‚Šã‚’ï¼Œä½•å±¤ã«ã‚‚é‡ã­ãŸã‚‚ã®ã§ã‚ã‚‹ï¼æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯ï¼Œãƒ–ãƒ­ãƒƒã‚¯ã¸ã®å…¥åŠ›ï¼Œç·šå½¢å±¤ï¼ŒReLU(rectified linear unit; \\(\\max (0,x)\\))ï¼Œç·šå½¢å±¤ã®æµã‚Œã«ï¼Œå…¥åŠ›ãã®ã‚‚ã®ã‚’åŠ ãˆãŸã‚‚ã®ã«ï¼ŒReLUã‚’è¡Œã†ã“ã¨ã«ã‚ˆã£ã¦å‡ºåŠ›ã‚’å¾—ã‚‹ï¼ å…¥åŠ›ã‚’ãã®ã¾ã¾æœ€çµ‚ã®æ´»æ€§åŒ–é–¢æ•°ã®å‰ã«ç¹‹ã’ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ï¼Œå¿…è¦ã®ãªã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’è·³ã°ã—ã¦è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šï¼Œã“ã‚Œã«ã‚ˆã£ã¦å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ç™ºç”Ÿã™ã‚‹å‹¾é…æ¶ˆå¤±ã‚„å‹¾é…çˆ†ç™ºã‚’é¿ã‘ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚‹ï¼ æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã¯ï¼Œç•³ã¿è¾¼ã¿å±¤ã®é–“ã«ã€Œè¿‘é“ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆï¼‰ã€ã‚’å…¥ã‚ŒãŸã‚‚ã®ã«ä»–ãªã‚‰ãªã„ï¼ã“ã®ã€Œè¿‘é“ã€ã‚’å…¥ã‚Œã‚‹ã“ã¨ã«ã‚ˆã£ã¦ï¼Œæœ€é©åŒ–ãŒæ¥½ã«ãªã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ï¼å±€æ‰€è§£ãŒæ¸›å°‘ã—ï¼Œ æ»‘ã‚‰ã‹ãªç©ºé–“ï¼ˆãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ï¼‰ã§ã®æœ€é©åŒ–ã«ãªã‚‹ã®ã ï¼\næ®‹å·®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’å™¨learnã‚’ä½œæˆã—ã¦ã‹ã‚‰learn.summaryã‚’ã¿ã‚‹ã¨ã€ãã®æ§‹é€ ãŒã‚ã‹ã‚‹ã€‚ ä»¥ä¸‹ã§ç”¨ã„ã‚‹resnet34ã¯34å±¤ã®å¤§è¦æ¨¡ãªç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã‚ã‚‹ã€‚å®Ÿè¡Œã™ã‚‹ã¨ã€å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ãŒèª­ã¿è¾¼ã¾ã‚Œï¼Œã“ã®é‡ã¿ã‚’ã‚‚ã¨ã«è»¢ç§»å­¦ç¿’ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ï¼\n\n\nè»¢ç§»å­¦ç¿’\né€šå¸¸ã®è¨“ç·´ã«ãŠã„ã¦ã¯ï¼ŒåˆæœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿ï¼‰ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«è¨­å®šã•ã‚Œã‚‹ï¼ã—ã‹ã—ï¼Œãƒ©ãƒ³ãƒ€ãƒ ãªåˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’ã¯ï¼Œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¤§è¦æ¨¡ã«ãªã‚‹ã¨è†¨å¤§ãªæ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼ãã“ã§ï¼Œç‰¹å®šã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¯¾ã—ã¦ï¼Œäº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿ï¼‰ã‚’ç”¨ã„ã‚‹ã“ã¨ãŒè¡Œã‚ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ããŸï¼ã“ã‚ŒãŒè»¢ç§»å­¦ç¿’ (transfer learning) ã§ã‚ã‚‹ï¼\nå¤šå±¤ã®ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ç™ºç”Ÿã‚’ç”¨ã„ã¦ç”»åƒã®åˆ†é¡ã‚’ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã‚ˆã†ï¼å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œã¦ï¼Œæœ€åˆã®æ–¹ã®å±¤ã§ã¯ç·šã‚„è§’ãªã©ã®ç°¡å˜ãªå½¢çŠ¶ã‚’æŠ½å‡ºã™ã‚‹ã‚ˆã†ã«ãªã‚Šï¼Œå±¤ãŒæ·±ã¾ã‚‹ã«ã¤ã‚Œã¦å¾ã€…ã«è¤‡é›‘ãªå½¢çŠ¶ã‚’å­¦ç¿’ã™ã‚‹ã‚ˆã†ã«ãªã‚‹ï¼ãŸã¨ãˆã°ï¼ŒçŒ«ã®ãµã‚ãµã‚ã—ãŸæ¯›ã«åå¿œã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚„ï¼ŒçŒ«ã®ç›®ã«åå¿œã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå‡ºã¦ãã‚‹ï¼æœ€çµ‚å±¤ã®ç›´å‰ã§ã¯ï¼Œåˆ†é¡ã—ãŸã„ç‰©ã®ç‰¹å¾´ã‚’æŠ½å‡ºã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒã‚ã‚‹ï¼è»¢ç§»å­¦ç¿’ã§ã¯ï¼Œä»–ã®ç›®çš„ã®ãŸã‚ã«è¨“ç·´ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”¨ã„ï¼Œåˆ¤åˆ¥ã‚’è¡Œã†æœ€çµ‚å±¤ã ã‘ã«å¯¾ã—ã¦è¨“ç·´ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ï¼‰ã‚’è¡Œã†ï¼ç·šã‚„è§’ã®åˆ¤åˆ¥ã¯å…±é€šã§ã‚ã‚‹ãŒï¼Œæœ€çµ‚çš„ãªåˆ†é¡ã¯ï¼Œå¯¾è±¡ã¨ã™ã‚‹ã‚‚ã®ã«ä¾å­˜ã—ã¦å†è¨“ç·´ã‚’ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã‹ã‚‰ã ï¼\næœ€çµ‚å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒååˆ†ã«è¨“ç·´ã•ã‚ŒãŸã‚‰ï¼Œä¸Šå±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚‚è¨“ç·´ã‚’è¡Œã†æ–¹ãŒè‰¯ã„ï¼fine_tuneãƒ¡ã‚½ãƒƒãƒ‰ã¯ï¼Œã“ã‚Œã‚’è‡ªå‹•çš„ã«ã—ã¦ãã‚Œã‚‹ï¼\n\n\nå­¦ç¿’ç‡ã®èª¿æ•´\næ·±å±¤å­¦ç¿’ã§æœ€ã‚‚é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ï¼Œå­¦ç¿’ç‡(learning rate: lrã¨ç•¥ã•ã‚Œã‚‹ï¼‰ã§ã‚ã‚‹ï¼æ·±å±¤å­¦ç¿’ã§ã¯ï¼Œé‡ã¿ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰èª¿æ•´ã®ãŸã‚ã«éç·šå½¢æœ€é©åŒ–ã‚’è¡Œã†ï¼\nã¤ã¾ã‚Šï¼Œå‹¾é…ã«é©å½“ãªã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’ä¹—ã˜ã¦ç¾åœ¨ã®å€¤ã‹ã‚‰æ¸›ã˜ã‚‹æ“ä½œã‚’ç¹°ã‚Šè¿”ã™ï¼ã“ã®éç·šå½¢æœ€é©åŒ–ã«ãŠã‘ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã®ã“ã¨ã‚’ï¼Œå­¦ç¿’ç‡ã¨å‘¼ã‚“ã§ã„ã‚‹ï¼\nã“ã‚Œã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ï¼Œfastaiã§ã¯å­¦ç¿’å™¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«lr_find() ã¨ã„ã†ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æº–å‚™ã—ã¦ã„ã‚‹ï¼\nè©•ä¾¡å°ºåº¦(metricsï¼‰ã«èª¤å·®ç‡ã‚’æŒ‡å®šã—ãŸå­¦ç¿’å™¨learnã‚’ä½œæˆã—ã¦learn.lr_find()ã¨ã™ã‚‹ï¼\nlr_findã¯ï¼Œå­¦ç¿’ç‡ã‚’å°ã•ãªå€¤ã‹ã‚‰1åå¾©ã”ã¨ã«2å€ã«ã—ãŸã¨ãã®æå‡ºé–¢æ•°ï¼ˆç›®çš„é–¢æ•°ã®ã“ã¨ï¼‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ãã‚Œã‚‹ï¼ ç›®å®‰ã ãŒï¼Œæœ€å°å€¤ã‚’ã‚‚ã¤è°·ã«å…¥ã‚‹ã‚ãŸã‚Šã®å­¦ç¿’ç‡ãŒè‰¯ã„ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ï¼\n\nlearn = vision_learner(dls,resnet34, metrics=error_rate, cbs=ShowGraphCallback())\nlearn.lr_find()\n\n/Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /Users/mikiokubo/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:02&lt;00:00, 40.8MB/s]\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0020892962347716093)\n\n\n\n\n\n\n\n\n\næå‡ºé–¢æ•°ãŒæœ€å°ã«ãªã‚‹ã®ã¯ï¼Œå­¦ç¿’ç‡ãŒ0.2ã‚ãŸã‚Šã ãŒï¼Œæœ€ã‚‚å¤§ããªè°·ã®ä¸‹ã‚Šå‚ã«å…¥ã‚‹ã‚ãŸã‚ŠãŒè‰¯ã„ã¨ã•ã‚Œã¦ã„ã‚‹ï¼ã“ã“ã§ã¯ï¼Œå­¦ç¿’ç‡ã‚’1e-2 (0.01)ã«è¨­å®šã—ã¦è¨“ç·´ã—ã¦ã¿ã‚‹ï¼\nã“ã‚Œã«ã¯å­¦ç¿’å™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®fit_tuneãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼å¼•æ•°ã¯ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆæœ€é©åŒ–ã®åå¾©å›æ•°ï¼›ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’ä½•å›ä½¿ã†ã‹ã‚’è¡¨ã™ï¼‰ã¨å­¦ç¿’ç‡ã§ã‚ã‚‹ï¼\nãªãŠï¼Œå®Ÿéš›ã®åå¾©ã”ã¨ã®å­¦ç¿’ç‡ã¯ï¼Œå­¦ç¿’å™¨ã®cbså¼•æ•°ã‚’ShowGraphCallback()ã¨ã™ã‚‹ã¨ï¼Œè¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n\ndoc(learn.fine_tune)\n\nLearner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, *, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, start_epoch=0)\nFine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\n\nTo get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev\n\n\n\nlearn.fine_tune(2, base_lr=0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.200721\n0.140300\n0.038273\n00:14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.034825\n0.004138\n0.001472\n00:24\n\n\n1\n0.005775\n0.003441\n0.000981\n00:21\n\n\n\n\n\n\n\n\n\n\n\n\nè©•ä¾¡å°ºåº¦ã®èª¤å·®ç‡ã¯éå¸¸ã«å°ã•ãã€å›³ã‹ã‚‰è¨“ç·´ã¯ã†ã¾ãè¡Œã‚ã‚Œã¦ã„ã‚‹ã‚ˆã†ã ã€‚ çµæœã‚’è¡¨ç¤ºã—ã¦ã¿ã‚‹ã€‚å¤§ä½“å½“ãŸã£ã¦ã„ã‚‹ã‚ˆã†ã ã€‚\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfine_tuneã§ã¯ã€æœ€çµ‚å±¤ä»¥å¤–ã‚’å›ºå®šã—ã¦ï¼ˆæ—¢å®šå€¤ã§ã¯ï¼‘å›ï¼‰è¨“ç·´ã‚’è¡Œã„ã€ãã®å¾Œã€fit_one_cycleã‚’ç”¨ã„ã¦ã€æŒ‡å®šã—ãŸã‚¨ãƒãƒƒã‚¯æ•°ã ã‘è¨“ç·´ã™ã‚‹ã€‚ fit_one_cycleã¯ï¼Œå­¦ç¿’ç‡ã‚’å°ã•ãªå€¤ã‹ã‚‰æœ€å¤§å­¦ç¿’ç‡ã¾ã§å¢—ã‚„ã—ï¼Œãã®å¾Œå¾ã€…ã«æ¸›å°‘ã•ã›ã¦ã„ãï¼åŒæ™‚ã«ï¼Œæ…£æ€§é …ã‚’å¾ã€…ã«ä¸‹ã’ã¦ï¼Œãã®å¾Œå¢—åŠ ã•ã›ã¦ã„ãæœ€é©åŒ–æ³•ã§ï¼Œã“ã‚Œã‚’ä½¿ã†ã¨åæŸãŒé€Ÿããªã‚‹ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ï¼\nfine_tuneãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°ã¯ã‚¨ãƒãƒƒã‚¯æ•°ã¨åŸºæœ¬å­¦ç¿’ç‡ base_lr ã§ã‚ã‚‹ï¼\nåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’è§£é‡ˆã¯ã€ClassificationInterpretation()ã‚¯ãƒ©ã‚¹ã®from_learnerãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã¦ã§ãã‚‹ã€‚ plot_top_lossesã‚’ç”¨ã„ã‚‹ã¨ï¼Œæå‡ºé–¢æ•°ãŒæ‚ªã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚’æç”»ã—ã¦ãã‚Œã‚‹ï¼ å¼•æ•°ã¯ç”»åƒæ•°ã¨ç”»åƒã®ã‚µã‚¤ã‚ºã§ã‚ã‚‹ï¼\n\ninterp = ClassificationInterpretation.from_learner(learn)\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(9, figsize=(7,7))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\næ­£è§£ã¨å¤–ã‚Œã‚’è¡¨ã™è¡¨ï¼ˆæ··åŒè¡Œåˆ—ã¨ã‚ˆã°ã‚Œã‚‹ï¼‰ã‚’å‡ºåŠ›ã™ã‚‹ã«ã¯ï¼Œplot_confusion_matrixã‚’ä½¿ã†ï¼\n\ninterp.plot_confusion_matrix()",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#cifar10",
    "href": "16fastai.html#cifar10",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "Cifar10",
    "text": "Cifar10\nCifar10ã¯ç²—ã„ç”»åƒã‹ã‚‰ï¼Œ10ç¨®é¡ã®ç‰©ä½“ã‚’å½“ã¦ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹ï¼\nImageDataLoaderã®from_forder()ãƒ¡ã‚½ãƒƒãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ï¼ æ¤œè¨¼ï¼ˆãƒ†ã‚¹ãƒˆï¼‰ãƒ‡ãƒ¼ã‚¿ã¯10%ã«è¨­å®šã™ã‚‹ï¼\n\n\npath = untar_data(URLs.CIFAR)\n\n\n\n\n\n\n    \n      \n      100.00% [168173568/168168549 00:01&lt;00:00]\n    \n    \n\n\n\ndls = ImageDataLoaders.from_folder(path,valid_pct=0.1)\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet50, metrics=[error_rate,accuracy])\nlr= learn.lr_find() \nprint(lr)\n\n/usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.001737800776027143)\n\n\n\n\n\n\n\n\n\nãƒ‡ãƒ¼ã‚¿ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒ¢ãƒ‡ãƒ«ï¼šRESNETï¼‰ã‚’ã‚ã‚ã›ã¦å­¦ç¿’å™¨ã‚’ç”Ÿæˆã™ã‚‹ï¼\nãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯æ­£è§£ç‡(accuracy)ã¨ã™ã‚‹ï¼\n\nlearn.fine_tune(10, base_lr=1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n1.720446\n1.487195\n0.504833\n0.495167\n01:36\n\n\n\n\n\n\n\n\n\n\n    \n      \n      10.00% [1/10 01:36&lt;14:28]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n1.095862\n0.945130\n0.328167\n0.671833\n01:36\n\n\n\n\n\n    \n      \n      56.35% [475/843 00:49&lt;00:38 0.8759]\n    \n    \n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n1.095862\n0.945130\n0.328167\n0.671833\n01:36\n\n\n1\n0.819684\n0.715804\n0.244000\n0.756000\n01:33\n\n\n2\n0.614426\n0.609761\n0.205833\n0.794167\n01:33\n\n\n3\n0.429096\n0.574650\n0.193333\n0.806667\n01:34\n\n\n4\n0.293377\n0.609565\n0.186167\n0.813833\n01:34\n\n\n5\n0.157066\n0.681249\n0.180667\n0.819333\n01:34\n\n\n6\n0.088063\n0.761130\n0.183500\n0.816500\n01:34\n\n\n7\n0.047697\n0.804281\n0.181500\n0.818500\n01:34\n\n\n8\n0.023899\n0.817061\n0.181333\n0.818667\n01:34\n\n\n9\n0.026840\n0.833239\n0.180333\n0.819667\n01:33\n\n\n\n\n\næå‡ºé–¢æ•°ã®å¤§ãã„é †ã«5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã™ã‚‹ï¼\n\ninterp = Interpretation.from_learner(learn)\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5)",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#pets",
    "href": "16fastai.html#pets",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "PETS",
    "text": "PETS\nç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çŠ¬ã‹çŒ«ã‹ã‚’åˆ¤åˆ¥ã™ã‚‹ï¼\nãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚­ã‚¯ãƒãƒ£ï¼‰ã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ResNetã‚’ç”¨ã„ã‚‹ï¼\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.PETS)\npath\n\nPath('/Users/mikiokubo/.fastai/data/oxford-iiit-pet')\n\n\n\npath.ls()\n\n(#2) [Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/annotations')]\n\n\n\npath_anno = path/\"annotations\"\npath_img = path/\"images\"\n\n\nfnames = get_image_files(path_img)\nfnames[:5]\n\n(#5) [Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/pug_52.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/basset_hound_112.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/Siamese_193.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/shiba_inu_122.jpg')]\n\n\n\nfiles = get_image_files(path/\"images\")\nlen(files)\n\n7390\n\n\nçŠ¬ã‹çŒ«ã‹ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®æœ€åˆã®æ–‡å­—ãŒå¤§æ–‡å­—ã‹å°æ–‡å­—ã‹ã§åˆ¤åˆ¥ã§ãã‚‹ï¼\nImageDataLoadersã‚¯ãƒ©ã‚¹ã®from_name_func()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ï¼\nå¼•æ•°ã¯é †ã«ï¼Œ\n\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ path\nãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ files\nãƒ©ãƒ™ãƒ«åã‚’åˆ¤å®šã™ã‚‹é–¢æ•° label_func\nãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆã“ã“ã§ã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã®å¤‰æ›´ï¼‰ item_tfms\n\nã§ã‚ã‚‹ï¼\n\ndef label_func(f): return f[0].isupper() #çŠ¬çŒ«ã®åˆ¤å®š\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n/usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.156178\n0.016154\n0.004060\n00:52\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.054338\n0.005073\n0.001353\n00:54\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nä»Šåº¦ã¯ï¼ŒåŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ï¼Œ\\(37\\)ç¨®é¡ã®PETã®ç¨®é¡ã‚’åˆ¤åˆ¥ã™ã‚‹ï¼\nãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«ã¯æ­£è¦è¡¨ç¾ã‚’ç”¨ã„ã‚‹ï¼\nImageDataLoaderã‚¯ãƒ©ã‚¹ã®from_name_re()ãƒ¡ã‚½ãƒƒãƒ‰ã¯ï¼Œæ­£è¦è¡¨ç¾ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ï¼\nå¼•æ•°ã¯é †ã«ï¼Œ\n\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ path\nãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ files\nã‚¯ãƒ©ã‚¹åã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æŠ½å‡ºã™ã‚‹ãŸã‚ã®æ­£è¦è¡¨ç¾ pat\nãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆã“ã“ã§ã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã®å¤‰æ›´ï¼‰ item_tfms\naug_transformsã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å¢—å¤§ batch_tfms\n\nã§ã‚ã‚‹ï¼\n\npat = r\"^(.*)_\\d+.jpg\"\ndls = ImageDataLoaders.from_name_re(path, files, pat, item_tfms=Resize(460),\n                                    batch_tfms=aug_transforms(size=224))\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(4, 0.001)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.070160\n0.406866\n0.123139\n01:07\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.576859\n0.279217\n0.085250\n01:10\n\n\n1\n0.409450\n0.238351\n0.067659\n01:10\n\n\n2\n0.260191\n0.211842\n0.070365\n01:10\n\n\n3\n0.193102\n0.204086\n0.064953\n01:10\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp = Interpretation.from_learner(learn)\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(9, figsize=(15,10))",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿",
    "href": "16fastai.html#è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿",
    "text": "è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿\n\nfrom fastai.tabular.all import *\n\n\nä¾‹é¡Œï¼š ã‚µãƒ©ãƒªãƒ¼ã®åˆ†é¡\nADULT_SAMPLEã¯ï¼Œå°è¦æ¨¡ãªè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚Šï¼Œ$50kä»¥ä¸Šã®åå…¥ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’å½“ã¦ã‚‹ã®ãŒç›®çš„ã ï¼\n\npath = untar_data(URLs.ADULT_SAMPLE)\npath\n\nPath('/Users/mikiokubo/.fastai/data/adult_sample')\n\n\n\ndf = pd.read_csv(path / \"adult.csv\")\ndf.head().T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nage\n49\n44\n38\n38\n42\n\n\nworkclass\nPrivate\nPrivate\nPrivate\nSelf-emp-inc\nSelf-emp-not-inc\n\n\nfnlwgt\n101320\n236746\n96185\n112847\n82297\n\n\neducation\nAssoc-acdm\nMasters\nHS-grad\nProf-school\n7th-8th\n\n\neducation-num\n12.0\n14.0\nNaN\n15.0\nNaN\n\n\nmarital-status\nMarried-civ-spouse\nDivorced\nDivorced\nMarried-civ-spouse\nMarried-civ-spouse\n\n\noccupation\nNaN\nExec-managerial\nNaN\nProf-specialty\nOther-service\n\n\nrelationship\nWife\nNot-in-family\nUnmarried\nHusband\nWife\n\n\nrace\nWhite\nWhite\nBlack\nAsian-Pac-Islander\nBlack\n\n\nsex\nFemale\nMale\nFemale\nMale\nFemale\n\n\ncapital-gain\n0\n10520\n0\n0\n0\n\n\ncapital-loss\n1902\n0\n0\n0\n0\n\n\nhours-per-week\n40\n45\n32\n40\n50\n\n\nnative-country\nUnited-States\nUnited-States\nUnited-States\nUnited-States\nUnited-States\n\n\nsalary\n&gt;=50k\n&gt;=50k\n&lt;50k\n&gt;=50k\n&lt;50k\n\n\n\n\n\n\n\nè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã¯ TabularDataLoaders ã§ã‚ã‚Šï¼Œã“ã‚Œã¯from_csvãƒ¡ã‚½ãƒƒãƒ‰ã‚„from_dfã‚’ç”¨ã„ã¦ä½œã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\nfrom_csvã®ä¸»ãªå¼•æ•°ã®æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šï¼\n\ncsv: csvãƒ•ã‚¡ã‚¤ãƒ«\npath: ãƒ•ã‚¡ã‚¤ãƒ«ã®ç½®ãå ´æ‰€\ny_names: å¾“å±å¤‰æ•°ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰ã®åˆ—åï¼ˆã®ãƒªã‚¹ãƒˆï¼‰\nvalid_idx: æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\nproc: å‰å‡¦ç†ã®æ–¹æ³•ã‚’å…¥ã‚ŒãŸãƒªã‚¹ãƒˆ\ncat_names: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã®ãƒªã‚¹ãƒˆ\ncont_names: é€£ç¶šé‡ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã®ãƒªã‚¹ãƒˆ\nã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨é€£ç¶šé‡ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•çš„ã«åˆ†ã‘ã¦ãã‚Œã‚‹ä»¥ä¸‹ã®é–¢æ•°ã‚‚æº–å‚™ã•ã‚Œã¦ã„ã‚‹ï¼\n\ncont_names, cat_names = cont_cat_split(df=ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ , dep_var=å¾“å±å¤‰æ•°ã®åˆ—å)\n\nprocs: å‰å‡¦ç†ã®æŒ‡å®š\n\nå‰å‡¦ç†ã«ã¯ä»¥ä¸‹ã®ã‚‚ã®ãŒã‚ã‚‹ï¼\n\nCategorify: cat_nameså¼•æ•°ã§ä¸ãˆãŸåˆ—ãƒªã‚¹ãƒˆã‚’ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã¨ã™ã‚‹ï¼\nFillMissingï¼š cont_namesã«å«ã¾ã‚Œã‚‹é€£ç¶šå¤‰æ•°ã«å¯¾ã—ã¦æ¬ æå€¤å‡¦ç†ã‚’è¡Œã†ï¼\n\nå¼•æ•°ã®FillStrategyã«ã¯[MEDIAN, COMMON, CONSTANT]ãŒã‚ã‚Šï¼Œé †ã«ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ³ï¼Œæœ€é »å€¤ï¼Œå®šæ•°ï¼ˆfill_valã§æŒ‡å®šï¼‰ã§ã‚ã‚‹ï¼ ã¾ãŸï¼Œadd_colå¼•æ•°ãŒTrueã®ã¨ãã«ã¯ï¼Œæ¬ æå€¤ã§ã‚ã‚‹ã“ã¨ã‚’è¡¨ã™åˆ—ã‚’è¿½åŠ ã™ã‚‹ï¼\n\nNormalize: cont_namesã«å«ã¾ã‚Œã‚‹é€£ç¶šå¤‰æ•°ã®æ­£è¦åŒ–ã‚’è¡Œã†ï¼(å¹³å‡ã‚’å¼•ã„ã¦æ¨™æº–åå·®+å¾®å°‘é‡ã§å‰²ã‚‹ï¼ï¼‰\n\næ™‚åˆ»å‹ã®åˆ—ã‚’è‡ªå‹•çš„ã«å¹¾ã¤ã‹ã®å¤‰æ•°ã«å¤‰æ›ã™ã‚‹ä»¥ä¸‹ã®é–¢æ•°ãŒæº–å‚™ã•ã‚Œã¦ã„ã‚‹ï¼\nadd_datepart(df, fldname, drop=True, time=False)\nfldnameã¯æ™‚åˆ»å‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹åˆ—åã§ã‚ã‚Šï¼ŒdropãŒTrueã®ã¨ãå…ƒã®åˆ—ã‚’å‰Šé™¤ã™ã‚‹ï¼ã¾ãŸtimeãŒTrueã®ã¨ãã«ã¯ï¼Œæ—¥ä»˜ã ã‘ã§ãªãæ™‚ï¼Œåˆ†ï¼Œç§’ã®åˆ—ã‚‚è¿½åŠ ã™ã‚‹ï¼\n\ndls = TabularDataLoaders.from_csv(\n    path / \"adult.csv\",\n    path=path,\n    y_names= \"salary\",\n    cat_names=[\n        \"workclass\",\n        \"education\",\n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n    ],\n    cont_names=[\"age\", \"fnlwgt\", \"education-num\"],\n    procs=[Categorify, FillMissing, Normalize],\n)\n\n\ndls.show_batch()\n\n\n\n\n\nworkclass\neducation\nmarital-status\noccupation\nrelationship\nrace\neducation-num_na\nage\nfnlwgt\neducation-num\nsalary\n\n\n\n\n0\nState-gov\nHS-grad\nMarried-civ-spouse\nCraft-repair\nHusband\nWhite\nFalse\n58.000001\n200315.999803\n9.0\n&lt;50k\n\n\n1\nPrivate\nAssoc-voc\nNever-married\nTransport-moving\nOwn-child\nWhite\nFalse\n34.000000\n124827.001701\n11.0\n&lt;50k\n\n\n2\nPrivate\nBachelors\nMarried-civ-spouse\nTech-support\nHusband\nWhite\nFalse\n54.000000\n171924.000267\n13.0\n&lt;50k\n\n\n3\nPrivate\nHS-grad\nMarried-civ-spouse\nTransport-moving\nHusband\nBlack\nFalse\n50.000000\n378746.998755\n9.0\n&lt;50k\n\n\n4\nPrivate\nSome-college\nNever-married\nAdm-clerical\nOwn-child\nWhite\nFalse\n25.000000\n60485.001827\n10.0\n&lt;50k\n\n\n5\nState-gov\n11th\nMarried-civ-spouse\nTransport-moving\nHusband\nWhite\nFalse\n30.000000\n54318.006147\n7.0\n&lt;50k\n\n\n6\n?\nSome-college\nMarried-civ-spouse\n?\nHusband\nWhite\nFalse\n56.999999\n296516.003827\n10.0\n&lt;50k\n\n\n7\nPrivate\nHS-grad\nDivorced\nExec-managerial\nNot-in-family\nWhite\nFalse\n39.000000\n188540.000017\n9.0\n&lt;50k\n\n\n8\nPrivate\nBachelors\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nFalse\n26.000000\n391349.005619\n13.0\n&lt;50k\n\n\n9\nPrivate\nHS-grad\nMarried-civ-spouse\nAdm-clerical\nWife\nWhite\nFalse\n25.000000\n303430.997391\n9.0\n&lt;50k\n\n\n\n\n\n\ncont_names, cat_names = cont_cat_split(df, max_card=50, dep_var=\"salary\")\ncat_names\n\n['workclass',\n 'education',\n 'marital-status',\n 'occupation',\n 'relationship',\n 'race',\n 'sex',\n 'native-country']\n\n\ntabular_learneré–¢æ•°ã§è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®æ·±å±¤å­¦ç¿’å™¨ã‚’ä½œã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\nä¸»ãªå¼•æ•°ã®æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šï¼ - dls: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ - layers: ãƒ¬ã‚¤ãƒ¤ã®æ•°ã‚’æŒ‡å®šã—ãŸãƒªã‚¹ãƒˆ - emb_szs: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã‚’ã‚­ãƒ¼ï¼ŒåŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã‚’å€¤ã¨ã—ãŸè¾æ›¸ - metrics: è©•ä¾¡å°ºåº¦(accuracyãªã©ï¼‰ - emb_drop: åŸ‹ã‚è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ã®drop outç‡\n\n# æ·±å±¤å­¦ç¿’(PyTorch)ã®å­¦ç¿’å™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹learnã‚’ç”Ÿæˆã—ï¼Œfitãƒ¡ã‚½ãƒƒãƒ‰ã§è¨“ç·´ï¼å¼•æ•°ã¯ã‚¨ãƒãƒƒã‚¯æ•°ã¨å­¦ç¿’ç‡ï¼\nlearn = tabular_learner(dls, metrics=[accuracy])\n\n\nlearn.fit_one_cycle(3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.387215\n0.363688\n0.833077\n00:10\n\n\n1\n0.356962\n0.358292\n0.835534\n00:05\n\n\n2\n0.352825\n0.355646\n0.834613\n00:05\n\n\n\n\n\nsummaryå±æ€§ã‚’ã¿ã‚‹ã¨ï¼Œå­¦ç¿’å™¨ã¯ï¼ŒåŸ‹ã‚è¾¼ã¿å±¤ã«ç¶šã„ã¦2ã¤ã®ç·šå½¢å±¤ã‚’é…ç½®ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ï¼\n\nlearn.summary()\n\n\n\n\n\n\n\n\nTabularModel (Input shape: 64 x 7)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 6              \nEmbedding                                 60         True      \n____________________________________________________________________________\n                     64 x 8              \nEmbedding                                 136        True      \n____________________________________________________________________________\n                     64 x 5              \nEmbedding                                 40         True      \n____________________________________________________________________________\n                     64 x 8              \nEmbedding                                 128        True      \n____________________________________________________________________________\n                     64 x 5              \nEmbedding                                 35         True      \n____________________________________________________________________________\n                     64 x 4              \nEmbedding                                 24         True      \n____________________________________________________________________________\n                     64 x 3              \nEmbedding                                 9          True      \nDropout                                                        \nBatchNorm1d                               6          True      \n____________________________________________________________________________\n                     64 x 200            \nLinear                                    8400       True      \nReLU                                                           \nBatchNorm1d                               400        True      \n____________________________________________________________________________\n                     64 x 100            \nLinear                                    20000      True      \nReLU                                                           \nBatchNorm1d                               200        True      \n____________________________________________________________________________\n                     64 x 2              \nLinear                                    202        True      \n____________________________________________________________________________\n\nTotal params: 29,640\nTotal trainable params: 29,640\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nModel unfrozen\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n\n\n\n\nä¾‹é¡Œï¼šä½å®…ä¾¡æ ¼ã®äºˆæ¸¬\nBostonã®ä½å®…ä¾¡æ ¼ã®äºˆæ¸¬ã‚’æ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ãŸå›å¸°åˆ†æã§è¡Œã†ï¼\nmedvãŒä½å®…ã®ä¾¡æ ¼ã§ï¼Œä»–ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆçŠ¯ç½ªç‡ã‚„äººå£ãªã©ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰äºˆæ¸¬ã™ã‚‹ï¼\nãŸã ã—ï¼Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆtrain_idx,valid_idxï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ï¼Œ ä»¥ä¸‹ã«ç¤ºã™ã‚ˆã†ã«ï¼Œscikit-learnã®train_test_splitã‚’ç”¨ã„ã‚‹ï¼\né€£ç¶šãƒ‡ãƒ¼ã‚¿ã¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®åˆ—ã¯ï¼Œ cont_cat_splité–¢æ•°ã‚’ç”¨ã„ã‚‹ï¼ å¼•æ•°ã® max_card \\(=50\\) ã¯ \\(50\\)ä»¥ä¸‹ã®ç¨®é¡ã—ã‹ã‚‚ãŸãªã„åˆ—ã¯ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã¨ã¿ãªã™ã“ã¨ã‚’æ„å‘³ã™ã‚‹ï¼ ã¾ãŸï¼Œå¼•æ•°ã® dep_varã¯å¾“å±å¤‰æ•°åã§ã‚ã‚‹ï¼\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nboston = pd.read_csv(\"http://logopt.com/data/Boston.csv\",index_col=0)\nprocs = [Categorify, FillMissing, Normalize] #å‰å‡¦ç†ã®ç¨®é¡ã‚’æº–å‚™ï¼\ntrain_idx, valid_idx = train_test_split(range(len(boston)), test_size=0.3) #æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æº–å‚™ï¼\ndep_var = \"medv\" #å¾“å±å¤‰æ•°åã‚’æº–å‚™ï¼\n\ncont_names, cat_names = cont_cat_split(boston, max_card = 50, dep_var=dep_var)\nprint(cat_names, cont_names)\n\n['chas', 'rad'] ['crim', 'zn', 'indus', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'black', 'lstat']\n\n\næº–å‚™ãŒã§ããŸã®ã§ï¼Œ TabularDataLoadersã®from_dfãƒ¡ã‚½ãƒƒãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ç”Ÿæˆã—ï¼Œãã‚Œã‚’ã‚‚ã¨ã«tabular_learneré–¢æ•°ã§å­¦ç¿’å™¨ learn ã‚’ä½œã‚‹ï¼ è©•ä¾¡å°ºåº¦(metrics)ã«ã¯ rmse ï¼ˆrooted mean square error)ã‚’ç”¨ã„ã‚‹ï¼\nfit_one_cycleæ³•ã‚’ç”¨ã„ã¦ï¼Œ 30ã‚¨ãƒãƒƒã‚¯ï¼Œæœ€å¤§å­¦ç¿’ç‡0.001ã§è¨“ç·´ã™ã‚‹ï¼\n\ndls = TabularDataLoaders.from_df(boston, y_names=dep_var, procs = procs, cont_names=cont_names, cat_names=cat_names)\nlearn = tabular_learner(dls, metrics=rmse)\nlearn.fit_one_cycle(30,1e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n_rmse\ntime\n\n\n\n\n0\n583.483521\n612.155334\n24.741774\n00:00\n\n\n1\n581.810120\n611.932373\n24.737268\n00:00\n\n\n2\n578.824707\n608.098999\n24.659664\n00:00\n\n\n3\n574.245300\n598.170288\n24.457520\n00:00\n\n\n4\n567.828918\n579.099182\n24.064480\n00:00\n\n\n5\n556.611816\n545.330933\n23.352322\n00:00\n\n\n6\n537.237671\n484.291077\n22.006615\n00:00\n\n\n7\n508.969635\n396.657959\n19.916273\n00:00\n\n\n8\n469.514343\n293.581970\n17.134233\n00:00\n\n\n9\n424.950714\n204.514008\n14.300838\n00:00\n\n\n10\n377.579315\n132.951584\n11.530462\n00:00\n\n\n11\n334.218781\n91.368095\n9.558666\n00:00\n\n\n12\n295.503296\n62.303730\n7.893271\n00:00\n\n\n13\n261.120300\n45.621571\n6.754374\n00:00\n\n\n14\n231.821823\n36.233948\n6.019464\n00:00\n\n\n15\n205.355896\n31.576481\n5.619295\n00:00\n\n\n16\n182.979019\n28.494129\n5.337989\n00:00\n\n\n17\n163.683594\n26.984529\n5.194664\n00:00\n\n\n18\n146.885498\n25.802696\n5.079636\n00:00\n\n\n19\n132.069229\n23.790033\n4.877503\n00:00\n\n\n20\n119.163567\n23.443287\n4.841827\n00:00\n\n\n21\n107.618279\n22.275869\n4.719732\n00:00\n\n\n22\n97.619194\n21.489685\n4.635697\n00:00\n\n\n23\n88.492821\n20.707258\n4.550523\n00:00\n\n\n24\n80.453087\n20.948822\n4.576989\n00:00\n\n\n25\n73.967003\n20.161036\n4.490104\n00:00\n\n\n26\n68.017082\n20.298531\n4.505389\n00:00\n\n\n27\n62.846817\n20.439318\n4.520987\n00:00\n\n\n28\n58.536152\n20.634672\n4.542541\n00:00\n\n\n29\n53.941723\n20.251295\n4.500144\n00:00\n\n\n\n\n\n\n\nå•é¡Œï¼ˆã‚¹ãƒ‘ãƒ ï¼‰\nãƒ¡ãƒ¼ãƒ«ãŒã‚¹ãƒ‘ãƒ ï¼ˆspamï¼›è¿·æƒ‘ãƒ¡ã‚¤ãƒ«ï¼‰ã‹å¦ã‹ã‚’ï¼Œæ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ã¦åˆ¤å®šã›ã‚ˆï¼\nãƒ‡ãƒ¼ã‚¿ã¯ï¼Œæ§˜ã€…ãªæ•°å€¤æƒ…å ±ã‹ã‚‰ï¼Œis_spamåˆ—ãŒ1 ï¼ˆã‚¹ãƒ‘ãƒ ï¼‰ã‹ï¼Œ0ï¼ˆã‚¹ãƒ‘ãƒ ã§ãªã„ï¼‰ã‹ã‚’åˆ¤å®šã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ï¼\nè©•ä¾¡å°ºåº¦ã¯accuracyã¨ã™ã‚‹ï¼\n\nspam = pd.read_csv(\"http://logopt.com/data/spam.csv\")\nspam.head()\n\n\n\n\n\n\n\n\nword_freq_make\nword_freq_address\nword_freq_all\nword_freq_3d\nword_freq_our\nword_freq_over\nword_freq_remove\nword_freq_internet\nword_freq_order\nword_freq_mail\n...\nchar_freq_;\nchar_freq_(\nchar_freq_[\nchar_freq_!\nchar_freq_$\nchar_freq_#\ncapital_run_length_average\ncapital_run_length_longest\ncapital_run_length_total\nis_spam\n\n\n\n\n0\n0.21\n0.28\n0.50\n0.0\n0.14\n0.28\n0.21\n0.07\n0.00\n0.94\n...\n0.00\n0.132\n0.0\n0.372\n0.180\n0.048\n5.114\n101\n1028\n1\n\n\n1\n0.06\n0.00\n0.71\n0.0\n1.23\n0.19\n0.19\n0.12\n0.64\n0.25\n...\n0.01\n0.143\n0.0\n0.276\n0.184\n0.010\n9.821\n485\n2259\n1\n\n\n2\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n...\n0.00\n0.137\n0.0\n0.137\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n3\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n...\n0.00\n0.135\n0.0\n0.135\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n4\n0.00\n0.00\n0.00\n0.0\n1.85\n0.00\n0.00\n1.85\n0.00\n0.00\n...\n0.00\n0.223\n0.0\n0.000\n0.000\n0.000\n3.000\n15\n54\n1\n\n\n\n\n5 rows Ã— 58 columns\n\n\n\n\n\nå•é¡Œï¼ˆæ¯’ã‚­ãƒã‚³ï¼‰\nãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¯’ã‚­ãƒã‚³ã‹å¦ã‹ã‚’ï¼Œæ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ã¦åˆ¤å®šã›ã‚ˆï¼\ntargetåˆ—ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆå¾“å±å¤‰æ•°ï¼‰ã§ã‚ã‚Šï¼ŒedibleãŒé£Ÿç”¨ï¼ŒpoisonousãŒæ¯’ã§ã‚ã‚‹ï¼\nè©•ä¾¡å°ºåº¦ã¯accuracyã¨ã™ã‚‹ï¼\n\nmashroom = pd.read_csv(\n    \"http://logopt.com/data/mashroom.csv\",\n    dtype={\"shape\": \"category\", \"surface\": \"category\", \"color\": \"category\"},\n)\nmashroom.head()\n\n\n\n\n\n\n\n\ntarget\nshape\nsurface\ncolor\n\n\n\n\n0\nedible\nconvex\nsmooth\nyellow\n\n\n1\nedible\nbell\nsmooth\nwhite\n\n\n2\npoisonous\nconvex\nscaly\nwhite\n\n\n3\nedible\nconvex\nsmooth\ngray\n\n\n4\nedible\nconvex\nscaly\nyellow\n\n\n\n\n\n\n\n\n\nå•é¡Œï¼ˆã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ï¼‰\ntitanicãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ·±å±¤å­¦ç¿’ã‚’è¡Œã„ï¼Œæ­»äº¡ç¢ºç‡ã®æ¨å®šã‚’è¡Œãˆï¼\n\ntitanic = pd.read_csv(\"http://logopt.com/data/titanic.csv\")\ntitanic.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n\nå•é¡Œï¼ˆèƒ¸éƒ¨ç™Œï¼‰\nhttp://logopt.com/data/cancer.csv ã«ã‚ã‚‹èƒ¸éƒ¨ç™Œã‹å¦ã‹ã‚’åˆ¤å®šã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ï¼Œæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹åˆ†é¡ã‚’è¡Œãˆï¼\næœ€åˆã®åˆ—diagnosisãŒç™Œã‹å¦ã‹ã‚’è¡¨ã™ã‚‚ã®ã§ã‚ã‚Šï¼Œâ€œMâ€ãŒæ‚ªæ€§ï¼ˆmalignantï¼‰ï¼Œâ€œBâ€ãŒè‰¯æ€§ï¼ˆbenignï¼‰ã‚’è¡¨ã™ï¼\n\ncancer = pd.read_csv(\"http://logopt.com/data/cancer.csv\", index_col=0)\ncancer.head()\n\n\n\n\n\n\n\n\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows Ã— 31 columns\n\n\n\n\n\nå•é¡Œï¼ˆéƒ¨å±‹ï¼‰\nä»¥ä¸‹ã®éƒ¨å±‹ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å¦ã‹ã‚’åˆ¤å®šã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ï¼Œæ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹åˆ†é¡ã‚’è¡Œãˆï¼\noccupancyåˆ—ãŒéƒ¨å±‹ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹å¦ã‹ã‚’è¡¨ã™æƒ…å ±ã§ã‚ã‚Šï¼Œã“ã‚Œã‚’datetimeåˆ—ä»¥å¤–ã®æƒ…å ±ã‹ã‚‰åˆ†é¡ã›ã‚ˆï¼\n\noccupancy = pd.read_csv(\"http://logopt.com/data/occupancy.csv\")\noccupancy.head()\n\n\n\n\n\n\n\n\ndatetime\ntemperature\nrelative humidity\nlight\nCO2\nhumidity\noccupancy\n\n\n\n\n0\n2015-02-04 17:51:00\n23.18\n27.2720\n426.0\n721.25\n0.004793\n1\n\n\n1\n2015-02-04 17:51:59\n23.15\n27.2675\n429.5\n714.00\n0.004783\n1\n\n\n2\n2015-02-04 17:53:00\n23.15\n27.2450\n426.0\n713.50\n0.004779\n1\n\n\n3\n2015-02-04 17:54:00\n23.15\n27.2000\n426.0\n708.25\n0.004772\n1\n\n\n4\n2015-02-04 17:55:00\n23.10\n27.2000\n426.0\n704.50\n0.004757\n1",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#ç”»åƒãƒ‡ãƒ¼ã‚¿",
    "href": "16fastai.html#ç”»åƒãƒ‡ãƒ¼ã‚¿",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "ç”»åƒãƒ‡ãƒ¼ã‚¿",
    "text": "ç”»åƒãƒ‡ãƒ¼ã‚¿\nãƒ‡ãƒ¼ã‚¿ä¸€è¦§ã¯ Data Externalã«ã‚ã‚‹ï¼\nhttp://docs.fast.ai/data.external#download_url\n\nfrom fastai.vision import *\n\n\nè¤‡æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆã™ã‚‹åˆ†é¡\nPASCAL_2007ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼Œè¤‡æ•°ã®ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬ã‚’è¡Œã†ï¼\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.PASCAL_2007)\n\n\n\n\n\ndf = pd.read_csv(path/\"train.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n\n\n\n\n\nãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ DataBlockã‚’å§‹ã‚ã«ç”Ÿæˆã—ã¦ï¼Œãã‚Œã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œã‚‹ï¼\nãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ã¯ï¼Œä»¥ä¸‹ã®å¼•æ•°ã‚’ã‚‚ã¤ï¼\n\nblocks: ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹æˆã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¿ãƒ—ãƒ«ï¼› ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ã¨ï¼ˆè¤‡æ•°ã®ï¼‰ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ–ãƒ­ãƒƒã‚¯\nsplitter: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™é–¢æ•°\nget_x: ç‹¬ç«‹å¤‰æ•°ï¼ˆç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’è¿”ã™é–¢æ•°\nget_y: å¾“å±å¤‰æ•°ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰ã‚’è¿”ã™é–¢æ•°\nitem_tfms: å€‹ã€…ã®ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã®æŒ‡ç¤º\nbatch_tfms: ãƒãƒƒãƒã«å¯¾ã™ã‚‹å¤‰æ›ã®æŒ‡ç¤º\n\n\ndef get_x(r): return path/\"train\"/r[\"fname\"]\ndef get_y(r): return r[\"labels\"].split(\" \")\ndef splitter(df):\n    train = df.index[df[\"is_valid\"]].tolist()\n    valid = df.index[df[\"is_valid\"]].tolist()\n    return train,valid\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\ndls.show_batch(nrows=1, ncols=3)\n\n\n\n\n\n\n\n\nè©•ä¾¡å°ºåº¦ã«ã¯å¤šãƒ©ãƒ™ãƒ«ç”¨ã®æ­£è§£ç‡ accuracy_multiã‚’ç”¨ã„ã‚‹ï¼ ã¾ãŸï¼Œé–¢æ•°partialã§ï¼Œå¼•æ•°ã®é–¾å€¤ï¼ˆthresh)ã‚’0.2ã«å›ºå®šã—ã¦æ¸¡ã™ï¼ partialã¯æ¨™æº–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®functoolsã«å«ã¾ã‚Œã¦ã„ã‚‹ãŒï¼Œfastaiã§ã¯ã™ã§ã«importã—ãŸçŠ¶æ…‹ã«ãªã£ã¦ã„ã‚‹ï¼\nfine_tuneã§è¨“ç·´ã‚’ã™ã‚‹ãŒï¼Œæœ€çµ‚å±¤ä»¥å¤–ã‚’å›ºå®šã—ã¦ï¼ˆfreezeã—ã¦ï¼‰ï¼”ã‚¨ãƒãƒƒã‚¯è¨“ç·´ã—ï¼Œãã®å¾Œï¼Œæœ€çµ‚å±¤ä»¥å¤–ã‚‚è‡ªç”±ã«ã—ã¦3ã‚¨ãƒãƒƒã‚¯è¨“ç·´ã™ã‚‹ï¼\n\nlearn = vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.939143\n0.686238\n0.235538\n00:28\n\n\n1\n0.824816\n0.574355\n0.280578\n00:27\n\n\n2\n0.605865\n0.203421\n0.807829\n00:28\n\n\n3\n0.359789\n0.127288\n0.937928\n00:28\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.138787\n0.124408\n0.941434\n00:29\n\n\n1\n0.118910\n0.108459\n0.948426\n00:29\n\n\n2\n0.097468\n0.105211\n0.952430\n00:29\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\nç”»åƒã‹ã‚‰äººã®é ­ã®ä¸­å¿ƒã‚’å½“ã¦ã‚‹å›å¸°\nç”»åƒãƒ‡ãƒ¼ã‚¿ã¯åˆ†é¡ã ã‘ã§ãªãï¼Œå›å¸°ã‚’è¡Œã†ã“ã¨ã‚‚ã§ãã‚‹ï¼ BIWIãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼Œã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’è¡¨ç¤ºã™ã‚‹ï¼\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f\"{str(x)[:-7]}pose.txt\")\nimg2pose(img_files[0])\n\nPath('/root/.fastai/data/biwi_head_pose/06/frame_00079_pose.txt')\n\n\n\ncal = np.genfromtxt(path/\"01\"/\"rgb.cal\", skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name==\"13\"),\n    batch_tfms=[*aug_transforms(size=(240,320)), \n                Normalize.from_stats(*imagenet_stats)]\n)\n\n\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(lr_min=0.004786301031708717, lr_steep=1.3182567499825382e-06)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(4, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.052732\n0.005105\n02:07\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.005605\n0.001754\n02:17\n\n\n1\n0.003696\n0.001718\n02:17\n\n\n2\n0.002149\n0.000066\n02:16\n\n\n3\n0.001374\n0.000060\n02:18\n\n\n\n\n\næ­£è§£ã¨äºˆæ¸¬ã‚’è¡¨ç¤º\n\nlearn.show_results()",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°",
    "href": "16fastai.html#å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°",
    "text": "å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n\nfrom fastai.tabular.all import *\nfrom fastai.collab import *\n\nå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(collaborative filtering)ã¨ã¯ï¼Œæ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ (recommender system)ã®ä¸€ç¨®ã§ï¼Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ä¸¡æ–¹ã®æ½œåœ¨å› å­ã‚’è€ƒæ…®ã—ã¦ï¼Œãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’æ±ºã‚ã‚‹æ‰‹æ³•ã ï¼\næ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ˆãè¦‹ã‹ã‘ã‚‹ã®ã¯ï¼Œã€Œã“ã®å•†å“ã‚’è²·ã£ãŸäººã¯ã“ã®å•†å“ã‚‚è²·ã£ã¦ã„ã¾ã™ã€ã¨ã‹ã€Œæœ€ã‚‚è‰¯ãå£²ã‚Œã¦ã„ã‚‹ã®ã¯ã“ã®å•†å“ã§ã™ã€ãªã©ã®çŒ¿ã§ã‚‚ã§ãã‚‹ã‚¿ã‚¤ãƒ—ã®ã‚‚ã®ã ï¼ã“ã®ã‚ˆã†ãªå˜ç´”ãªã‚‚ã®ã§ã¯ãªãï¼Œã‚ãªãŸã«ä¼¼ãŸæ½œåœ¨å› å­ã‚’ã‚‚ã¤äººãŒï¼Œé«˜ã„ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’ã¤ã‘ã¦ã„ã‚‹ï¼ˆã‚‚ã—ãã¯è‰¯ãè³¼å…¥ã™ã‚‹ï¼‰å•†å“ã«è¿‘ã„æ½œåœ¨å› å­ã‚’ã‚‚ã£ãŸå•†å“ã‚’ç´¹ä»‹ã™ã‚‹ã®ãŒï¼Œå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ã‚ã‚‹ï¼\næ©Ÿæ¢°å­¦ç¿’ã®ä¸­ã§ï¼ˆAndrew NgãŒå®Ÿå‹™å®¶ã‹ã‚‰èã„ãŸè©±ã ãŒï¼‰å®Ÿå‹™ã§æœ€ã‚‚å½¹ã«ç«‹ã¤ï¼Œã‚‚ã—ãã¯æœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã®ãŒã“ã‚Œã ï¼ èƒŒæ™¯ã«ã‚ã‚‹ç†è«–ã‚’ç°¡å˜ã«ç´¹ä»‹ã—ã‚ˆã†ï¼\nã„ã¾ï¼Œé¡§å®¢ã¨å•†å“ã®é›†åˆã¨ã¨ã‚‚ã«ï¼Œå•†å“ \\(i\\) ã«å¯¾ã—ã¦é¡§å®¢ \\(j\\) ãŒè©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ãŒä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¨ã™ã‚‹ï¼ ãŸã ã—ï¼Œé¡§å®¢ãŒè©•ä¾¡ã‚’ã¤ã‘ãŸå•†å“ã¯é€šå¸¸ã¯å°‘ãªãï¼Œãƒ‡ãƒ¼ã‚¿ã¯æ¥µã‚ã¦ç–ãªè¡Œåˆ—ã¨ã—ã¦ä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹ï¼ å•†å“ \\(i\\) ã«å¯¾ã—ã¦é¡§å®¢ \\(j\\) ãŒè©•ä¾¡ã‚’è¡Œã£ã¦ã„ã‚‹ã¨ã \\(1\\)ï¼Œãã‚Œä»¥å¤–ã®ã¨ã \\(0\\) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ \\(r(i,j)\\) ã¨ã™ã‚‹ï¼ \\(r(i,j)=1\\) ã®å ´åˆã«ã¯ï¼Œé¡§å®¢ \\(j\\) ã¯å•†å“ \\(i\\) ã«å¯¾ã—ã¦é›¢æ•£å€¤ã®ï¼ˆãŸã¨ãˆã° \\(1\\) ã‹ã‚‰ \\(5\\) ã®æ•´æ•°ãªã©ã§ï¼‰è©•ä¾¡ç‚¹ã‚’ã¤ã‘ã‚‹ï¼ ã“ã®è©•ä¾¡ç‚¹ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚’ \\(y^{(i,j)}\\) ã¨ã™ã‚‹ï¼ã“ã‚ŒãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«ãªã‚‹ï¼ ã“ã‚Œã‚’ã‚‚ã¨ã«ï¼Œè©•ä¾¡ç‚¹ãŒã¤ã‘ã‚‰ã‚Œã¦ã„ãªã„ï¼ˆ\\(r(i,j)=0\\) ã®ï¼‰å ´æ‰€ã®è©•ä¾¡ç‚¹ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒå•é¡Œã®ç›®çš„ã¨ãªã‚‹ï¼\næ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã®ãŸã‚ã®æ‰‹æ³•ã¯ï¼Œ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹æ¨å¥¨(contents based recommendation)ã¨å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨(collaborative filtering recommendation)ã®2ã¤ã«åˆ†é¡ã§ãã‚‹ï¼\nã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹æ¨å¥¨ã§ã¯ï¼Œå•†å“ \\(i\\) ã«å¯¾ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« \\(x^{(i)}  \\in R^n\\) ãŒä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹ã¨ä»®å®šã™ã‚‹ï¼ ãŸã¨ãˆã°ï¼Œå•†å“ã‚’æ˜ ç”»ã¨ã—ãŸã¨ãã«ï¼Œç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯æ˜ ç”»ã®ç¨®åˆ¥ï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ŒSFï¼Œãƒ›ãƒ©ãƒ¼ï¼Œæ‹æ„›ã‚‚ã®ï¼Œã‚¹ãƒªãƒ©ãƒ¼ ï¼Œãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ãªã©ï¼‰ã®åº¦åˆã„ã‚’è¡¨ã™ï¼ ãŸã¨ãˆã°ï¼Œã‚¹ã‚¿ãƒ¼ã‚¦ã‚©ãƒ¼ã‚ºã¯SFåº¦ \\(0.8\\)ï¼Œæ‹æ„›åº¦ \\(0.1\\)ï¼Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³åº¦ \\(0.1\\) ã¨æ¡ç‚¹ã•ã‚Œã‚‹ï¼\né¡§å®¢ \\(j\\) ã®ç‰¹å¾´ã«å¯¾ã™ã‚‹é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ \\(w^{(j)}  \\in R^n\\) ã¨ã™ã‚‹ï¼ã“ã‚Œã¯é¡§å®¢ãŒã©ã†ã„ã£ãŸæ˜ ç”»ã®ç¨®åˆ¥ã‚’å¥½ã‚€ã®ã‹ã‚’è¡¨ã™ï¼ ã“ã‚Œã‚’ç·šå½¢å›å¸°ã‚’ç”¨ã„ã¦æ±‚ã‚ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã¨ä»®èª¬é–¢æ•°ã¯ï¼Œ \\[\n  h_w (x)=w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n  \n\\] ã¨ãªã‚‹ï¼ æœ€é©ãªé‡ã¿ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯ï¼Œä»¥ä¸‹ã«å®šç¾©ã•ã‚Œã‚‹è²»ç”¨é–¢æ•°ã‚’æœ€å°ã«ã™ã‚‹é‡ã¿ãƒ™ã‚¯ãƒˆãƒ« \\(w^{(j)}\\) ã‚’æ±‚ã‚ã‚Œã°ã‚ˆã„ï¼ \\[\n\\frac{1}{2} \\sum_{i: r(i,j) = 1 } \\left(  (w^{(j)})^T (x^{(i)}) -y^{(i,j)} \\right)^2\n\\]\næ˜ ç”»ã”ã¨ã«ç‰¹å¾´ã‚’è¦‹ç©ã‚‚ã‚‹ã“ã¨ã¯å®Ÿéš›ã«ã¯é›£ã—ã„ï¼ ãã“ã§ï¼Œå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨ã§ã¯ï¼Œå•†å“ã”ã¨ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« \\(x^{(i)}  \\in R^n\\) ã‚’å®šæ•°ã¨ã—ã¦ä¸ãˆã‚‹ã®ã§ã¯ãªãï¼Œ å¤‰æ•°ã¨ã¿ãªã—ã¦é¡§å®¢ã”ã¨ã®é‡ã¿ã¨åŒæ™‚ã«æœ€é©åŒ–ã‚’è¡Œã†ï¼ ã™ã¹ã¦ã®é¡§å®¢ã¨å•†å“ã«å¯¾ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ã®èª¤å·®ã®2ä¹—å’Œã‚’æœ€å°åŒ–ã™ã‚‹å•é¡Œã¯ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ã‘ã‚‹ï¼ \\[\n\\min_{w, x} \\frac{1}{2} \\sum_{(i,j): r(i,j) = 1 } \\left(  (w^{(j)})^T (x^{(i)}) -y^{(i,j)} \\right)^2\n\\]\nã“ã®å•é¡Œã‚’ç›´æ¥æœ€é©åŒ–ã—ã¦ã‚‚ã‚ˆã„ãŒï¼Œ\\(x\\) ã¨ \\(w\\) ã‚’äº¤äº’ã«ç·šå½¢å›å¸°ã‚’ç”¨ã„ã¦è§£ãç°¡ä¾¿æ³•ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ï¼ ã™ãªã‚ã¡ï¼Œé©å½“ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ¨å®šå€¤ \\(x^{(i)}\\) ã‚’ç”¨ã„ã¦é¡§å®¢ \\(j\\) ã«å¯¾ã™ã‚‹é‡ã¿ãƒ™ã‚¯ãƒˆãƒ« \\(w^{(j)}\\) ã‚’æ±‚ã‚ãŸå¾Œã«ï¼Œ ä»Šåº¦ã¯ \\(w^{(j)}\\) ã‚’ç”¨ã„ã¦ \\(x^{(i)}\\) ã‚’æ±‚ã‚ã‚‹ã®ã§ã‚ã‚‹ï¼ã“ã®æ“ä½œã‚’åæŸã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã›ã°ã‚ˆã„ï¼\nä¸Šã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦å¾—ã‚‰ã‚ŒãŸå•†å“ \\(i\\) ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« \\(x^{(i)}\\) ã‚’ç”¨ã„ã‚‹ã¨ï¼Œ é¡ä¼¼ã®å•†å“ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ ãŸã¨ãˆã°ï¼Œ\\(x^{(i)}\\) ã‚’ \\(n\\)æ¬¡å…ƒç©ºé–“å†…ã®ç‚¹ã¨ã¿ãªã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ã«ã‚ˆã£ã¦ï¼Œ å•†å“ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒã§ãã‚‹ï¼åŒæ§˜ã«é¡§å®¢ \\(j\\) ã®é‡ã¿ãƒ™ã‚¯ãƒˆãƒ« \\(w^{(j)}\\) ã‚’ç”¨ã„ã‚‹ã“ã¨ã«ã‚ˆã£ã¦é¡§å®¢ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒã§ãã‚‹ï¼\næœ‰åãªä¾‹é¡Œï¼ˆæ˜ ç”»ã®è©•ä¾¡å€¤ã‚’å½“ã¦ã‚‹ï¼‰ã§ã‚ã‚‹MovieLensã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼\nãƒ‡ãƒ¼ã‚¿ã«ã¯timestampåˆ—ãŒã¤ã„ã¦ã„ã‚‹ãŒï¼Œã¨ã‚Šã‚ãˆãšã“ã‚Œã¯ç„¡è¦–ã—ã¦ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°(rating)ã‚’äºˆæ¸¬ã—ã¦ã¿ã‚‹ï¼\n\npath = untar_data(URLs.ML_100k)\nratings = pd.read_csv(path/\"u.data\", delimiter=\"\\t\", header=None,\n                      usecols=(0,1,2), names=[\"user\",\"movie\",\"rating\"])\nratings.head()\n\n\n\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\n\n\n\n\n0\n196\n242\n3\n\n\n1\n186\n302\n3\n\n\n2\n22\n377\n1\n\n\n3\n244\n51\n2\n\n\n4\n166\n346\n1\n\n\n\n\n\n\n\n\nmovies = pd.read_csv(path/\"u.item\",  delimiter=\"|\", encoding=\"latin-1\",\n                     usecols=(0,1), names=(\"movie\",\"title\"), header=None)\nmovies.head()\n\n\n\n\n\n\n\n\nmovie\ntitle\n\n\n\n\n0\n1\nToy Story (1995)\n\n\n1\n2\nGoldenEye (1995)\n\n\n2\n3\nFour Rooms (1995)\n\n\n3\n4\nGet Shorty (1995)\n\n\n4\n5\nCopycat (1995)\n\n\n\n\n\n\n\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\ntitle\n\n\n\n\n0\n196\n242\n3\nKolya (1996)\n\n\n1\n63\n242\n3\nKolya (1996)\n\n\n2\n226\n242\n5\nKolya (1996)\n\n\n3\n154\n242\n3\nKolya (1996)\n\n\n4\n306\n242\n5\nKolya (1996)\n\n\n\n\n\n\n\nCollabDataLoadersã‚¯ãƒ©ã‚¹ã®from_dfãƒ¡ã‚½ãƒƒãƒ‰ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å…¥ã‚Œã‚‹ã¨ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦ãã‚Œã‚‹ï¼\nå¼•æ•°ã¯ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ (ratings)ï¼Œæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡(pct_val)ï¼Œãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼Œã‚¢ã‚¤ãƒ†ãƒ ï¼Œãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’è¡¨ã™åˆ—åã ï¼\n\ndls = CollabDataLoaders.from_df(ratings, item_name=\"title\", bs=64)\n\nä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’collab_learneré–¢æ•°ã«å…¥ã‚Œã‚‹ã¨å­¦ç¿’å™¨ï¼ˆèª¤å·®ã‚’æœ€å°ã«ã™ã‚‹æ½œåœ¨å› å­è¡Œåˆ—ã®é‡ã¿ã®æœ€é©åŒ–ãŒç›®çš„ï¼‰ã‚’ä½œã£ã¦ãã‚Œã‚‹ï¼äºˆæ¸¬ã—ãŸã„ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ï¼Œæ˜Ÿï¼•ã¤ã¾ã§ãªã®ã§ï¼Œy_rangeã§æŒ‡å®šã™ã‚‹ï¼\nãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ(data)ï¼Œæ½œåœ¨å› å­ã®æ•°(n_factors)ã‚’æŒ‡å®šã—ã¦ã„ã‚‹ãŒï¼Œä»–ã«ã‚‚metricsã¯è©•ä¾¡å°ºåº¦ï¼Œwdã¯weight decayã§æ­£å‰‡åŒ–ã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã©ã‚’æŒ‡å®šã§ãã‚‹ï¼\ndef collab_learner(data, n_factors:int=None, use_nn:bool=False, metrics=None,\n                  emb_szs:Dict[str,int]=None, wd:float=0.01, **kwargs)-&gt;Learner\n\ndls.show_batch()\n\n\n\n\n\nuser\ntitle\nrating\n\n\n\n\n0\n348\nJack (1996)\n4\n\n\n1\n346\nTwelve Monkeys (1995)\n2\n\n\n2\n110\nSimple Twist of Fate, A (1994)\n2\n\n\n3\n72\nConan the Barbarian (1981)\n2\n\n\n4\n864\nDeath and the Maiden (1994)\n4\n\n\n5\n347\nTwelve Monkeys (1995)\n4\n\n\n6\n731\nSabrina (1954)\n4\n\n\n7\n751\nRaising Arizona (1987)\n3\n\n\n8\n344\nLeaving Las Vegas (1995)\n4\n\n\n9\n577\nDevil in a Blue Dress (1995)\n4\n\n\n\n\n\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.935071\n0.929474\n00:06\n\n\n1\n0.812040\n0.860315\n00:06\n\n\n2\n0.631619\n0.864089\n00:06\n\n\n3\n0.400996\n0.885280\n00:06\n\n\n4\n0.289997\n0.891847\n00:06\n\n\n\n\n\næ¤œè¨¼ã®æå‡ºé–¢æ•°ã‚’ã¿ã‚‹ã¨ã€éå‰°é©åˆã—ã¦ã„ã‚‹ã‚ˆã†ã ï¼ˆé€”ä¸­ã¾ã§ä¸‹ãŒã£ã¦ã„ã‚‹ãŒã€æœ€å¾Œã¯ä¸Šæ˜‡ã—ã¦ã„ã‚‹ï¼‰ã€‚\nL2æ­£å‰‡åŒ–é–¢æ•°ã‚’å…¥ã‚Œã¦ã¿ã‚ˆã†ã€‚fastaiã§ã¯ã€é‡ã¿æ¸›è¡° (weight decay: wd) ã¨ã„ã†å¼•æ•°ã§æŒ‡å®šã™ã‚‹ã€‚\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.952608\n0.940679\n00:06\n\n\n1\n0.839875\n0.867949\n00:06\n\n\n2\n0.742433\n0.825799\n00:06\n\n\n3\n0.591568\n0.814826\n00:06\n\n\n4\n0.479914\n0.816404\n00:06\n\n\n\n\n\nè¨“ç·´ãƒ­ã‚¹ã¯æ‚ªåŒ–ã—ã¦ã„ã‚‹ãŒã€æ¤œè¨¼ãƒ­ã‚¹ã¯æ”¹å–„ã—ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ã€‚\nãƒˆãƒƒãƒ—1000ã®æ˜ ç”»ã‚’æŠ½å‡ºã—ã€åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ä¸»æˆåˆ†åˆ†æã§2æ¬¡å…ƒã«è½ã¨ã—ã¦æç”»ã—ã¦ã¿ã‚‹ã€‚\nãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒã€è‡ªå‹•çš„ã«æ˜ ç”»ã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ã€‚\n\ng = ratings.groupby(\"title\")[\"rating\"].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes[\"title\"].o2i[m] for m in top_movies])\nmovie_w = learn.model.i_weight.weight[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = np.random.choice(len(top_movies), 50, replace=False)\nidxs = list(range(100))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#æ„å‘³åˆ†å‰²",
    "href": "16fastai.html#æ„å‘³åˆ†å‰²",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "æ„å‘³åˆ†å‰²",
    "text": "æ„å‘³åˆ†å‰²\nä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ä¸ãˆã‚‰ã‚ŒãŸç”»åƒã®åˆ†å‰²ï¼ˆå„ãƒ”ã‚¯ã‚»ãƒ«ãŒã©ã®ç‰©ä½“ã«å±ã™ã‚‹ã®ã‹ã‚’åˆ†é¡ã™ã‚‹ã“ã¨ï¼›ã“ã‚Œã‚’æ„å‘³åˆ†å‰²(semantic segmentation)ã¨å‘¼ã¶ï¼‰ã«ç”¨ã„ã‚‰ã‚Œã‚‹ã€‚\n\nCamvid: Motion-based Segmentation and Recognition Dataset (CAMVID, CAMVID_TINY)\n\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/\"labels\"/f\"{o.stem}_P{o.suffix}\",\n    codes = np.loadtxt(path/\"codes.txt\", dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n2.760665\n5.930563\n01:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n2.219755\n1.865447\n01:15\n\n\n1\n1.819075\n1.280244\n01:20\n\n\n2\n1.559273\n1.177241\n01:17\n\n\n3\n1.374030\n0.868488\n01:16\n\n\n4\n1.210444\n0.832017\n01:16\n\n\n5\n1.082514\n0.755146\n01:16\n\n\n6\n0.980371\n0.728570\n01:15\n\n\n7\n0.902825\n0.727522\n01:16\n\n\n\n\n\n\nlearn.show_results(max_n=6, figsize=(7,8))",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿",
    "href": "16fastai.html#ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿",
    "text": "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿\nfastaiã§ã¯ï¼ŒWikipediaã®è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸå­¦ç¿’æ¸ˆã¿ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹AWD_LSTMã‚’æº–å‚™ã—ã¦ã„ã‚‹ï¼ æ˜ ç”»ã®æ‰¹è©•ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ï¼Œfastaiã®è‡ªç„¶è¨€èªå‡¦ç†ã‚’è©¦ã—ã¦ã¿ã‚‹ï¼\n\nfrom fastai.text.all import *\n\n\npath = untar_data(URLs.IMDB)\n\nget_imdb = partial(get_text_files, folders=[\"train\", \"test\", \"unsup\"])\n\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n\n\n\n\n\n\n\nè¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ dls_lm ã‚’ã‚‚ã¨ã«ï¼Œå­¦ç¿’æ¸ˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿AWD_LSTMã‚’èª­ã¿è¾¼ã‚“ã§å­¦ç¿’å™¨ã‚’ã¤ãã‚‹ï¼\n\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3, \n    metrics=[accuracy, Perplexity()]).to_fp16()\n\n\n\n\né©å½“ãªæ–‡ç« TEXT ã‚’å…¥ã‚Œã¦ï¼Œãã®å¾Œã®æ–‡ç« ã‚’ä½œã‚‰ã›ã‚‹ï¼tempertureã¯æ–‡ç« ã«ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’ä»˜ä¸ã™ã‚‹ãŸã‚ã«ç”¨ã„ã‚‰ã‚Œã‚‹ï¼\n\nTEXT = \"This is a pen. That is an\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]\nprint(preds)\n\n\n\n\n\n\n\n[\"This is a pen . That is an allusion to what i ' ve known as The Radio Times . BBC Radio 1 ! is an example of how a different composer can work with an orchestra and which contains over half a\",\n 'This is a pen . That is an especially unusual phrase for an artist who has been ascribed to the term , and is sometimes referred to as the \" Artist Generation \" . The term is sometimes defined as defining the evolution of the']\n\n\nè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ï¼Œæ˜ ç”»ã®æ‰¹è©•ã®ãƒ†ã‚­ã‚¹ãƒˆãŒï¼Œãƒã‚¬ãƒ†ã‚£ãƒ–ã‹ãƒ‘ã‚·ãƒ†ã‚£ãƒ–ã‹ã‚’åˆ¤åˆ¥ã™ã‚‹å­¦ç¿’å™¨ã‚’ã¤ãã‚‹ï¼\næ˜ ç”»æ‰¹è©•ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIMDBã‚’èª­ã¿è¾¼ã‚“ã§ï¼Œè¨€èªãƒ¢ãƒ‡ãƒ« AWD_LSTM ã‚’ç”¨ã„ã¦è¨“ç·´ã™ã‚‹ï¼\n\nfrom fastai.text.all import *\n\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=\"test\")\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.598281\n0.404613\n0.822120\n04:11\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.318746\n0.245778\n0.898840\n07:56\n\n\n1\n0.249433\n0.221797\n0.908000\n08:07\n\n\n2\n0.183325\n0.190910\n0.926360\n08:19\n\n\n3\n0.153639\n0.194017\n0.926240\n08:11\n\n\n\n\n\näºˆæ¸¬ã—ã¦ã¿ã‚‹ï¼\n\nprint( learn.predict(\"I really liked that movie!\") )\n\n\n\n\n('pos', tensor(1), tensor([3.4028e-04, 9.9966e-01]))",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#ç”»åƒç”Ÿæˆ",
    "href": "16fastai.html#ç”»åƒç”Ÿæˆ",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "ç”»åƒç”Ÿæˆ",
    "text": "ç”»åƒç”Ÿæˆ\nã¡ã‚‡ã£ã¨å‰ã¾ã§ã¯ GAN (generative adversarial network; æ•µå¯¾çš„ç”Ÿæˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)ãŒæµè¡Œã—ã¦ã„ãŸãŒï¼Œ æœ€è¿‘ã§ã¯ æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« (diffusion model)ã‚’ç”¨ã„ã¦ï¼Œé«˜ç²¾åº¦ãªç”»åƒã‚’é«˜é€Ÿã«ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ããŸï¼\n\nå•é¡Œï¼ˆDALLãƒ»E 2ï¼‰\nDALLãƒ»E2 https://openai.com/dall-e-2/ ã«ç™»éŒ²ã—ã¦ï¼Œã‚ªãƒªã‚¸ãƒŠãƒ«ã®ç”»åƒã‚’ç”Ÿæˆã›ã‚ˆï¼\nï¼ˆæ³¨æ„ï¼š ç”Ÿæˆã§ãã‚‹ç”»åƒæ•°ã«åˆ¶é™ãŒã‚ã‚‹ï¼ˆæ¯æœˆãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹ï¼‰\n\n\nå•é¡Œï¼ˆHugging Face Diffusersï¼‰\nDiffusers https://github.com/huggingface/diffusers/ ã®Quickstartã«ã‚ã‚‹ Getting started with Diffusers ã‚’Google Colab ã§é–‹ã„ã¦ï¼Œãƒ‰ãƒ©ã‚¤ãƒ–ã«ã‚³ãƒ”ãƒ¼ã‚’ä¿å­˜ã—ã¦ã‹ã‚‰ï¼Œæœ€åˆã®ç”»åƒç”Ÿæˆã¾ã§ã‚’å®Ÿè¡Œã›ã‚ˆï¼\nï¼ˆæ³¨æ„ï¼š ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§GPUã‚’ã‚ªãƒ³ã«ã—ã¦ã‹ã‚‰å®Ÿè¡Œã™ã‚‹ï¼ æœ‰æ–™ç‰ˆã®Colab Pro(+)ã«ç™»éŒ²ã™ã‚‹å¿…è¦ã¯ãªã„ï¼‰\n\n\nå•é¡Œ ï¼ˆHugging Face)\nHugging Face ã®ãƒ¢ãƒ‡ãƒ« https://huggingface.co/models ã®Tasksï¼ˆ+22 Taskã‚’æŠ¼ã™ã¨ãŸãã•ã‚“å‡ºã¦ãã‚‹)ã‹ã‚‰å¥½ããªã‚‚ã®ã‚’é¸ã³ï¼Œè©¦ã—ã¦ã¿ã‚ˆï¼ ã¾ãŸï¼Œã©ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹è§£èª¬ã‚’èª­ã¿ï¼Œ ï¼ˆã§ãã‚Œã°ï¼‰ Google Colabã§å‹•ã‹ã—ã¦ã¿ã‚ˆï¼\nï¼ˆæ³¨æ„ï¼š ã—ã°ã‚‰ãã¯loginãªã—ã§ä½¿ãˆã‚‹ãŒï¼Œæ™‚é–“åˆ¶é™ã‚’è¶…ãˆã‚‹ã¨loginãŒå¿…è¦ã«ãªã‚‹ï¼‰",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#æ·±å±¤å­¦ç¿’ã®åŸºç¤ã‚’å›³ã§è§£èª¬",
    "href": "16fastai.html#æ·±å±¤å­¦ç¿’ã®åŸºç¤ã‚’å›³ã§è§£èª¬",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "æ·±å±¤å­¦ç¿’ã®åŸºç¤ã‚’å›³ã§è§£èª¬",
    "text": "æ·±å±¤å­¦ç¿’ã®åŸºç¤ã‚’å›³ã§è§£èª¬\né€šå¸¸ã®ï¼ˆå®Œå…¨çµåˆå±¤ã‹ã‚‰æˆã‚‹ï¼‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ã¤ã„ã¦ã¯ï¼Œscikit-learn ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ã®ç« ã§è¿°ã¹ãŸï¼ ä»¥ä¸‹ã§ã¯ï¼Œæœ¬ç« ã§ç´¹ä»‹ã—ãŸå¹¾ã¤ã‹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¤ã„ã¦è§£èª¬ã™ã‚‹ï¼\n\nç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ\nç”»åƒãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®Œå…¨çµåˆå±¤ã ã‘ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ä½¿ã†ã“ã¨ã¯ï¼Œè†¨å¤§ãªé‡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã™ã‚‹ã®ã§ï¼Œé©å½“ãªé¸æŠã§ã¯ãªã„ï¼ å®Œå…¨çµåˆå±¤ã®ã‹ã‚ã‚Šã«ç•³ã¿è¾¼ã¿(convolusion)ã‚’ç”¨ã„ãŸå±¤ã‚’ç”¨ã„ã‚‹æ–¹æ³•ãŒï¼Œç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã‚ã‚‹ï¼\nç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã¯ï¼Œä»¥ä¸‹ã®å›³ã«ç¤ºã™ã‚ˆã†ã«ï¼Œç•³ã¿è¾¼ã¿ï¼ˆä¸€ç¨®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡Œåˆ—ã®ä¹—ç®—ï¼‰ã‚’è¡Œã£ãŸå¾Œã«ï¼Œæ´»æ€§åŒ–é–¢æ•°ã¨ã—ã¦LeLUã‚’ç”¨ã„ï¼Œã•ã‚‰ã«ãƒãƒƒã‚¯ã‚¹ãƒ—ãƒ¼ãƒ«ã§ãƒ‡ãƒ¼ã‚¿ã‚’å°ã•ãã™ã‚‹æ“ä½œã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ãï¼ ãã—ã¦ï¼Œæœ€å¾Œã®å±¤ã ã‘ã‚’å®Œå…¨çµåˆå±¤ã¨ã—ï¼Œåˆ†é¡ã‚‚ã—ãã¯å›å¸°ã‚’è¡Œã†ï¼\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nå›å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ\næ–‡å­—åˆ—ã‚„éŸ³å£°ãªã©ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¨ã—ã¦ï¼Œå›å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ(recurrent neural nettwork)ãŒã‚ã‚‹ï¼\né•·ã• \\(T\\) ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ \\(x^{&lt;1&gt;},x^{&lt;2&gt;},\\ldots,x^{&lt;T&gt;}\\) ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãï¼Œ å›å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ï¼ŒåˆæœŸçŠ¶æ…‹ \\(a^{&lt;0&gt;}\\) ã‹ã‚‰çŠ¶æ…‹ã®åˆ— \\(a^{&lt;1&gt;},a^{&lt;2&gt;},\\ldots,a^{&lt;T&gt;}\\) ã‚’é †æ¬¡ç”Ÿæˆã—ã¦ã„ãï¼\nå„æ™‚åˆ» \\(t=1,2,\\ldots,T\\) ã«ãŠã„ã¦ï¼Œãƒ‡ãƒ¼ã‚¿ \\(x^{&lt;t&gt;}\\) ã¨å‰ã®çŠ¶æ…‹ \\(a^{&lt;t-1&gt;}\\) ã‚’çµåˆã—ãŸã‚‚ã®ã‚’å…¥åŠ›ã¨ã—æ´»æ€§åŒ–é–¢æ•°ï¼ˆé€šå¸¸ã¯\\(\\tanh\\)ï¼‰ã‚’ç”¨ã„ã¦ï¼Œæ¬¡ã®çŠ¶æ…‹ \\(a^{&lt;t&gt;}\\) ã‚’ç”Ÿæˆã—ã¦ã„ãï¼\n\n\n\n\n\n\n\n\n\n\n\n\né•·çŸ­æœŸè¨˜æ†¶\nå›å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ï¼Œèª¤å·®é€†ä¼æ’­ã®éš›ã«å‹¾é…ãŒç„¡é™å¤§ã«ç™ºæ•£ã—ãŸã‚Šæ¶ˆå¤±ã—ã¦ã—ã¾ã†ã¨ã„ã†å¼±ç‚¹ã‚’ã‚‚ã£ã¦ã„ã‚‹ï¼ ãã®å¼±ç‚¹ã‚’å…‹æœã™ã‚‹ãŸã‚ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã—ã¦ï¼Œé•·çŸ­æœŸè¨˜æ†¶(long short-term memory: LSTM)ãŒã‚ã‚‹ï¼\nLSTMã®ç‰¹å¾´ã¯ï¼Œé•·æœŸã®è¨˜æ†¶ã®ãŸã‚ã®ã‚»ãƒ«(cell) \\(c^{&lt;t&gt;}\\) ã¨ï¼Œé€šå¸¸ã®çŠ¶æ…‹ï¼ˆçŸ­æœŸè¨˜æ†¶ã«ç›¸å½“ã™ã‚‹ï¼‰ \\(a^{&lt;t&gt;}\\) ã®ä¸¡è€…ã‚’ä¿æŒã™ã‚‹ã“ã¨ã§ã‚ã‚‹ï¼ ã“ã®2ç¨®é¡ã®è¨˜æ†¶æƒ…å ±ã‚’ï¼Œã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•° \\(\\sigma\\) ã®å‡ºåŠ›ï¼ˆ\\(0\\) ã¨ \\(1\\) ã®é–“ã«ãªã‚‹ï¼‰ã¨ä¹—ã˜ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ï¼Œ ãã®ã¾ã¾ä¿æŒã™ã‚‹ã‹ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã‹ã‚’æ±ºã‚ï¼Œ å‹¾é…ç™ºæ•£ï¼ˆæ¶ˆå¤±ï¼‰ã‚’é¿ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n\n\n\n\n\n\n\n\n\n\n\n\nåŸ‹ã‚è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ\nã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šæ¬¡å…ƒã®ä½ã„ç‰¹å¾´ã«å†™åƒã™ã‚‹ãŸã‚ã«åŸ‹ã‚è¾¼ã¿å±¤(embedding layer)ãŒä½¿ã‚ã‚Œã‚‹ï¼ä»¥ä¸‹ã®ä¾‹ã§ã¯ï¼Œ10ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ã‚‚ã¤ãƒ‡ãƒ¼ã‚¿ã‚’ï¼Œ\\(10\\times5\\)ã®é‡ã¿è¡Œåˆ—ã‚’ç”¨ã„ã¦5æ¬¡å…ƒã®ç‰¹å¾´ã«åŸ‹ã‚è¾¼ã‚“ã§ã„ã‚‹ï¼\n\n\n\n\n\n\n\n\n\n\n\n\nè‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³\nè‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³(self attention)ã¯ï¼Œæœ€è¿‘æ³¨ç›®ã‚’æµ´ã³ã¦ã„ã‚‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åŸºç¤ã¨ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚Šï¼Œ è‡ªç„¶è¨€èªå‡¦ç†ã«å¤§ããªé€²æ­©ã‚’ã‚‚ãŸã‚‰ã—ãŸï¼\nè‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã§ã¯ï¼Œå…¥åŠ›ã•ã‚ŒãŸæ–‡ç« ã‚’LSTMã®ã‚ˆã†ã«é †ç•ªã«å…¥åŠ›ã™ã‚‹ã®ã§ã¯ãªãï¼Œä¸€åº¦ã«èª­ã¿è¾¼ã‚€ï¼ ã¾ãšï¼Œå…¥åŠ›ã•ã‚ŒãŸæ–‡å­—ã®åŸ‹ã‚è¾¼ã¿ã«ï¼Œæ–‡å­—ã®ä½ç½®æƒ…å ±ã‚’æ­£å¼¦ãƒ»ä½™å¼¦æ›²ç·šã‚’ç”¨ã„ã¦ä»˜åŠ ã—ï¼Œ ãã‚Œã«å¯¾ã—ã¦ã‚¯ã‚¨ãƒªãƒ¼ï¼Œã‚­ãƒ¼ï¼Œå€¤ã‚’è¡¨ã™3ã¤ã®å…¨çµåˆå±¤ã‚’é©ç”¨ã—ï¼Œ3ã¤ã®è¡Œåˆ—ï¼ˆãƒ†ãƒ³ã‚½ãƒ«ï¼‰ \\(Q,K,V\\) ã‚’å¾—ã‚‹ï¼ æ¬¡ã«ï¼Œã‚¯ã‚¨ãƒªè¡Œåˆ— \\(Q\\) ã¨ã‚­ãƒ¼è¡Œåˆ— \\(K\\) ã®å†…ç©ã‚’ã¨ã‚Šï¼Œãã‚Œã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ï¼Œå…¥åŠ›ã•ã‚ŒãŸæ–‡å­—åŒå£«ã®é–¢ä¿‚ã‚’è¡¨ã™ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å¾—ã‚‹ï¼ æœ€å¾Œã«ï¼Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«å€¤è¡Œåˆ— \\(V\\) ã‚’ä¹—ã˜ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å‡ºåŠ›ã‚’å¾—ã‚‹ï¼",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "16fastai.html#å®Ÿè·µçš„ãªæ·±å±¤å­¦ç¿’ã®ãƒ¬ã‚·ãƒ”ã¨èƒŒæ™¯ã«ã‚ã‚‹ç†è«–",
    "href": "16fastai.html#å®Ÿè·µçš„ãªæ·±å±¤å­¦ç¿’ã®ãƒ¬ã‚·ãƒ”ã¨èƒŒæ™¯ã«ã‚ã‚‹ç†è«–",
    "title": "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "å®Ÿè·µçš„ãªæ·±å±¤å­¦ç¿’ã®ãƒ¬ã‚·ãƒ”ã¨èƒŒæ™¯ã«ã‚ã‚‹ç†è«–",
    "text": "å®Ÿè·µçš„ãªæ·±å±¤å­¦ç¿’ã®ãƒ¬ã‚·ãƒ”ã¨èƒŒæ™¯ã«ã‚ã‚‹ç†è«–\nä¸Šã§ã¯ä¾‹ã‚’ç¤ºã™ã“ã¨ã«ã‚ˆã£ã¦æ·±å±¤å­¦ç¿’ã®ã€Œé›°å›²æ°—ã€ã‚’å­¦ã‚“ã ãŒã€å®Ÿéš›å•é¡Œã‚’è§£ããŸã‚ã«ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²ã‚ã‚‹ãŸã‚ã®ã‚³ãƒ„ã‚„ã€èƒŒæ™¯ã«ã‚ã‚‹ç†è«–ã‚‚ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ ä»¥ä¸‹ã§ã¯ã€ãã‚Œã‚‰ã«ã¤ã„ã¦ç°¡å˜ã«è¿°ã¹ã‚‹ã€‚\n\nè¨“ç·´ã€æ¤œè¨¼ã€ãƒ†ã‚¹ãƒˆé›†åˆ\nä»Šã¾ã§ã®ä¾‹é¡Œã§ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆè¨“ç·´ï¼‰é›†åˆã¨ãƒ†ã‚¹ãƒˆé›†åˆã®2ã¤ã«åˆ†ã‘ã¦ã„ãŸï¼ ç ”ç©¶ã‚„å‹‰å¼·ã®ãŸã‚ã«ã¯ï¼Œã“ã®2ã¤ã«åˆ†ã‘ã‚‹ã ã‘ã§ååˆ†ã§ã‚ã‚‹ãŒï¼Œ å®Ÿéš›å•é¡Œã«é©ç”¨ã™ã‚‹éš›ã«ã¯ï¼Œè¨“ç·´é›†åˆ(training set)ï¼Œæ¤œè¨¼é›†åˆ(validation set)ï¼ˆé–‹ç™ºé›†åˆ(development set)ã¨ã‚ˆã°ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ï¼‰ï¼Œãƒ†ã‚¹ãƒˆé›†åˆ(test set)ã®3ã¤ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†ã‘ã¦å®Ÿé¨“ã‚’è¡Œã†å¿…è¦ãŒã‚ã‚‹ï¼ è¨“ç·´é›†åˆã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿ï¼‰ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ï¼Œæ¤œè¨¼é›†åˆã§å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ã†ã¾ãå‹•ãã‚ˆã†ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ï¼Œæœ€å¾Œã«ãƒ†ã‚¹ãƒˆé›†åˆã§è©•ä¾¡ã™ã‚‹ï¼ ãƒ†ã‚¹ãƒˆé›†åˆã¯å®Ÿé¨“ã§ã¯ä½¿ç”¨ã§ããªã„ã‚ˆã†ã«éš ã—ã¦ãŠãï¼æ¤œè¨¼é›†åˆã¯è¨“ç·´é›†åˆã‹ã‚‰é©å½“ãªå‰²åˆã§æŠ½å‡ºã—ã¦ã‚‚è‰¯ã„ï¼\næ˜”ã¯è¨“ç·´é›†åˆã¯\\(6\\)å‰²ã€æ¤œè¨¼é›†åˆã¯ \\(2\\)å‰²ã€ãƒ†ã‚¹ãƒˆé›†åˆã¯ \\(2\\) å‰²ã¨è¨€ã‚ã‚Œã¦ã„ãŸã€‚ã—ã‹ã—ã€æœ€è¿‘ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤§è¦æ¨¡åŒ–ã—ã¦ãŠã‚Šã€ æ¤œè¨¼ã¨ãƒ†ã‚¹ãƒˆã«ã¯ä¸€å®šæ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚Œã°ååˆ†ã§ã‚ã‚‹ã€‚ä¾‹ãˆã°ã€è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã¯ã€ \\(98\\)%ã‚’è¨“ç·´ã€æ¤œè¨¼ã¨ãƒ†ã‚¹ãƒˆã«ã¯æ®‹ã‚Šã®\\(1\\)%ãšã¤ã¨ã—ã¦ã‚‚è‰¯ã„ã€‚\n\n\nãƒã‚¤ã‚¢ã‚¹ã¨ãƒãƒªã‚¢ãƒ³ã‚¹ / éå‰°é©åˆã¨ä¸è¶³é©åˆ\næ·±å±¤å­¦ç¿’ã®ä¾‹é¡Œï¼ˆMNISTã¨ã‹Cifarã¨ã‹ï¼‰ã§ã¯ï¼Œç¾åœ¨ã®ä¸–ç•Œè¨˜éŒ²ãŒã©ã®ã‚ãŸã‚Šãªã®ã‹ãŒåˆ†ã‹ã‚‹ãŒï¼Œå®Ÿéš›å•é¡Œã«ãŠã„ã¦ã©ã“ã¾ã§å­¦ç¿’ã‚’é€²ã‚ã‚Œã°ã‚ˆã„ã®ã‹ã¯ï¼Œä¸€èˆ¬ã«ã¯åˆ†ã‹ã‚‰ãªã„ï¼ã‚ˆã‚Šä¸€èˆ¬çš„ãªæœ€é©åŒ–ç†è«–ã§ã¯ï¼Œé©å½“ãªç·©å’Œå•é¡Œã‚’ç”¨ã„ãŸé™ç•Œå€¤ï¼ˆæœ€å°åŒ–ã®å ´åˆã«ã¯ä¸‹ç•Œï¼‰ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ï¼Œèª¤å·®ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ã—ã‹ã—ï¼Œæ·±å±¤å­¦ç¿’ã§ã¯ï¼Œãã‚ŒãŒé›£ã—ã„å ´åˆãŒå¤šã„ï¼ãã®ã‚ˆã†ãªå ´åˆã«ï¼Œäººã§ãƒ†ã‚¹ãƒˆã‚’ã—ã¦ã¿ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ï¼Œå¯èƒ½ãªèª¤å·®ã‚’æ¨æ¸¬ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã‚‹ï¼\näººé–“ãŒã©ã‚“ãªã«é ‘å¼µã£ã¦ã‚‚å‡ºãªã„ãã‚‰ã„ã®èª¤å·®ï¼ˆã‚¨ãƒ©ãƒ¼ç‡ï¼‰ã‚’Bayesæœ€é©èª¤å·®(Bayesâ€ optimal error)ã¨ã‚ˆã¶ï¼\nãƒã‚¤ã‚¢ã‚¹  = è¨“ç·´èª¤å·® - Baysæœ€é©èª¤å·®ï¼ˆã‚‚ã—ãã¯äººé–“æ°´æº–èª¤å·®ï¼‰\nãƒãƒªã‚¢ãƒ³ã‚¹ = æ¤œè¨¼èª¤å·® - è¨“ç·´èª¤å·®\nãƒãƒªã‚¢ãƒ³ã‚¹ãŒå¤§ãã„çŠ¶æ…‹ã‚’éå‰°é©åˆ(overfit)ã¨ã‚ˆã¶ï¼ãƒã‚¤ã‚¢ã‚¹ãŒå¤§ãã„çŠ¶æ…‹ã‚’éå°é©åˆ(underfit)ã¨ã‚ˆã³ï¼Œã“ã‚Œã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœ€é©åŒ–ãŒååˆ†ã§ãªã„ã“ã¨ã‚’è¡¨ã™ï¼\nã¾ãšã¯è¨“ç·´é›†åˆã§ã®æ­£è§£ç‡ã‚’ä¸Šã’ã‚‹ã“ã¨ã‚’ç›®æ¨™ã«å®Ÿé¨“ã‚’ã™ã‚‹ã®ã ãŒï¼Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½ãŒã„ã¾ã„ã¡ãªçŠ¶æ…‹ã‚’ã€Œé«˜ãƒã‚¤ã‚¢ã‚¹ã€ã¨ã‚ˆã¶ï¼ã“ã‚Œã‚’æ”¹å–„ã™ã‚‹ã«ã¯ï¼Œãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¦æ¨¡ã‚’å¤§ããã—ã¦å­¦ç¿’å®¹é‡ã‚’å¤§ããã—ãŸã‚Šï¼Œè¨“ç·´æ™‚é–“ã‚’é•·ãã—ãŸã‚Šï¼Œæœ€é©åŒ–ã®æ–¹æ³•ã‚’å¤‰ãˆãŸã‚Šã™ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã‚‹ï¼\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãã“ãã“ã®æˆç¸¾ã‚’ã‚ã’ã‚‰ã‚ŒãŸã‚‰ï¼Œä»Šåº¦ã¯æ¤œè¨¼é›†åˆã«å¯¾ã—ã¦æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ï¼ã“ã‚ŒãŒã„ã¾ã„ã¡ãªçŠ¶æ…‹ãŒã€Œé«˜ãƒãƒªã‚¢ãƒ³ã‚¹ã€ã§ã‚ã‚‹ï¼ ã“ã‚Œã‚’æ”¹å–„ã™ã‚‹ã«ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ã®é‡ã‚’å¢—ã‚„ã—ãŸã‚Šï¼Œæ­£å‰‡åŒ–ãƒ»æ­£è¦åŒ–ã‚’è¡Œã£ãŸã‚Šã™ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã‚‹ï¼ã“ã‚Œã«ã¯è‰²ã€…ãªæ–¹æ³•ãŒè€ƒãˆã‚‰ã‚Œã‚‹ãŒï¼Œæ·±å±¤å­¦ç¿’ã§æ‰‹è»½ãªã®ã¯ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¿½åŠ ã—ãŸã‚Šï¼Œï¼ˆç¢ºç‡çš„é™ä¸‹æ³•ã®å ´åˆã«ã¯L2ãƒãƒ«ãƒ ã®ã‹ã‘å…·åˆã‚’è¡¨ã™ï¼‰é‡ã¿æ¸›è¡°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤§ããã—ãŸã‚Šï¼Œãƒãƒƒãƒæ­£è¦åŒ–ã‚’è¡Œã†ã“ã¨ã§ã‚ã‚‹ï¼\n\n\næ·±å±¤å­¦ç¿’ã®ãƒ¬ã‚·ãƒ”\næ·±å±¤å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ­£ã—ã„æ–¹å‘ã«å°ããŸã‚ã«ã¯ï¼Œå¤šå°‘ã®ã‚³ãƒ„ãŒã‚ã‚‹ï¼ã“ã“ã§ã¯ï¼Œãã®ã‚ˆã†ãªã‚³ãƒ„ã‚’ä¼æˆã™ã‚‹ï¼\næ¤œè¨¼é›†åˆã«ãŠã‘ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒä¸ååˆ†ãªã¨ãã«ä½•ã‚’ã™ã‚Œã°è‰¯ã„ã ã‚ã†ã‹ï¼Ÿä»¥ä¸‹ã®ã‚ˆã†ãªæ§˜ã€…ãªæ–¹æ³•ãŒæ€ã„ã¤ãï¼\n\nã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã‚‹ï¼\nã‚ˆã‚Šå¤šãã®è¨“ç·´é›†åˆã‚’é›†ã‚ã‚‹ï¼\nè¨“ç·´ã«ã‚ˆã‚Šå¤šãã®æ™‚é–“ã‚’ã‹ã‘ã‚‹ï¼\næ§˜ã€…ãªæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦ã™ï¼\nã‚ˆã‚Šå¤§ããªã‚¢ãƒ¼ã‚­ãƒ†ã‚­ã‚¯ãƒãƒ£ã«ã—ã¦ã¿ã‚‹ï¼\nã‚¢ãƒ¼ã‚­ãƒ†ã‚­ã‚¯ãƒãƒ£ã‚’å¤‰æ›´ã—ã¦ã¿ã‚‹ï¼\nãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã‚’è¿½åŠ ã—ã¦ã¿ã‚‹ï¼\nL2æ­£å‰‡åŒ–(regularizationï¼›é‡ã¿æ¸›è¡°(weight decay)ã¨åŒç¾©èª)ã‚’è¿½åŠ ã™ã‚‹ï¼\nãƒãƒƒãƒæ­£è¦åŒ–(batch normalization)ã‚’è¿½åŠ ã™ã‚‹ï¼\n\nã—ã‹ã—ï¼Œã“ã‚Œã‚‰ã‚’å ´å½“ãŸã‚Šçš„ã«é©ç”¨ã—ã¦ã‚‚æ™‚é–“ãŒã‹ã‹ã‚‹ã°ã‹ã‚Šã§ï¼ŒåŠ¹æœã¯ä¸ŠãŒã‚‰ãªã„ï¼é‡è¦ãªã®ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã¯èª¿æ•´ã™ã‚‹é‡ã¿ã®ã“ã¨ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã‚ˆã³ï¼Œãã‚Œä»¥å¤–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã‚ˆã¶ï¼‰ã®è¨­å®šã§ã‚ã‚‹ï¼ã“ã‚Œã«ã¯ï¼Œä»¥ä¸‹ã®æ‰‹é †ãŒæ¨å¥¨ã•ã‚Œã‚‹ï¼\n\nç”»åƒã®å ´åˆã«ã¯è§£åƒåº¦ã‚’è½ã¨ã—ãŸå°ã•ãªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å§‹ã‚ã¦ã€å¾ã€…ã«è§£åƒåº¦ã‚’ä¸Šã’ã¦ã„ãã€‚\nè¨“ç·´é›†åˆã«å¯¾ã—ã¦æå‡ºé–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ï¼ã§ãã‚Œã°ä¸‹é™ï¼ˆäººé–“ã®æ°´æº–ï¼‰ã«è¿‘ã¥ãã‚ˆã†ã«ã™ã‚‹ï¼ã“ã‚ŒãŒã†ã¾ãã„ã‹ãªã„å ´åˆã«ã¯ï¼Œå­¦ç¿’ç‡ã‚’é©æ­£ãªå€¤ã«è¨­å®šã™ã‚‹ã€‚å­¦ç¿’ç‡ãŒå°ã•ã™ãã‚‹ã¨éå°‘é©åˆã«ãªã‚Šã€å¤§ãã™ãã‚‹æœ€é©åŒ–ã®æ¢ç´¢ãŒç™ºæ•£ã™ã‚‹ã€‚å­¦ç¿’ç‡ã‚’é©æ­£ãªå€¤ã«ã—ã¦ã‚‚ã€æå‡ºé–¢æ•°ã®å€¤ãŒæƒ³å®šã‚ˆã‚Šã‚‚å¤§ãã„å ´åˆã«ã¯ã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚­ã‚¯ãƒãƒ£ã‚’è©¦ã™ã‹ï¼Œç•°ãªã‚‹æœ€é©åŒ–æ‰‹æ³•ã‚’è©¦ã™ï¼ãƒãƒƒãƒæ­£è¦åŒ–ã‚’ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«è¿½åŠ ã—ã€æœ€é©åŒ–ã—ã‚„ã™ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆæ®‹å·®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ï¼‰ã‚’é¸æŠã™ã‚‹ã“ã¨ã‚‚å¿˜ã‚Œã¦ã¯ãªã‚‰ãªã„ã€‚ ãƒ¡ãƒ¢ãƒªã‚„è¨ˆç®—é€Ÿåº¦ãŒååˆ†ã§ãªã„ã¨ãã«ã¯ã€å˜ç²¾åº¦è¨ˆç®—ã‚’ã™ã‚‹ã‹ã€ã‚ˆã‚Šå¤§ããªãƒ¡ãƒ¢ãƒªã‚’ã‚‚ã¤GPUã«å¤‰æ›´ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã™ã‚‹ã€‚\nè»¢ç§»å­¦ç¿’ã‚’è¡Œã£ã¦ã„ã‚‹å ´åˆã«ã¯ã€å›ºå®šã—ã¦ã„ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªç”±ã«å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã‹ã‚‰ã€å†ã³è¨“ç·´ã‚’è¡Œã†ã€‚\nï¼ˆä¸Šã®æ‰‹é †ã¨ä¸¦è¡Œã—ã¦ï¼‰æ¤œè¨¼é›†åˆã«å¯¾ã—ã¦ç›®çš„ã¨ã™ã‚‹è©•ä¾¡å°ºåº¦ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ã‚’é”æˆã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã‚’è¡Œã†ï¼ã“ã‚ŒãŒã†ã¾ãã„ã‹ãªã„ï¼ˆéå‰°é©åˆã—ã¦ã„ã‚‹ï¼‰å ´åˆã«ã¯ï¼Œãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¿½åŠ ã™ã‚‹ã‹ï¼Œé‡ã¿æ¸›è¡°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã‹ï¼Œè¨“ç·´é›†åˆã‚’å¢—ã‚„ã™ã‹ï¼Œãƒ‡ãƒ¼ã‚¿å¢—å¤§ã‚’è¡Œã†ï¼\nãƒ†ã‚¹ãƒˆé›†åˆã«å¯¾ã—ã¦è‰¯ã„çµæœãŒå‡ºã‚‹ã‚ˆã†ã«è¨“ç·´ã‚’è¡Œã†ï¼ã“ã‚ŒãŒã†ã¾ãã„ã‹ãªã„å ´åˆã«ã¯ï¼Œæ¤œè¨¼é›†åˆã‚’å¢—ã‚„ã™ã‹ï¼Œãƒ‡ãƒ¼ã‚¿å¢—å¤§ã‚’è¡Œã†ï¼\nå®Ÿå•é¡Œã«å¯¾ã™ã‚‹æ€§èƒ½è©•ä¾¡ã‚’è¡Œã†ï¼ã“ã‚ŒãŒã†ã¾ãã„ã‹ãªã„å ´åˆã«ã¯ï¼Œæ¤œè¨¼é›†åˆã‚„ãƒ†ã‚¹ãƒˆé›†åˆãŒå®Ÿå•é¡Œã‚’åæ˜ ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’èª¿ã¹ï¼Œé©å®œå¢—ã‚„ã™ï¼\n\nä¸Šã§ã¯ã„ã•ã•ã‹æŠ½è±¡çš„ã«æ‰‹é †ã‚’ç´¹ä»‹ã—ãŸãŒï¼Œå…·ä½“çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é©æ­£åŒ–ã¯ä»¥ä¸‹ã®æ‰‹é †ãŒæ¨å¥¨ã•ã‚Œã¦ã„ã‚‹ï¼\n\nãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚­ã‚¯ãƒãƒ£ï¼‰ã‚’é¸æŠã™ã‚‹éš›ã«ã¯ï¼Œè§£ããŸã„å•é¡Œã«ä¼¼ãŸå•é¡Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®æˆç¸¾ã‚’ https://dawn.cs.stanford.edu/benchmark/ ã‚„https://benchmarks.ai/ ã§èª¿ã¹ã¦ï¼Œãã“ã§ä¸Šä½ã®ï¼ˆã‹ã¤ç°¡å˜ãªï¼‰ã‚‚ã®ã‚’é¸æŠã™ã‚‹ï¼ãŸã¨ãˆã°ï¼Œç”»åƒã‹ã‚‰ç‰©ä½“ã‚’å½“ã¦ãŸã„å ´åˆã«ã¯ï¼Œç”»åƒã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹åˆ†ã‘ãªã‚‰ResNetï¼ˆãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚‹ãªã‚‰DenseNetã‚„Wide ResNetï¼‰ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸã‚‚ã®ï¼Œç”»åƒåˆ†é¡ï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ãªã‚‰UNETã‚’é¸ã¶ï¼\nãŸã ã—ç«¶æŠ€ä¼šã§ä¸Šä½ã®ã‚‚ã®ã¯å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆãŒå¤šã„ï¼ãƒ¢ãƒ‡ãƒ«ã®è¦æ¨¡ãŒå¢—å¤§ã™ã‚‹ã«ã—ãŸãŒã„èª¤å·®ã¯å°ã•ããªã‚‹ï¼ˆç²¾åº¦ãŒä¸ŠãŒã‚‹ï¼‰ãŒï¼Œãã®ä¸€æ–¹ã§è¨ˆç®—æ™‚é–“ãŒå¢—åŠ ã™ã‚‹ï¼è§£ãã¹ãå•é¡Œã®è¤‡é›‘ã•ã‚’è€ƒæ…®ã—ã¦ãªã‚‹ã¹ãå°ã•ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã¦ï¼Œååˆ†ãªç²¾åº¦ãŒå¾—ã‚‰ã‚Œãªã‹ã£ãŸã¨ãã«ï¼Œå¤§ãã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™ã¨ã„ã†æ–¹æ³•ãŒæ¨å¥¨ã•ã‚Œã‚‹ï¼\nã™ã§ã«å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ãŒã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãŸã¨ãˆã°resnet34ï¼‰ã‚’ç”¨ã„ï¼Œè»¢ç§»å­¦ç¿’ã‚’è¡Œã†å ´åˆã«ã¯ï¼Œæœ€çµ‚å±¤ä»¥å¤–ã®é‡ã¿ã‚’å¤‰ãˆãªã„ã‚ˆã†ãªçŠ¶æ…‹ã§è¨“ç·´ã‚’è¡Œã†ï¼å­¦ç¿’ç‡ã‚’lr_find()ã§å¯è¦–åŒ–ã—ï¼Œæå‡ºé–¢æ•°ãŒä¸‹é™ã—ã¦ã„ã‚‹é€”ä¸­ã®ç¯„å›²ã‚’æ±‚ã‚ã‚‹ï¼\nå¾—ã‚‰ã‚ŒãŸé©æ­£ãªå­¦ç¿’ç‡ã‚’ç”¨ã„ã¦ï¼Œæœ€çµ‚å±¤ã ã‘ã‚’æ•°ã‚¨ãƒãƒƒã‚¯è¨“ç·´ã™ã‚‹ï¼æå‡ºé–¢æ•°ã‚„ç²¾åº¦ã®æ¨ç§»ã‚’å¯è¦–åŒ–ã—ï¼Œæ­£ã—ãè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ï¼\nä¸Šå±¤ã‚‚è¨“ç·´ã§ãã‚‹ã‚ˆã†ã«è¨­å®šã—ï¼Œå†ã³lr_find()ã§é©åˆ‡ãªå­¦ç¿’ç‡ã‚’æ¢ç´¢ã™ã‚‹ï¼\næœ€ä¸‹å±¤ã‚’ä¸Šã§æ±‚ã‚ãŸå­¦ç¿’ç‡ã¨ã—ï¼Œä¸Šå±¤ã®å›ºã¾ã‚Šã¯ä¸‹å±¤ã®å›ºã¾ã‚Šã‚ˆã‚Šã‚„ã‚„ï¼ˆç”»åƒã®å ´åˆã«ã¯10åˆ†ã®1ï¼Œãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã«ã¯0.26å€ï¼‰å°ã•ã‚ã«ãªã‚‹ã‚ˆã†ã«è¨­å®šã—ï¼Œéå‰°é©åˆã«ãªã‚‹ã¾ã§è¨“ç·´ã™ã‚‹ï¼ï¼ˆfastaiã§ã¯å±¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’3å±¤ã«ãªã‚‹ã‚ˆã†ã«ã¾ã¨ã‚ã¦ã„ã‚‹ï¼ï¼‰\nã™ãã«éå‰°é©åˆã«ãªã£ã¦ã„ã‚‹å ´åˆã«ã¯ï¼Œãã‚Œã‚’æŠ‘æ­¢ã™ã‚‹æ–¹æ³•ã‚’å–ã‚Šå…¥ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ä»¥ä¸‹ã®é †ã«è©¦ã™.\n\nã‚‚ã£ã¨ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã‚‹ï¼\nãƒ‡ãƒ¼ã‚¿å¢—å¤§ã‚’è¡Œã†ï¼\nã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒ¢ãƒ‡ãƒ«ï¼‰ã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã‚„ãƒãƒƒãƒæ­£è¦åŒ–ã‚’è¿½åŠ ã™ã‚‹ï¼\næ­£å‰‡åŒ–ã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿æ¸›è¡°ç‡: weight decay(wd)ï¼‰ã‚’å¤§ãã‚ã«ã™ã‚‹ï¼\nã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å˜ç´”åŒ–ã™ã‚‹ï¼\n\nç”»åƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆã«ã¯ï¼Œä¸Šã®æ‰‹é †ã‚’è§£åƒåº¦ã‚’ä¸‹ã’ã¦è¡Œã„ï¼Œå¾ã€…ã«è§£åƒåº¦ã‚’ä¸Šã’ã¦ç¹°ã‚Šè¿”ã™ï¼ä»–ã®å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã«ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ç”¨ã„ã¦ä¸Šã®æ‰‹é †ã‚’è¡Œã„ï¼Œé©åˆ‡ãªçµæœãŒå‡ºãŸã‚‰å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦æœ¬å®Ÿé¨“ã‚’è¡Œã†ï¼\n\n\n\nL2æ­£å‰‡åŒ–ï¼ˆé‡ã¿æ¸›è¡°ï¼‰ãŒéå‰°é©åˆã‚’å‰Šæ¸›ã™ã‚‹ç›´æ„Ÿçš„ãªç†ç”±\n\né‡ã¿æ¸›è¡°ç‡ãŒå¤§ãããªã‚‹ã¨ï¼Œé‡ã¿ \\(w\\) ã¯å°ã•ããªã‚Šï¼Œ0ã«ãªã‚‹ã‚‚ã®ãŒå¢—ãˆã‚‹ï¼ãã‚Œã«ã‚ˆã£ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒã‚ˆã‚Šç–ã«ãªã‚Šï¼Œï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¨åŒæ§˜ã«ï¼‰éå‰°é©åˆã‚’å‰Šæ¸›ã™ã‚‹ï¼\ntanhãªã©ã®éç·šå½¢ãªæ´»æ€§åŒ–é–¢æ•°ã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆï¼Œé‡ã¿æ¸›è¡°ç‡ lambda ãŒå¤§ãããªã‚‹ã¨ \\(w\\) ãŒå°ã•ããªã‚‹ã®ã§ï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¸ã®å…¥åŠ›ã‚‚å°ã•ããªã‚‹ï¼ˆ0ã«è¿‘ããªã‚‹ï¼‰ï¼tanhãªã©ã®æ´»æ€§åŒ–é–¢æ•°ã‚’å¯è¦–åŒ–ã™ã‚‹ã¨åˆ†ã‹ã‚‹ã‚ˆã†ã«0ä»˜è¿‘ã§ã¯ç·šå½¢é–¢æ•°ã«è¿‘ã„å½¢ã‚’ã—ã¦ã„ã‚‹ï¼ã—ãŸãŒã£ã¦ï¼Œéç·šå½¢ãªæ´»æ€§åŒ–é–¢æ•°ã‚‚ç·šå½¢é–¢æ•°ã¨åŒã˜ã‚ˆã†ãªåƒãã‚’ã™ã‚‹ã‚ˆã†ã«ãªã‚Šï¼Œã“ã‚Œã«ã‚ˆã£ã¦éå‰°é©åˆãŒå‰Šæ¸›ã§ãã‚‹ï¼\n\n\n\nãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\nãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ä»¥ä¸‹ã®é †ã§é‡è¦ã§ã‚ã‚‹ã€‚\n\nå­¦ç¿’ç‡ (learning rate: lr)\næ…£æ€§é …ï¼ˆãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼‰(momentum)\nãƒŸãƒ‹ãƒãƒƒãƒã®å¤§ãã•\néš ã‚Œå±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°\nå±¤ã®æ•°\nå­¦ç¿’ç‡ã®æ¸›ã‚‰ã—æ–¹\næ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (weight decay: wd)\næ´»æ€§åŒ–é–¢æ•°\nAdamã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n\n\n\nè©•ä¾¡å°ºåº¦ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰\nã“ã“ã§ã¯fastaiã§ä½¿ã‚ã‚Œã‚‹ä»£è¡¨çš„ãªè©•ä¾¡å°ºåº¦ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ã«ã¤ã„ã¦è§£èª¬ã™ã‚‹ï¼\n\næ­£è§£ç‡ (accuracy)\n\nå…¥åŠ›ã®ä¸­ã§æœ€å¤§å€¤ã®ã‚¯ãƒ©ã‚¹ãŒæ­£è§£ã‚¯ãƒ©ã‚¹ã¨ä¸€è‡´ã—ã¦ã„ã‚‹å‰²åˆï¼æ­£ç­”ç‡ï¼Œç²¾åº¦ã¨è¨³ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ï¼ æ­£è§£ãŒ1ã¤ã®ã‚¯ãƒ©ã‚¹ã«å±ã—ã¦ã„ã‚‹ã¨ãï¼ˆã“ã‚Œã‚’1ãƒ©ãƒ™ãƒ«å•é¡Œã¨ã‚ˆã¶ï¼‰ã«ç”¨ã„ã‚‰ã‚Œã‚‹ï¼\nä¾‹ï¼š å…¥åŠ›ã¨ã—ã¦3ã¤ã®ã‚¯ãƒ©ã‚¹ã‹ã‚‰æˆã‚‹5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸ãˆã‚‹ï¼æ­£è§£ã¯ã™ã¹ã¦ã‚¯ãƒ©ã‚¹1ã¨ã™ã‚‹ï¼å…¥åŠ›ã®ä¸­ã§å€¤ãŒæœ€å¤§ã®ã‚‚ã®ã¯ï¼Œä¸Šã®ã‚³ãƒ¼ãƒ‰ã®ä¸­ã«ã‚ã‚‹ã‚ˆã†ã«argmaxãƒ¡ã‚½ãƒƒãƒ‰ã§æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼å¾—ã‚‰ã‚ŒãŸ(1,0,1,0,0)ãŒæ­£è§£(1,1,1,1,1)ã¨ä¸€è‡´ã—ã¦ã„ã‚‹å‰²åˆã¯ 0.4 ã¨è¨ˆç®—ã§ãã‚‹ï¼\n    from fastai.metrics import *\n    in_ = torch.Tensor([ [0.3,0.5,0.2],\n                         [0.6,0.2,0.2],\n                         [0.1,0.6,0.3],\n                         [0.9,0.0,0.1],\n                         [0.8,0.1,0.1],\n                        ])\n    targs = torch.Tensor([1,1,1,1,1]).long()\n    print(in_.argmax(dim=-1).view(5,-1))\n    print(accuracy(in_, targs))\n\n&gt;&gt;&gt;\n    tensor([[1],\n            [0],\n            [1],\n            [0],\n            [0]])\n    tensor(0.4000)\n2å€¤åˆ†é¡ã®å ´åˆã«ã¯ï¼Œ æ­£è§£ã‹å¦ã‹(true/false)ã¨é™½æ€§ã¨äºˆæ¸¬ã—ãŸã‹å¦ã‹(positive/negative)ãŒã‚ã‚‹ã®ã§ï¼Œä»¥ä¸‹ã®4é€šã‚Šã®å ´åˆãŒã‚ã‚‹ï¼\n\nTN : çœŸé™°æ€§ (true negative)\nFP : å½é™½æ€§ (false positive)\nFN : å½é™°æ€§ (false negative)\nTP : çœŸé™½æ€§ (true positive)\n\næ­£è§£ç‡ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼\n\\[\n\\mathrm{accuracy} = \\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{FN}+\\mathrm{FP}+\\mathrm{TN}}\n\\]\n\né–¾å€¤ä»˜ãæ­£è§£ç‡\n\näºˆæ¸¬å€¤ã«å¯¾ã™ã‚‹ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã®å€¤ãŒï¼Œä¸ãˆãŸé–¾å€¤ï¼ˆè¦å®šå€¤ã¯0.5ï¼‰ã‚ˆã‚Šå¤§ãã„ã¨ãã«1ï¼Œãã‚Œä»¥å¤–ã®ã¨ã0ã¨è¨ˆç®—ã—ï¼Œãã®çµæœã¨æ­£è§£ã‚’æ¯”è¼ƒã—ãŸã¨ãã®æ­£è§£ç‡ï¼\nä¾‹ï¼š ãƒ©ãƒ³ãƒ€ãƒ ãªæ¨™æº–æ­£è¦åˆ†å¸ƒã¨ã—ã¦ä¸ãˆãŸ5ã¤ã®äºˆæ¸¬å€¤ã«å¯¾ã—ã¦ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§[0,1]ã®å€¤ã«å¤‰æ›ã—ï¼Œé–¾å€¤0.5 ã‚ˆã‚Šå¤§ãã„ã‚‚ã®ã¨æ­£è§£ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ï¼Œé–¾å€¤ä»˜ãæ­£è§£ç‡0.6ã‚’å¾—ã‚‹ï¼\n    y_pred = torch.randn(5)\n    y_true = torch.Tensor([1,1,1,1,1]).long()\n    print(\"y_pred = \", y_pred)\n    print(\"sigmoid = \", y_pred.sigmoid())\n    print(accuracy_thresh(y_pred, y_true))\n\n&gt;&gt;&gt;\n    y_pred =  tensor([ 0.8596,  0.3210, -0.1176,  1.0431, -0.7974])\n    sigmoid =  tensor([0.7026, 0.5796, 0.4706, 0.7394, 0.3106])\n    tensor(0.6000)\n\nãƒˆãƒƒãƒ— \\(k\\) æ­£è§£ç‡\n\nå…¥åŠ›å€¤ãŒå¤§ãã„ã‚‚ã®ã‹ã‚‰kå€‹é¸æŠã—ï¼Œãã‚Œã‚‰ã‚’æ­£è§£ã¨æ¯”è¼ƒã—ãŸã¨ãã®æ­£è§£ç‡ï¼\nä¾‹ï¼š æ­£è§£ç‡ã¨åŒã˜ä¾‹é¡Œã‚’ç”¨ã„ã‚‹ï¼ãƒˆãƒƒãƒ—2ã®ã‚¯ãƒ©ã‚¹ã‚’å‡ºåŠ›ã™ã‚‹ã¨ï¼Œæ­£è§£ã®1ã¯2ç•ªç›®ã«ã¯å…¥ã£ã¦ã„ãªã„ã®ã§ï¼Œãƒˆãƒƒãƒ—1ã®æ­£è§£ç‡ã¨åŒã˜0.4ã‚’å¾—ã‚‹ï¼\n    print(in_.topk(k=2, dim=-1)[1])\n    print(top_k_accuracy(in_,targs,k=2))\n\n&gt;&gt;&gt;\n    tensor([[1, 0],\n            [0, 2],\n            [1, 2],\n            [0, 2],\n            [0, 2]])\n    tensor(0.4000)\n\nãƒ€ã‚¤ã‚¹ä¿‚æ•°(dice coefficient)\n\nåˆ†å‰²å•é¡Œã§ç”¨ã„ã‚‰ã‚Œã‚‹é›†åˆã®é¡ä¼¼åº¦ã‚’è¡¨ã™è©•ä¾¡å°ºåº¦ã§ã‚ã‚Šï¼Œå¼•æ•° iou (intersection over union) ãŒçœŸã®ã¨ãã«ã¯ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã™ã‚‹ï¼\n\\[DICE(A,B)=\\frac{|A \\cap B|}{|A|+|B|-|A \\cap B| + 1 }\\]\nå¼•æ•°iouãŒå½ï¼ˆæ—¢å®šå€¤ï¼‰ã®ã¨ãã«ã¯ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã™ã‚‹ï¼\n\\[DICE(A,B)=\\frac{2|A \\cap B|}{|A|+|B|}\\]\nä¾‹ï¼š\n    print(\"iou=False:\", dice(in_,targs,iou=False))\n    print(\"iou=True:\",dice(in_,targs,iou=True))\n\n&gt;&gt;&gt;\n    iou=False: tensor(0.5714)\n    iou=True: tensor(0.3333)\n\nèª¤å·®ç‡ (error rate)\n\n\\(1-\\)æ­£è§£ç‡ã§ã‚ã‚Šï¼Œä¸Šã®ä¾‹é¡Œã§ã¯\\(1-0.4=0.6\\)ã¨ãªã‚‹ï¼\n\næ±ºå®šä¿‚æ•°(coefficient of determination) \\(R^2\\)\n\nå›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ã©ã‚Œãã‚‰ã„èª¬æ˜ã§ãã¦ã„ã‚‹ã‹ï¼ˆå›å¸°åˆ†æã®ç²¾åº¦ï¼‰ã‚’è¡¨ã™æŒ‡æ¨™ã§ã‚ã‚Šï¼Œ1ã«è¿‘ã„ã»ã©ç²¾åº¦ãŒè‰¯ã„ã¨è§£é‡ˆã§ãã‚‹ï¼\n\\[R^2 = 1 - {\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\over \\sum_{i=1}^n (y_i-\\bar{y})^2 }\\]\nã“ã®å®šç¾©ã ã¨ï¼ˆè¨˜å·ãŒ\\(R^2\\)ã§ã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšï¼‰è² ã«ãªã‚‹å ´åˆã‚‚ã‚ã‚‹ã®ã§ï¼Œæ³¨æ„ã‚’è¦ã™ã‚‹ï¼æœ€å¤§å€¤ã¯1ã§èª¤å·®ãŒ0ã®çŠ¶æ…‹ã§ã‚ã‚‹ï¼\\(R^2\\)ãŒ0ã¨ã¯ï¼Œå¹³å‡ã§äºˆæ¸¬ã‚’ã—ãŸå ´åˆã¨åŒã˜ç²¾åº¦ã¨ã„ã†æ„å‘³ã§ã‚ã‚Šï¼Œè² ã®å ´åˆã¯å¹³å‡å€¤ã‚ˆã‚Šæ‚ªã„äºˆæ¸¬ã‚’æ„å‘³ã™ã‚‹ï¼\n\nå¹³å‡è‡ªä¹—èª¤å·® (mean squared error)\n\nèª¤å·®ã®è‡ªä¹—ã®å¹³å‡å€¤ã§ã‚ã‚Šï¼Œ\\(i\\)ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã®æ­£è§£ï¼ˆç›®æ¨™å€¤ï¼‰ã‚’\\(y_i\\)ï¼Œäºˆæ¸¬å€¤ã‚’\\(\\hat{y}_{i}\\) ã¨ã—ãŸã¨ãï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼\n\\[MSE = \\frac{\\sum_{i=1}^n (\\hat y_i - y_i)^2}{n}\\]\nã“ã‚Œã®å¹³æ–¹æ ¹ã‚’ã¨ã£ãŸã‚‚ã®ãŒroot_mean_squared_error (RMSE) ã§ã‚ã‚‹ï¼\n\\[RMSE =\\sqrt{\\frac{\\sum_{i=1}^n (\\hat y_i - y_i)^2}{n}}\\]\n\nå¹³å‡çµ¶å¯¾èª¤å·® (mean absolute error)\n\nèª¤å·®ã®çµ¶å¯¾å€¤ã®å¹³å‡å€¤ã§ã‚ã‚‹ï¼\n\\[MAE = \\frac{\\sum_{i=1}^n |\\hat y_i - y_i|}{n}\\]\n\nå¹³å‡è‡ªä¹—å¯¾æ•°èª¤å·® (mean squared logarithmic error)\n\näºˆæ¸¬å€¤ï¼Œæ­£è§£ã¨ã‚‚ã«å¯¾æ•°ã‚’ã¨ã£ãŸã‚‚ã®ã§è©•ä¾¡ã—ãŸå¹³å‡è‡ªä¹—èª¤å·®ã§ã‚ã‚‹ï¼\nã“ã‚Œã®å¹³æ–¹æ ¹ã‚’ã¨ã£ãŸã‚‚ã®ãŒroot mean squared logarithmic error (RMSLE) ã§ã‚ã‚‹ï¼\n\nMAPE å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·® (mean absolute percentage error)\n\n\\[MAPE = \\frac{\\sum_{i=1}^n | (\\hat y_i - y_i)/y_i) |}{n}\\]\n\né©åˆç‡ (precision)ï¼šæ­£ã¨äºˆæ¸¬ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ï¼Œå®Ÿéš›ã«æ­£ã§ã‚ã‚‹ã‚‚ã®ã®å‰²åˆ\n\n\\[\n     \\mathrm{precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}\n\\]\n\nå†ç¾ç‡ (recall)ï¼šå®Ÿéš›ã«æ­£ã§ã‚ã‚‹ã‚‚ã®ã®ã†ã¡ï¼Œæ­£ã§ã‚ã‚‹ã¨äºˆæ¸¬ã•ã‚ŒãŸã‚‚ã®ã®å‰²åˆ\n\n\\[\n\\mathrm{recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\n\\]\n\n\\(f\\)ãƒ™ãƒ¼ã‚¿\n\né©åˆç‡ã¨å†ç¾ç‡ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \\(\\beta\\) ã§èª¿æ•´ã—ãŸè©•ä¾¡å°ºåº¦ã§ã‚ã‚Šï¼Œä¸»ã«2å€¤åˆ†é¡ã§ç”¨ã„ã‚‰ã‚Œã‚‹ï¼\n\\[f_\\beta = (1 + \\beta^2)  \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\mathrm{precision}) + \\mathrm{recall}}\\]\n\nå¯„ä¸ç‡(explained variance)\n\nã€Œ\\(1 -\\)èª¤å·®ã®åˆ†æ•£/æ­£è§£ã®åˆ†æ•£ã€ã¨å®šç¾©ã•ã‚Œã‚‹ï¼\n\nè‡ªåˆ†ã§æ–°ã—ã„è©•ä¾¡å°ºåº¦ã‚’ä½œã‚‹æ–¹æ³•\nä»–ã®è©•ä¾¡å°ºåº¦ã‹ã‚‰æ–°ãŸã«è©•ä¾¡å°ºåº¦ã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ï¼Œæ¨™æº–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®functoolsã«ã‚ã‚‹partialã‚’ä½¿ã†ã¨ç°¡å˜ã«ã§ãã‚‹ï¼fastaiã§ã¯ã™ã§ã«importã—ãŸçŠ¶æ…‹ã«ã‚ã‚‹ã®ã§ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«å‘¼ã³å‡ºã›ã°è‰¯ã„ï¼\n    acc_02 = partial(accuracy_thresh, thresh=0.2)\n    f_05 = partial(fbeta, beta=0.5)\næœ€åˆã®è¡Œã§ã¯ï¼Œé–¾å€¤ä»˜ãæ­£è§£ç‡ã«å¯¾ã—ã¦ï¼Œé–¾å€¤ã‚’0.2ã«å›ºå®šã—ãŸè©•ä¾¡å°ºåº¦acc_02ã‚’ç”Ÿæˆã—ï¼Œæ¬¡ã®è¡Œã§ã¯fãƒ™ãƒ¼ã‚¿ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆbeta) ã‚’0.5ã«å›ºå®šã—ãŸè©•ä¾¡å°ºåº¦f_05ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ï¼",
    "crumbs": [
      "fastaiã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "18pydantic.html",
    "href": "18pydantic.html",
    "title": "Pydanticã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼",
    "section": "",
    "text": "Pydantic ã¯ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚ã‚‹ï¼\nPythonã¯ï¼Œå‹•çš„ã«å‹ä»˜ã‘ã‚’è¡Œã†ã®ãŒç‰¹å¾´ã§ã‚ã‚Šï¼Œãã‚Œã¯åˆ©ç‚¹ã§ã‚‚ã‚ã‚Šå¼±ç‚¹ã§ã‚‚ã‚ã‚‹ï¼ãŸã¨ãˆã°ï¼Œä»¥ä¸‹ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãŒæ›¸ã‘ã‚‹ï¼\n\nx = 1 #æ•´æ•°å‹ int ã«å‹ä»˜ã‘\ntype(x)\n\nint\n\n\n\nx = \"Hello\"\ntype(x)\n\nstr\n\n\nå‹ï¼ˆã‚¿ã‚¤ãƒ—ï¼‰ã‚’æ°—ã«ã™ã‚‹å¿…è¦ãŒãªã„ã®ã§ï¼Œåˆå­¦è€…ãŒæ°—æ¥½ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã‘ã‚‹ã¨ã„ã†ã¯åˆ©ç‚¹ã§ã‚ã‚‹ãŒï¼Œ ã¡ã‚ƒã‚“ã¨ã—ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ããŸã„äººã«ã¨ã£ã¦ã¯ï¼Œã“ã®ä»•æ§˜ã¯å¬‰ã—ããªã„ï¼\nãã®ãŸã‚ï¼Œæœ€è¿‘ã®Pythonã§ã¯å‹ãƒ’ãƒ³ãƒˆã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸï¼ãŸã¨ãˆã°ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«æ•´æ•°ã‚’2å€ã—ãŸæ•´æ•°ã‚’è¿”ã™é–¢æ•°ã‚’å®šç¾©ã§ãã‚‹ï¼\n\ndef multiple2( x:int ) -&gt; int:\n    return x*2\nmultiple2(100)\n\n200\n\n\nã“ã‚Œã¯ï¼Œå¼•æ•°ã®xã‚’æ•´æ•°å‹ int ã§ï¼Œè¿”å€¤ã‚‚æ•´æ•°å‹ã§ã‚ã‚‹ã‚ˆã†ã«å‹ãƒ’ãƒ³ãƒˆã‚’ä¸ãˆãŸã‚‚ã®ã§ã‚ã‚‹ãŒï¼Œ ã“ã‚Œã¯å˜ã«ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿ã‚„ã™ãã™ã‚‹ãŸã‚ã®ãƒ’ãƒ³ãƒˆï¼ˆé£¾ã‚Šï¼‰ã§ã‚ã‚‹ãŸã‚ï¼Œå®Ÿéš›ã«ã¯æ–‡å­—åˆ—ã‚’å¼•æ•°ã¨ã—ã¦ä¸ãˆã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã—ãªã„ï¼\n\nmultiple2(\"Hello\")\n\n'HelloHello'\n\n\nã“ã†ã„ã£ãŸäºˆæœŸã—ãªã„çµæœã‚’å‡ºã•ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®æ‰‹æ®µãŒãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ (data validiation) ã§ã‚ã‚‹ï¼ Pydanticã‚’ä½¿ã†ã¨ï¼Œãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãŒå®¹æ˜“ã«ãªã‚‹ã ã‘ã§ãªãï¼Œã‚¯ãƒ©ã‚¹ã‚’è¨­è¨ˆã™ã‚‹ã®ãŒæ¥½ã«ãªã‚‹ï¼\næ—©é€Ÿä½¿ã£ã¦ã¿ã‚ˆã†ï¼ã¾ãšã¯ï¼ŒPydanticã®BaseModelã‚¯ãƒ©ã‚¹ã‹ã‚‰æ´¾ç”Ÿã•ã›ã¦Userã‚¯ãƒ©ã‚¹ã‚’ä½œã£ã¦ã¿ã‚‹ï¼ ã“ã®ã‚¯ãƒ©ã‚¹ã¯ï¼Œæ•´æ•°å€¤ã‚’ã¨ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ id ã¨æ–‡å­—åˆ—ã®åå‰ name ã®2ã¤ã®å±æ€§ï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã‚’ã‚‚ã¤ï¼\n\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    id: int\n    name: str = \"Mikio Kubo\"\n\nidã¯å¿…é ˆã§ã‚ã‚Šçœç•¥ã§ããªã„ï¼ä¸€æ–¹ï¼Œnameã¯æ—¢å®šå€¤ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã®ã§çœç•¥ã§ãã‚‹ï¼ ä»¥ä¸‹ã§ã¯ï¼Œæ–‡å­—åˆ— â€˜123â€™ ã§idã‚’æŒ‡å®šã—ã¦ã„ã‚‹ãŒï¼Œå‹æŒ‡å®šæ©Ÿèƒ½ã§è‡ªå‹•çš„ã«æ•´æ•°å€¤ã«å¤‰æ›ã•ã‚Œã‚‹ï¼ ã¾ãŸï¼Œnameã¯çœç•¥ã™ã‚‹ã¨æ—¢å®šå€¤ãŒä»£å…¥ã•ã‚Œã‚‹ï¼\n\nuser = User(id='123')\nuser\n\nUser(id=123, name='Mikio Kubo')\n\n\n\n\n\nidãªã—ã§Userã‚¯ãƒ©ã‚¹ã‚’ä½¿ã†ã¨ã©ã†ãªã‚‹ã‹ï¼Ÿ\nnameå¼•æ•°ã«è‡ªåˆ†ã®åå‰ã‚’å…¥ã‚Œã‚‹ã¨ã©ã†ãªã‚‹ã‹ï¼Ÿ\n\nPydanticã§ä½œã£ãŸã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ï¼Œè¾æ›¸ã«å¤‰æ›ã§ãã‚‹ï¼ å¤‰æ›ã«ã¯model_dumpãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼è¾æ›¸ã®ã‚­ãƒ¼ã¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«ãªã‚‹ï¼\n\ndumped_user = user.model_dump()\nprint(type(dumped_user))\ndumped_user\n\n&lt;class 'dict'&gt;\n\n\n{'id': 123, 'name': 'Mikio Kubo'}\n\n\nãƒ‡ãƒ¼ã‚¿ã®WebçµŒç”±ã§ã®äº¤æ›ã®éš›ã«ã¯ï¼ŒJSON (JavaScript Object Notation) å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒä¾¿åˆ©ã§ã‚ã‚‹ï¼ å¤‰æ›ã«ã¯model_dump_jsonãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼\n\njson_user = user.model_dump_json()\nprint(type(json_user))\njson_user\n\n&lt;class 'str'&gt;\n\n\n'{\"id\":123,\"name\":\"Mikio Kubo\"}'\n\n\nã‚¯ãƒ©ã‚¹ã‹ã‚‰ä½œã‚‰ã‚ŒãŸè¾æ›¸ã‚„JSONã‹ã‚‰ï¼Œã‚¯ãƒ©ã‚¹ã‚’å†ç¾ã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ï¼ è¾æ›¸ã‹ã‚‰ã¯model_validateãƒ¡ã‚½ãƒƒãƒ‰ï¼ŒJSONã‹ã‚‰ã¯model_validate_jsonãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼\n\nprint( user.model_validate(dumped_user) )\nprint( user.model_validate_json(json_user) )\n\nid=123 name='Mikio Kubo'\nid=123 name='Mikio Kubo'\n\n\n\n\n\n\nè‡ªåˆ†ã®åå‰ã‚’nameã«è¨­å®šã—ãŸUserã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œã‚Šï¼Œãã‚Œã‚’è¾æ›¸ã«å¤‰æ›ã›ã‚ˆï¼\nä»Šåº¦ã¯JSONå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã›ã‚ˆï¼\nå¤‰æ›ã—ãŸè¾æ›¸ã¨JSONã‹ã‚‰ï¼Œå…ƒã®ã‚¯ãƒ©ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã›ã‚ˆï¼\n\nFieldã‚¯ãƒ©ã‚¹ã§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å€¤ã®æ—¢å®šå€¤ã‚„ç¯„å›²ã®æŒ‡å®šãªã©æ§˜ã€…ãªæƒ…å ±ã‚’ä»˜åŠ ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ ä»¥ä¸‹ã®ä¾‹ã§ã¯ï¼Œidã¯ge(greater than equal)ã‚’ç”¨ã„ã¦1ä»¥ä¸Šã®å€¤ã«åˆ¶é™ã—ï¼Œ åå‰ã®æ—¢å®šå€¤ (default value) ã¯Noneã¨ã—ã¦ã„ã‚‹ï¼\n\nfrom pydantic import Field\n\nclass User(BaseModel):\n    id: int   = Field(ge=1)\n    name: str = Field(default=None)\n\nuser = User(id=1)\nprint(user)\n\nid=1 name=None\n\n\nä¸Šã®ã‚¯ãƒ©ã‚¹ã®idã«0ã‚’å…¥ã‚Œã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã™ã‚‹ï¼ã“ã®æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã‚’tryâ€¦exceptæ§‹æ–‡ã§ã¨ã‚‰ãˆã¦ï¼Œã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ï¼Œ ValidationErrorã‚’ç”¨ã„ã‚‹ï¼\n\nfrom pydantic import ValidationError\n\ntry:\n    user = User(id=0)\nexcept ValueError as e:\n    print(e)\n\n1 validation error for User\nid\n  Input should be greater than or equal to 1 [type=greater_than_equal, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.1.2/v/greater_than_equal\n\n\n\n\n\n\nnameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ—¢å®šå€¤ã‚’è‡ªåˆ†ã®åå‰ã«ã—ï¼Œ0ä»¥ä¸Šï¼Œ120ä»¥ä¸‹ã®å€¤ã‚’ã¨ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ age ã‚’è¿½åŠ ã—ãŸã‚¯ãƒ©ã‚¹ Userã‚’ä½œã‚Œï¼\nä¸Šã§ä½œã£ãŸã‚¯ãƒ©ã‚¹ã« age = 150 ã‚’å…¥ã‚Œã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã™ã‚‹ï¼ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºã™ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã«ç›´ã›ï¼\n\næ¨™æº–ã®int, str, float, boolã®å‹ã ã‘ã§ãªãï¼Œæ§˜ã€…ãªå‹ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ å‹ã‚¯ãƒ©ã‚¹ã¯ï¼Œtypingãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãŠãï¼\n\nfrom typing import Tuple, List, Dict, Set, Union, Optional\n\nclass User(BaseModel):\n    id: int\n    height: Union[int, float]         # æ•´æ•°ã‹æµ®å‹•å°æ•°ç‚¹æ•°ã®ã„ãšã‚Œã‹ï¼ˆå‹ã®å’Œé›†åˆï¼‰\n    name: Optional[str]       = None  # çœç•¥å¯èƒ½ ï¼ˆãŸã ã—æ—¢å®šå€¤ã¯å¿…è¦ï¼‰\n    friends: List[str]                # å‹äººã®åå‰ã‚’å…¥ã‚Œæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ\n    fruits: Dict[str,int]             # å¥½ããªãƒ•ãƒ«ãƒ¼ãƒ„åã‚’ã‚­ãƒ¼ï¼Œè³¼å…¥æ•°ã‚’å€¤ã¨ã—ãŸè¾æ›¸\n\nUser(id = 123, \n     height = 178.0, \n     friends = [\"Kitty\", \"Mickey\", \"Donald\"],\n     fruits = {\"apple\":10, \"melon\":3}\n    )\n\nUser(id=123, height=178.0, name=None, friends=['Kitty', 'Mickey', 'Donald'], fruits={'apple': 10, 'melon': 3})\n\n\n\n\n\nä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚‚ã¤ã‚¯ãƒ©ã‚¹Customerã‚’Pydanticã®BaseModelã‹ã‚‰æ´¾ç”Ÿã•ã›ã¦ä½œã‚Œï¼ ã¾ãŸï¼Œé©å½“ãªãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦\n\næ•´æ•°ã‹æ–‡å­—åˆ—ã®id\næ–‡å­—åˆ—ã®name\nç·¯åº¦ãƒ»çµŒåº¦ã‚’è¡¨ã™æµ®å‹•å°æ•°ç‚¹æ•°ã®ã‚¿ãƒ—ãƒ«ã®location\næ‰±ã†å•†å“ã®åå‰ã‚’æ–‡å­—åˆ—ã¨ã—ãŸé›†åˆã®products",
    "crumbs": [
      "Pydanticã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼"
    ]
  },
  {
    "objectID": "18pydantic.html#pydanticã¨ã¯",
    "href": "18pydantic.html#pydanticã¨ã¯",
    "title": "Pydanticã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼",
    "section": "",
    "text": "Pydantic ã¯ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚ã‚‹ï¼\nPythonã¯ï¼Œå‹•çš„ã«å‹ä»˜ã‘ã‚’è¡Œã†ã®ãŒç‰¹å¾´ã§ã‚ã‚Šï¼Œãã‚Œã¯åˆ©ç‚¹ã§ã‚‚ã‚ã‚Šå¼±ç‚¹ã§ã‚‚ã‚ã‚‹ï¼ãŸã¨ãˆã°ï¼Œä»¥ä¸‹ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãŒæ›¸ã‘ã‚‹ï¼\n\nx = 1 #æ•´æ•°å‹ int ã«å‹ä»˜ã‘\ntype(x)\n\nint\n\n\n\nx = \"Hello\"\ntype(x)\n\nstr\n\n\nå‹ï¼ˆã‚¿ã‚¤ãƒ—ï¼‰ã‚’æ°—ã«ã™ã‚‹å¿…è¦ãŒãªã„ã®ã§ï¼Œåˆå­¦è€…ãŒæ°—æ¥½ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã‘ã‚‹ã¨ã„ã†ã¯åˆ©ç‚¹ã§ã‚ã‚‹ãŒï¼Œ ã¡ã‚ƒã‚“ã¨ã—ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ããŸã„äººã«ã¨ã£ã¦ã¯ï¼Œã“ã®ä»•æ§˜ã¯å¬‰ã—ããªã„ï¼\nãã®ãŸã‚ï¼Œæœ€è¿‘ã®Pythonã§ã¯å‹ãƒ’ãƒ³ãƒˆã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸï¼ãŸã¨ãˆã°ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«æ•´æ•°ã‚’2å€ã—ãŸæ•´æ•°ã‚’è¿”ã™é–¢æ•°ã‚’å®šç¾©ã§ãã‚‹ï¼\n\ndef multiple2( x:int ) -&gt; int:\n    return x*2\nmultiple2(100)\n\n200\n\n\nã“ã‚Œã¯ï¼Œå¼•æ•°ã®xã‚’æ•´æ•°å‹ int ã§ï¼Œè¿”å€¤ã‚‚æ•´æ•°å‹ã§ã‚ã‚‹ã‚ˆã†ã«å‹ãƒ’ãƒ³ãƒˆã‚’ä¸ãˆãŸã‚‚ã®ã§ã‚ã‚‹ãŒï¼Œ ã“ã‚Œã¯å˜ã«ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿ã‚„ã™ãã™ã‚‹ãŸã‚ã®ãƒ’ãƒ³ãƒˆï¼ˆé£¾ã‚Šï¼‰ã§ã‚ã‚‹ãŸã‚ï¼Œå®Ÿéš›ã«ã¯æ–‡å­—åˆ—ã‚’å¼•æ•°ã¨ã—ã¦ä¸ãˆã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã—ãªã„ï¼\n\nmultiple2(\"Hello\")\n\n'HelloHello'\n\n\nã“ã†ã„ã£ãŸäºˆæœŸã—ãªã„çµæœã‚’å‡ºã•ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®æ‰‹æ®µãŒãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ (data validiation) ã§ã‚ã‚‹ï¼ Pydanticã‚’ä½¿ã†ã¨ï¼Œãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãŒå®¹æ˜“ã«ãªã‚‹ã ã‘ã§ãªãï¼Œã‚¯ãƒ©ã‚¹ã‚’è¨­è¨ˆã™ã‚‹ã®ãŒæ¥½ã«ãªã‚‹ï¼\næ—©é€Ÿä½¿ã£ã¦ã¿ã‚ˆã†ï¼ã¾ãšã¯ï¼ŒPydanticã®BaseModelã‚¯ãƒ©ã‚¹ã‹ã‚‰æ´¾ç”Ÿã•ã›ã¦Userã‚¯ãƒ©ã‚¹ã‚’ä½œã£ã¦ã¿ã‚‹ï¼ ã“ã®ã‚¯ãƒ©ã‚¹ã¯ï¼Œæ•´æ•°å€¤ã‚’ã¨ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ id ã¨æ–‡å­—åˆ—ã®åå‰ name ã®2ã¤ã®å±æ€§ï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã‚’ã‚‚ã¤ï¼\n\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    id: int\n    name: str = \"Mikio Kubo\"\n\nidã¯å¿…é ˆã§ã‚ã‚Šçœç•¥ã§ããªã„ï¼ä¸€æ–¹ï¼Œnameã¯æ—¢å®šå€¤ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã®ã§çœç•¥ã§ãã‚‹ï¼ ä»¥ä¸‹ã§ã¯ï¼Œæ–‡å­—åˆ— â€˜123â€™ ã§idã‚’æŒ‡å®šã—ã¦ã„ã‚‹ãŒï¼Œå‹æŒ‡å®šæ©Ÿèƒ½ã§è‡ªå‹•çš„ã«æ•´æ•°å€¤ã«å¤‰æ›ã•ã‚Œã‚‹ï¼ ã¾ãŸï¼Œnameã¯çœç•¥ã™ã‚‹ã¨æ—¢å®šå€¤ãŒä»£å…¥ã•ã‚Œã‚‹ï¼\n\nuser = User(id='123')\nuser\n\nUser(id=123, name='Mikio Kubo')\n\n\n\n\n\nidãªã—ã§Userã‚¯ãƒ©ã‚¹ã‚’ä½¿ã†ã¨ã©ã†ãªã‚‹ã‹ï¼Ÿ\nnameå¼•æ•°ã«è‡ªåˆ†ã®åå‰ã‚’å…¥ã‚Œã‚‹ã¨ã©ã†ãªã‚‹ã‹ï¼Ÿ\n\nPydanticã§ä½œã£ãŸã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ï¼Œè¾æ›¸ã«å¤‰æ›ã§ãã‚‹ï¼ å¤‰æ›ã«ã¯model_dumpãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼è¾æ›¸ã®ã‚­ãƒ¼ã¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«ãªã‚‹ï¼\n\ndumped_user = user.model_dump()\nprint(type(dumped_user))\ndumped_user\n\n&lt;class 'dict'&gt;\n\n\n{'id': 123, 'name': 'Mikio Kubo'}\n\n\nãƒ‡ãƒ¼ã‚¿ã®WebçµŒç”±ã§ã®äº¤æ›ã®éš›ã«ã¯ï¼ŒJSON (JavaScript Object Notation) å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒä¾¿åˆ©ã§ã‚ã‚‹ï¼ å¤‰æ›ã«ã¯model_dump_jsonãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼\n\njson_user = user.model_dump_json()\nprint(type(json_user))\njson_user\n\n&lt;class 'str'&gt;\n\n\n'{\"id\":123,\"name\":\"Mikio Kubo\"}'\n\n\nã‚¯ãƒ©ã‚¹ã‹ã‚‰ä½œã‚‰ã‚ŒãŸè¾æ›¸ã‚„JSONã‹ã‚‰ï¼Œã‚¯ãƒ©ã‚¹ã‚’å†ç¾ã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ï¼ è¾æ›¸ã‹ã‚‰ã¯model_validateãƒ¡ã‚½ãƒƒãƒ‰ï¼ŒJSONã‹ã‚‰ã¯model_validate_jsonãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã‚‹ï¼\n\nprint( user.model_validate(dumped_user) )\nprint( user.model_validate_json(json_user) )\n\nid=123 name='Mikio Kubo'\nid=123 name='Mikio Kubo'\n\n\n\n\n\n\nè‡ªåˆ†ã®åå‰ã‚’nameã«è¨­å®šã—ãŸUserã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œã‚Šï¼Œãã‚Œã‚’è¾æ›¸ã«å¤‰æ›ã›ã‚ˆï¼\nä»Šåº¦ã¯JSONå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã›ã‚ˆï¼\nå¤‰æ›ã—ãŸè¾æ›¸ã¨JSONã‹ã‚‰ï¼Œå…ƒã®ã‚¯ãƒ©ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã›ã‚ˆï¼\n\nFieldã‚¯ãƒ©ã‚¹ã§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å€¤ã®æ—¢å®šå€¤ã‚„ç¯„å›²ã®æŒ‡å®šãªã©æ§˜ã€…ãªæƒ…å ±ã‚’ä»˜åŠ ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ ä»¥ä¸‹ã®ä¾‹ã§ã¯ï¼Œidã¯ge(greater than equal)ã‚’ç”¨ã„ã¦1ä»¥ä¸Šã®å€¤ã«åˆ¶é™ã—ï¼Œ åå‰ã®æ—¢å®šå€¤ (default value) ã¯Noneã¨ã—ã¦ã„ã‚‹ï¼\n\nfrom pydantic import Field\n\nclass User(BaseModel):\n    id: int   = Field(ge=1)\n    name: str = Field(default=None)\n\nuser = User(id=1)\nprint(user)\n\nid=1 name=None\n\n\nä¸Šã®ã‚¯ãƒ©ã‚¹ã®idã«0ã‚’å…¥ã‚Œã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã™ã‚‹ï¼ã“ã®æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã‚’tryâ€¦exceptæ§‹æ–‡ã§ã¨ã‚‰ãˆã¦ï¼Œã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ï¼Œ ValidationErrorã‚’ç”¨ã„ã‚‹ï¼\n\nfrom pydantic import ValidationError\n\ntry:\n    user = User(id=0)\nexcept ValueError as e:\n    print(e)\n\n1 validation error for User\nid\n  Input should be greater than or equal to 1 [type=greater_than_equal, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.1.2/v/greater_than_equal\n\n\n\n\n\n\nnameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ—¢å®šå€¤ã‚’è‡ªåˆ†ã®åå‰ã«ã—ï¼Œ0ä»¥ä¸Šï¼Œ120ä»¥ä¸‹ã®å€¤ã‚’ã¨ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ age ã‚’è¿½åŠ ã—ãŸã‚¯ãƒ©ã‚¹ Userã‚’ä½œã‚Œï¼\nä¸Šã§ä½œã£ãŸã‚¯ãƒ©ã‚¹ã« age = 150 ã‚’å…¥ã‚Œã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã™ã‚‹ï¼ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºã™ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã«ç›´ã›ï¼\n\næ¨™æº–ã®int, str, float, boolã®å‹ã ã‘ã§ãªãï¼Œæ§˜ã€…ãªå‹ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ å‹ã‚¯ãƒ©ã‚¹ã¯ï¼Œtypingãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãŠãï¼\n\nfrom typing import Tuple, List, Dict, Set, Union, Optional\n\nclass User(BaseModel):\n    id: int\n    height: Union[int, float]         # æ•´æ•°ã‹æµ®å‹•å°æ•°ç‚¹æ•°ã®ã„ãšã‚Œã‹ï¼ˆå‹ã®å’Œé›†åˆï¼‰\n    name: Optional[str]       = None  # çœç•¥å¯èƒ½ ï¼ˆãŸã ã—æ—¢å®šå€¤ã¯å¿…è¦ï¼‰\n    friends: List[str]                # å‹äººã®åå‰ã‚’å…¥ã‚Œæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ\n    fruits: Dict[str,int]             # å¥½ããªãƒ•ãƒ«ãƒ¼ãƒ„åã‚’ã‚­ãƒ¼ï¼Œè³¼å…¥æ•°ã‚’å€¤ã¨ã—ãŸè¾æ›¸\n\nUser(id = 123, \n     height = 178.0, \n     friends = [\"Kitty\", \"Mickey\", \"Donald\"],\n     fruits = {\"apple\":10, \"melon\":3}\n    )\n\nUser(id=123, height=178.0, name=None, friends=['Kitty', 'Mickey', 'Donald'], fruits={'apple': 10, 'melon': 3})\n\n\n\n\n\nä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚‚ã¤ã‚¯ãƒ©ã‚¹Customerã‚’Pydanticã®BaseModelã‹ã‚‰æ´¾ç”Ÿã•ã›ã¦ä½œã‚Œï¼ ã¾ãŸï¼Œé©å½“ãªãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦\n\næ•´æ•°ã‹æ–‡å­—åˆ—ã®id\næ–‡å­—åˆ—ã®name\nç·¯åº¦ãƒ»çµŒåº¦ã‚’è¡¨ã™æµ®å‹•å°æ•°ç‚¹æ•°ã®ã‚¿ãƒ—ãƒ«ã®location\næ‰±ã†å•†å“ã®åå‰ã‚’æ–‡å­—åˆ—ã¨ã—ãŸé›†åˆã®products",
    "crumbs": [
      "Pydanticã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼"
    ]
  },
  {
    "objectID": "31pytorch.html",
    "href": "31pytorch.html",
    "title": "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "",
    "text": "PyTorchãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ç”¨ã„ã¦ \\(N\\) è¡Œ \\(D\\) åˆ—ã®ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œåˆ— \\(x,y,z\\) ã‚’ä½œã‚Šï¼Œ \\(c = \\sum x*y + z\\) ã‚’è¨ˆç®—ã‚’ã™ã‚‹ï¼ æœ€å¾Œã«ï¼Œ è¨ˆç®—ã•ã‚ŒãŸ \\(c\\) ã® \\(x\\) ã«å¯¾ã™ã‚‹å‹¾é…ï¼ˆå¾®åˆ†å€¤ï¼‰ã‚’backwardã§è¨ˆç®—ã™ã‚‹ï¼\nrandã§ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œåˆ—ã‚’ã‚’ç”Ÿæˆã™ã‚‹éš›ã«ï¼Œ å¼•æ•° requires_grad ã‚’ True ã«è¨­å®šã—ã¦ãŠãã“ã¨ã«ã‚ˆã£ã¦ï¼Œ å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã‚‹ï¼ã€€\nå¤‰æ•°ã«å¯¾ã—ã¦ã¯ï¼Œã€€dataã§å€¤ãŒï¼Œ å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã‚ˆã†ã«æŒ‡å®šã—ãŸå¤‰æ•°ã«å¯¾ã—ã¦ã¯ gradã§å‹¾é…ãŒå¾—ã‚‰ã‚Œã‚‹ï¼\n\nimport torch\n\n\nN, D = 3,4 #3è¡Œ4åˆ—ã®ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆã—ï¼Œ å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹\nx = torch.rand( (N, D), requires_grad=True)\ny = torch.rand( (N, D), requires_grad=True)\nz = torch.rand( (N, D), requires_grad=True)\n\na = x*y\nb = a+z\nc = torch.sum(b)\n\nc.backward()\n\nc.data\n\ntensor(7.9498)\n\n\n\nprint(x.data)\n\ntensor([[0.1379, 0.2976, 0.8908, 0.8022],\n        [0.2833, 0.0333, 0.8762, 0.2876],\n        [0.3688, 0.8043, 0.9437, 0.0616]])\n\n\n\nprint(x.grad)\n\ntensor([[0.7403, 0.0179, 0.0335, 0.0769],\n        [0.5766, 0.5749, 0.5347, 0.4289],\n        [0.7062, 0.0719, 0.6709, 0.1248]])",
    "crumbs": [
      "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "31pytorch.html#ç°¡å˜ãªè¨ˆç®—",
    "href": "31pytorch.html#ç°¡å˜ãªè¨ˆç®—",
    "title": "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "",
    "text": "PyTorchãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ç”¨ã„ã¦ \\(N\\) è¡Œ \\(D\\) åˆ—ã®ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œåˆ— \\(x,y,z\\) ã‚’ä½œã‚Šï¼Œ \\(c = \\sum x*y + z\\) ã‚’è¨ˆç®—ã‚’ã™ã‚‹ï¼ æœ€å¾Œã«ï¼Œ è¨ˆç®—ã•ã‚ŒãŸ \\(c\\) ã® \\(x\\) ã«å¯¾ã™ã‚‹å‹¾é…ï¼ˆå¾®åˆ†å€¤ï¼‰ã‚’backwardã§è¨ˆç®—ã™ã‚‹ï¼\nrandã§ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œåˆ—ã‚’ã‚’ç”Ÿæˆã™ã‚‹éš›ã«ï¼Œ å¼•æ•° requires_grad ã‚’ True ã«è¨­å®šã—ã¦ãŠãã“ã¨ã«ã‚ˆã£ã¦ï¼Œ å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã‚‹ï¼ã€€\nå¤‰æ•°ã«å¯¾ã—ã¦ã¯ï¼Œã€€dataã§å€¤ãŒï¼Œ å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã‚ˆã†ã«æŒ‡å®šã—ãŸå¤‰æ•°ã«å¯¾ã—ã¦ã¯ gradã§å‹¾é…ãŒå¾—ã‚‰ã‚Œã‚‹ï¼\n\nimport torch\n\n\nN, D = 3,4 #3è¡Œ4åˆ—ã®ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆã—ï¼Œ å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹\nx = torch.rand( (N, D), requires_grad=True)\ny = torch.rand( (N, D), requires_grad=True)\nz = torch.rand( (N, D), requires_grad=True)\n\na = x*y\nb = a+z\nc = torch.sum(b)\n\nc.backward()\n\nc.data\n\ntensor(7.9498)\n\n\n\nprint(x.data)\n\ntensor([[0.1379, 0.2976, 0.8908, 0.8022],\n        [0.2833, 0.0333, 0.8762, 0.2876],\n        [0.3688, 0.8043, 0.9437, 0.0616]])\n\n\n\nprint(x.grad)\n\ntensor([[0.7403, 0.0179, 0.0335, 0.0769],\n        [0.5766, 0.5749, 0.5347, 0.4289],\n        [0.7062, 0.0719, 0.6709, 0.1248]])",
    "crumbs": [
      "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "31pytorch.html#æœ€å°2ä¹—æ³•",
    "href": "31pytorch.html#æœ€å°2ä¹—æ³•",
    "title": "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "æœ€å°2ä¹—æ³•",
    "text": "æœ€å°2ä¹—æ³•\n\\(y = a x\\) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \\(a\\) ã®æœ€é©åŒ–ã‚’æœ€å°2ä¹—æ³•ã«ã‚ˆã£ã¦è¡Œã†ï¼\nfrom_numpyã§NumPyã®é…åˆ—ã‚’PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã§ãã‚‹ï¼ã€€\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \\(a\\) ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¨­å®šã—ï¼Œå‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã™ã‚‹ï¼\näºˆæ¸¬å€¤ yhat ã‚’è¨ˆç®—ã—ãŸå¾Œã§ï¼Œ æå‡ºé–¢æ•° loss ã‚’èª¤å·®ã®2ä¹—å¹³å‡ã¨ã—ã¦è¨ˆç®—ã—ï¼Œ backward ã§å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ï¼\nå‹¾é…ã®é€†æ–¹å‘ã«å­¦ç¿’ç‡ lr (learning rate) ã ã‘ç§»å‹•ã•ã›ãŸã‚‚ã®ã‚’æ–°ã—ã„ \\(a\\) ã¨ã—ï¼Œ ãã‚Œã‚’n_epochså›ç¹°ã‚Šè¿”ã™ï¼\nã“ã“ã§ï¼Œåå¾©ã”ã¨ã«å‹¾é…ã‚’ \\(0\\) ã«åˆæœŸåŒ–ã™ã‚‹ a.grad.zero_ ã‚’å‘¼ã³å‡ºã™ã“ã¨ã«æ³¨æ„ã•ã‚ŒãŸã„ï¼\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\n\nx_numpy = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float32)\ny_numpy = np.array([15.0, 20.0, 34.0, 42.0, 58.0], dtype=np.float32)\n\nx = torch.from_numpy(x_numpy)\ny = torch.from_numpy(y_numpy)\n\na = torch.rand(1, requires_grad=True) #ã‚¹ã‚«ãƒ©ãƒ¼ã‚’å®šç¾©\n\nn_epochs = 10  #åå¾©å›æ•°ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ï¼‰\nlr = 0.01      #å­¦ç¿’ç‡\nfor epoch in range(n_epochs):\n    yhat = a*x \n    error = y - yhat\n    loss =(error**2).mean()\n    \n    loss.backward()\n    \n    with torch.no_grad():\n        a -= lr*a.grad\n\n    print(loss.data, a.data)\n    \n    a.grad.zero_()\n\ntensor(1317.2814) tensor([2.6670])\ntensor(803.9544) tensor([4.5403])\ntensor(491.6464) tensor([6.0014])\ntensor(301.6381) tensor([7.1411])\ntensor(186.0371) tensor([8.0301])\ntensor(115.7055) tensor([8.7234])\ntensor(72.9157) tensor([9.2643])\ntensor(46.8824) tensor([9.6861])\ntensor(31.0437) tensor([10.0152])\ntensor(21.4075) tensor([10.2719])",
    "crumbs": [
      "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "31pytorch.html#ç·šå½¢å›å¸°",
    "href": "31pytorch.html#ç·šå½¢å›å¸°",
    "title": "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "ç·šå½¢å›å¸°",
    "text": "ç·šå½¢å›å¸°\n\nã‚¯ãƒ©ã‚¹\nnn.Moduleã‚¯ãƒ©ã‚¹ã‹ã‚‰æ´¾ç”Ÿã•ã›ã¦ç·šå½¢å›å¸°ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ LinearRegression ã‚’ä½œã‚‹ï¼\nã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ init ã§è¦ªã‚¯ãƒ©ã‚¹ã‚’å‘¼ã³å‡ºã—ãŸå¾Œã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \\(a\\) ã‚’å®šç¾©ã™ã‚‹ï¼\nä¸ãˆã‚‰ã‚ŒãŸ \\(x\\) ã«å¯¾ã—ã¦äºˆæ¸¬å€¤ \\(y=ax\\) ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®é–¢æ•° forward ã‚’å®šç¾©ã™ã‚‹ï¼\nãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ model ã®state_dicãƒ¡ã‚½ãƒƒãƒ‰ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \\(a\\) ã®ç¾åœ¨ã®å€¤ã®è¾æ›¸ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n\nclass LinearRegression(nn.Module):\n    #ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n    def __init__(self):\n        super().__init__() #è¦ªã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’å‘¼ã¶\n        self.a = nn.Parameter(torch.rand(1, requires_grad=True)) #ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™\n        \n    #äºˆæ¸¬å€¤ã®è¨ˆç®—\n    def forward(self, x):\n        return self.a*x\n        \nmodel = LinearRegression()\n\nmodel.state_dict()  #ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ç¾åœ¨ã®å€¤ã®è¾æ›¸\n\nOrderedDict([('a', tensor([0.5324]))])\n\n\n\n\nè¨“ç·´\næå‡ºé–¢æ•°ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° loss_fn ã‚’æœ€å°2ä¹—èª¤å·® nn.MSELoss ã¨ã—ï¼Œ æœ€é©åŒ–ã¯ç‡çš„å‹¾é…é™ä¸‹æ³• (SGD: Stochastic Fradient Descent) optim.SDG ã¨ã™ã‚‹ï¼ å¼•æ•°ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ model.parameters() ã¨å­¦ç¿’ç‡ lr ã§ã‚ã‚‹ï¼\noptimizerã®stepã§å‹¾é…é™ä¸‹æ³•ã®1åå¾©ã‚’è¡Œã„ï¼Œ åå¾©ã”ã¨ã« zero_grad ã§å‹¾é…ã‚’ \\(0\\) ã«åˆæœŸåŒ–ã™ã‚‹ï¼\n\nloss_fn = nn.MSELoss()  #æå‡ºé–¢æ•°ï¼ˆæœ€å°2ä¹—èª¤å·®ï¼‰\noptimizer = optim.SGD(model.parameters(), lr)  # æœ€é©åŒ–ï¼ˆç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼‰ã‚’æº–å‚™ï¼› model.parameters()ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿\n\nfor epoch in range(n_epochs):\n    model.train()           #ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹\n    yhat = model(x)         #forwardãƒ¡ã‚½ãƒƒãƒ‰ã§äºˆæ¸¬å€¤ã‚’è¨ˆç®—ã™ã‚‹\n    loss = loss_fn(y, yhat) #æå‡ºé–¢æ•°\n    loss.backward()         #èª¤å·®é€†ä¼æ’­ã§å‹¾é…ã‚’è¨ˆç®—\n    \n    optimizer.step()        #æœ€é©åŒ–ã®ï¼‘åå¾©\n    optimizer.zero_grad()   #å‹¾é…ã‚’ï¼ã«ãƒªã‚»ãƒƒãƒˆ\n    \n    print(loss.data, model.state_dict())\n\ntensor(1253.9412) OrderedDict([('a', tensor([2.8753]))])\ntensor(765.4183) OrderedDict([('a', tensor([4.7027]))])\ntensor(468.2010) OrderedDict([('a', tensor([6.1281]))])\ntensor(287.3740) OrderedDict([('a', tensor([7.2399]))])\ntensor(177.3588) OrderedDict([('a', tensor([8.1072]))])\ntensor(110.4256) OrderedDict([('a', tensor([8.7836]))])\ntensor(69.7034) OrderedDict([('a', tensor([9.3112]))])\ntensor(44.9280) OrderedDict([('a', tensor([9.7227]))])\ntensor(29.8547) OrderedDict([('a', tensor([10.0437]))])\ntensor(20.6841) OrderedDict([('a', tensor([10.2941]))])",
    "crumbs": [
      "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "31pytorch.html#ç·šå½¢å±¤ã®è¿½åŠ ",
    "href": "31pytorch.html#ç·šå½¢å±¤ã®è¿½åŠ ",
    "title": "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "ç·šå½¢å±¤ã®è¿½åŠ ",
    "text": "ç·šå½¢å±¤ã®è¿½åŠ \nnn.Linear(å…¥åŠ›æ•°, å‡ºåŠ›æ•°)ã§ç·šå½¢å±¤ã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ã™ã‚‹ï¼ ãƒ¢ãƒ‡ãƒ«ã¯ \\(y = w_0 + w_1 x\\) ã¨ãªã‚‹ï¼\nãƒ‡ãƒ¼ã‚¿ã® \\(x,y\\) ã¯ç¸¦ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆshapeã¯ (5,1)ã€€ï¼‰ã«ãªãŠã—ã¦ãŠãï¼\n\nclass LayerLinearRegression(nn.Module):\n    #ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n    def __init__(self):\n        super().__init__() #è¦ªã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’å‘¼ã¶\n        self.linear = nn.Linear(1,1, dtype=torch.float32) #1å…¥åŠ›ãƒ»ï¼‘å‡ºåŠ›ã®ç·šå½¢å±¤\n        #self.linear =nn.Sequential(nn.Linear(1,2), nn.ReLU(), nn.Linear(2,1)) #å¤šå±¤ã®ãƒ¢ãƒ‡ãƒ«ã‚‚Sequentialã‚’ç”¨ã„ã¦ä½œã‚Œã‚‹\n    #äºˆæ¸¬å€¤ã®è¨ˆç®—\n    def forward(self, x):\n        return self.linear(x)\n        \nmodel = LayerLinearRegression()\n\nmodel.state_dict()  #ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ç¾åœ¨ã®å€¤ã®è¾æ›¸\n\nOrderedDict([('linear.weight', tensor([[-0.9852]])),\n             ('linear.bias', tensor([0.3170]))])\n\n\n\nx = x.reshape(-1,1)\ny = y.reshape(-1,1)\n\n\nloss_fn = nn.MSELoss()  #æå‡ºé–¢æ•°ï¼ˆæœ€å°2ä¹—èª¤å·®ï¼‰\noptimizer = optim.SGD(model.parameters(), lr)  # æœ€é©åŒ–ï¼ˆç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼‰ã‚’æº–å‚™ï¼› model.parameters()ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿\n\nn_epochs=10\nfor epoch in range(n_epochs):\n    model.train()           #ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹\n    yhat = model(x)         #forwardãƒ¡ã‚½ãƒƒãƒ‰ã§äºˆæ¸¬å€¤ã‚’è¨ˆç®—ã™ã‚‹\n    loss = loss_fn(y, yhat) #æå‡ºé–¢æ•°\n    loss.backward()         #èª¤å·®é€†ä¼æ’­ã§å‹¾é…ã‚’è¨ˆç®—\n    \n    optimizer.step()        #æœ€é©åŒ–ã®ï¼‘åå¾©\n    optimizer.zero_grad()   #å‹¾é…ã‚’ï¼ã«ãƒªã‚»ãƒƒãƒˆ\n    \n    print(loss.data, model.state_dict())\n\ntensor(1611.6226) OrderedDict([('linear.weight', tensor([[1.6726]])), ('linear.bias', tensor([1.0458]))])\ntensor(942.0174) OrderedDict([('linear.weight', tensor([[3.7018]])), ('linear.bias', tensor([1.6005]))])\ntensor(551.8026) OrderedDict([('linear.weight', tensor([[5.2514]])), ('linear.bias', tensor([2.0224]))])\ntensor(324.4026) OrderedDict([('linear.weight', tensor([[6.4348]])), ('linear.bias', tensor([2.3428]))])\ntensor(191.8832) OrderedDict([('linear.weight', tensor([[7.3385]])), ('linear.bias', tensor([2.5859]))])\ntensor(114.6554) OrderedDict([('linear.weight', tensor([[8.0289]])), ('linear.bias', tensor([2.7699]))])\ntensor(69.6488) OrderedDict([('linear.weight', tensor([[8.5564]])), ('linear.bias', tensor([2.9087]))])\ntensor(43.4192) OrderedDict([('linear.weight', tensor([[8.9594]])), ('linear.bias', tensor([3.0132]))])\ntensor(28.1319) OrderedDict([('linear.weight', tensor([[9.2676]])), ('linear.bias', tensor([3.0914]))])\ntensor(19.2212) OrderedDict([('linear.weight', tensor([[9.5032]])), ('linear.bias', tensor([3.1495]))])",
    "crumbs": [
      "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  },
  {
    "objectID": "31pytorch.html#ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼",
    "href": "31pytorch.html#ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼",
    "title": "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’",
    "section": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼",
    "text": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯TensorDatasetã§ç”Ÿæˆã•ã‚Œã‚‹ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ï¼\n1ãƒãƒƒãƒãšã¤ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã¯ DataLoaderã‚¯ãƒ©ã‚¹ã«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¥ã‚Œã‚‹ã“ã¨ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹ï¼\n\nfrom torch.utils.data import TensorDataset, DataLoader\ntrain_data = TensorDataset(x,y)\ntrain_data[0]\n\n(tensor([1.]), tensor([15.]))\n\n\n\ntrain_loader = DataLoader(dataset = train_data, batch_size=2, shuffle=True)\nfor x_batch, y_batch in train_loader:\n    print(x_batch, y_batch)\n\ntensor([[4.],\n        [2.]]) tensor([[42.],\n        [20.]])\ntensor([[3.],\n        [5.]]) tensor([[34.],\n        [58.]])\ntensor([[1.]]) tensor([[15.]])\n\n\n\nloss_fn = nn.MSELoss()  #æå‡ºé–¢æ•°ï¼ˆæœ€å°2ä¹—èª¤å·®ï¼‰\noptimizer = optim.SGD(model.parameters(), lr)  # æœ€é©åŒ–ï¼ˆç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼‰ã‚’æº–å‚™ï¼› model.parameters()ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿\n\nn_epochs=10\nfor epoch in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        model.train()           #ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹\n        yhat = model(x_batch)         #forwardãƒ¡ã‚½ãƒƒãƒ‰ã§äºˆæ¸¬å€¤ã‚’è¨ˆç®—ã™ã‚‹\n        loss = loss_fn(y_batch, yhat) #æå‡ºé–¢æ•°\n        loss.backward()         #èª¤å·®é€†ä¼æ’­ã§å‹¾é…ã‚’è¨ˆç®—\n\n        optimizer.step()        #æœ€é©åŒ–ã®ï¼‘åå¾©\n        optimizer.zero_grad()   #å‹¾é…ã‚’ï¼ã«ãƒªã‚»ãƒƒãƒˆ\n\n        print(loss.data, model.state_dict())\n\ntensor(5.4948) OrderedDict([('linear.weight', tensor([[9.5969]])), ('linear.bias', tensor([3.1964]))])\ntensor(26.1062) OrderedDict([('linear.weight', tensor([[9.8901]])), ('linear.bias', tensor([3.2406]))])\ntensor(0.6415) OrderedDict([('linear.weight', tensor([[9.8260]])), ('linear.bias', tensor([3.2246]))])\ntensor(2.0398) OrderedDict([('linear.weight', tensor([[9.8243]])), ('linear.bias', tensor([3.2388]))])\ntensor(4.9985) OrderedDict([('linear.weight', tensor([[9.8052]])), ('linear.bias', tensor([3.2228]))])\ntensor(33.0736) OrderedDict([('linear.weight', tensor([[10.3803]])), ('linear.bias', tensor([3.3379]))])\ntensor(12.2090) OrderedDict([('linear.weight', tensor([[10.4364]])), ('linear.bias', tensor([3.3245]))])\ntensor(0.9685) OrderedDict([('linear.weight', tensor([[10.4298]])), ('linear.bias', tensor([3.3305]))])\ntensor(9.3001) OrderedDict([('linear.weight', tensor([[10.1858]])), ('linear.bias', tensor([3.2695]))])\ntensor(8.4185) OrderedDict([('linear.weight', tensor([[10.3913]])), ('linear.bias', tensor([3.3230]))])\ntensor(8.5516) OrderedDict([('linear.weight', tensor([[10.2943]])), ('linear.bias', tensor([3.2770]))])\ntensor(6.0230) OrderedDict([('linear.weight', tensor([[10.0980]])), ('linear.bias', tensor([3.2279]))])\ntensor(5.9756) OrderedDict([('linear.weight', tensor([[10.0438]])), ('linear.bias', tensor([3.1984]))])\ntensor(2.4884) OrderedDict([('linear.weight', tensor([[10.0065]])), ('linear.bias', tensor([3.2023]))])\ntensor(22.7092) OrderedDict([('linear.weight', tensor([[10.4830]])), ('linear.bias', tensor([3.2976]))])\ntensor(2.8948) OrderedDict([('linear.weight', tensor([[10.5750]])), ('linear.bias', tensor([3.3130]))])\ntensor(16.4854) OrderedDict([('linear.weight', tensor([[10.3412]])), ('linear.bias', tensor([3.2322]))])\ntensor(2.0351) OrderedDict([('linear.weight', tensor([[10.3697]])), ('linear.bias', tensor([3.2608]))])\ntensor(8.0694) OrderedDict([('linear.weight', tensor([[10.2786]])), ('linear.bias', tensor([3.2171]))])\ntensor(6.8768) OrderedDict([('linear.weight', tensor([[10.4632]])), ('linear.bias', tensor([3.2660]))])\ntensor(9.7260) OrderedDict([('linear.weight', tensor([[10.2137]])), ('linear.bias', tensor([3.2036]))])\ntensor(1.2645) OrderedDict([('linear.weight', tensor([[10.2342]])), ('linear.bias', tensor([3.2210]))])\ntensor(8.8373) OrderedDict([('linear.weight', tensor([[10.3283]])), ('linear.bias', tensor([3.2355]))])\ntensor(15.1480) OrderedDict([('linear.weight', tensor([[10.1726]])), ('linear.bias', tensor([3.1577]))])\ntensor(14.0528) OrderedDict([('linear.weight', tensor([[10.3015]])), ('linear.bias', tensor([3.1624]))])\ntensor(1.1820) OrderedDict([('linear.weight', tensor([[10.3148]])), ('linear.bias', tensor([3.1771]))])\ntensor(5.9366) OrderedDict([('linear.weight', tensor([[10.1199]])), ('linear.bias', tensor([3.1284]))])\ntensor(7.2067) OrderedDict([('linear.weight', tensor([[10.0701]])), ('linear.bias', tensor([3.1122]))])\ntensor(10.5234) OrderedDict([('linear.weight', tensor([[10.3173]])), ('linear.bias', tensor([3.1644]))])\ntensor(5.9218) OrderedDict([('linear.weight', tensor([[10.1226]])), ('linear.bias', tensor([3.1157]))])",
    "crumbs": [
      "PyTorchã«ã‚ˆã‚‹æ·±å±¤å­¦ç¿’"
    ]
  }
]