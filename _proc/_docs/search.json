[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "データサイエンス練習問題集 II",
    "section": "",
    "text": "このページは、データサイエンス練習問題集 https://scmopt.github.io/analytics の続編であり、主に深層学習と最適化についての最新の話題を紹介する。"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "analytics2",
    "section": "Install",
    "text": "Install\npip install analytics2"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "analytics2",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "10forecast.html",
    "href": "10forecast.html",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "",
    "text": "#default_exp forecast\n# hide\n# hide_input\nfrom IPython.display import Image, YouTubeVideo\n\nYouTubeVideo(\"23VJTLk1twM\", width=200, height=150)\n# hide_input\nYouTubeVideo(\"LfbGEpZ1PeI\", width=200, height=150)"
  },
  {
    "objectID": "10forecast.html#prophetとは",
    "href": "10forecast.html#prophetとは",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "Prophetとは",
    "text": "Prophetとは\n\nProphet https://facebook.github.io/prophet/ は需要予測のためのパッケージである． ここでは，Prophetを用いた需要予測の方法について述べる． また，需要予測の基本原理と， Prophetの基礎になるベイズ推論をPyMCパッケージを用いて解説する．"
  },
  {
    "objectID": "10forecast.html#需要予測",
    "href": "10forecast.html#需要予測",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "需要予測",
    "text": "需要予測\n筆者は，以前は最適化の仕事は引き受けても予測の仕事は引き受けないことにしていた．予測は現場で長年働いている人の経験を加味して行うべきものであり，門外漢がいくらテクニックを駆使してもそれを超えることは難しいかと考えていたからだ．\nしかし最近になって，需要予測は当たらないといけないという考えを捨てて，誤差の管理を行うことだと割り切って考えることにして，予測に積極的に関与するようになった．多くの最適化モデルは，予測がある程度合っているという前提で構築される．予測をいいかげんにされると，最適化モデル自体が役に立たないものになる．ゴミを入れればゴミが出てくるからだ．\n誤差を管理することによって，需要予測を「点」で行うのではなく，「範囲」で行うことが可能になる．また，需要の分布も特定できるようになる．範囲内での最適化はロバスト最適化，確率分布を仮定した最適化は確率最適化という枠組みで解決可能になる．\n以下では，サプライ・チェイン最適化に関連した需要予測の基本と重要なモデルについて解説する．\n\n予測の公理\n最近，サプライ・チェインの現場において，予測に関する多くの誤りが浸透していることに気づいた．ここでは，このような誤用を減らして正しい予測手法を適用するための，予測の公理について述べる．\n\n予測は予測のためならず\n\nしばしば，予測を目的として仕事をしている人たちを見かける．予測は，ほかの重要な意思決定を行うための基礎となる手段であり，予測そのものを目的としてはならない．実際には，予測よりもその誤差を評価することの方が重要である．誤差が増えているのか，減っているのか，その理由は何かを考えることが，需要予測の真の目的なのである．\nたとえば，小売りの現場で需要予測を行うことは，在庫費用の削減や品切れ損出の回避などを目的としたものであり，予測の精度だけを問題にするのではなく，どの程度外れているのかという誤差の管理と，外れた場合の影響や，緊急発注などの回避手段とあわせて考える必要がある．\nまた，予測するだけでなく，なぜそのような値になったのかを究明することも重要である．需要が０という日が続いた場合には，それが，本当に需要がなかったのか，それとも在庫がないために売れなかったのか，陳列場所が悪かったために売れなかったのか，などをあわせて原因を分析する必要がある．\n多くのメーカーでは，需要予測をするためだけの部署を設けているが，これもナンセンスなのでできるだけ早くほかの部署と連携をとるように改めるべきである．需要は当てるものではなくコントロールするものであり，需要予測を当てることだけを目標としている部署は，廃止すべきである．\n\n予測は外れるもの\n\nしばしば予測が当たったとか外れたとかいう言葉を現場の人から聞くが，経営はギャンブルではないので的中というのはありえない．この人なら当たるとかいうのは迷信であり，たまたま当たったときに声を張り上げて宣伝しているか，誤差が大きいにもかかわらず当たったと宣伝しているかの何れかである．サプライ・チェインからはちょっと外れるが，地震の予測（予知ともいう）も似たようなものであり，日本中のどこかで地震が発生すると予測し，そのうち1つが当たると的中と宣伝していたりする．ましてや，株や競馬の予想的中などの宣伝は，たまたま当たったときの結果だけを掲載し，外れたときのものを消去して，予想的中の証拠として提出していたりする．いまだに，こんな宣伝にだまされる人がいるのかと感心するが，社内で需要予測が的中する人がいるという会社も似たような詐欺に遭っていると言える．\n重要なことは，どの程度外れたのかを時系列的に管理することである．誤差が増えている際には，その原因を追求し，予測手法を改善するなり，在庫を増やして品切れを回避するなりの行動をとるのが正しい方法である．\n\n集約すれば精度が上がる\n\n往々にして，個々のものの予測は難しいが，それをまとめたものの予測は容易になる．たとえば，特定の場所で特定のマグニチュードの地震が明日発生することを予測するのは難しいが，日本のどこかでマグニチュード4以上の地震が来年発生することは容易に予測できる．前者はほぼ\\(0\\)％であるが，後者はほぼ\\(１００\\)%の精度で予測できる．これが集約の力である．\nサプライ・チェインでも同様であり，商品を集約して商品群にすれば精度があがり，個々の店舗での売上でなく，地域内のすべての店舗の売上を集約すれば精度が上がる． 時間軸でも同様であり，１時間以内の需要を予測精度は，日，週，月，年単位と集約していくにしたがってあがっていく．\n予測精度だけを議論するのであれば，どんどん集約した方が得であるが，もちろんどんどん役に立たなくなる．前述したように，重要なことは，その予測を何に使うかであり，使用法にあわせて「適切に」集約を行うことである．\nまた，製品設計や在庫地点を考慮することによって，物理的に集約を行うこともできる．たとえば，製品のモジュール化や遅延差別化やリスク共同管理がこれに相当する．これらはモダンなサプライ・チェインの基本戦略であり，そのすべてがこの単純な公理（集約した方が予測精度が上がる）に基づくものであることは興味深い．\n\n目先の需要は当たりやすく，遠い未来の需要は予測しにくい\n\n明日の天気はある程度予測できるが，1年後の天気は予測しにくい．不確実性は時間が経過するにしたがって増大するからである．需要予測も同様である．たとえば，カップラーメンなどは一部の定番品を除いて，来年店頭に並んでいるかどうかも怪しい．\n近い未来の予測は，短期のオペレーショナルな意思決定に用いるので，比較的正確性が必要であるが，遠い未来の予測は長期のストラテジックな意思決定に用いるので，おおよそで構わない．さらに，長期の意思決定の際には，データは集約して行われるので，予測精度も上がる． 要は，目的のために適切な精度で管理できるように，時間軸を集約することが推奨される．たとえば，近い未来は予測精度がよいので，集約をせずに日単位で予測し，未来に行くに従って時間軸の集約を行い，週単位，月単位，年単位としていく訳である．このテクニックはテレスコーピングと呼ばれ，時間軸を含んだ実際問題を解くときによく用いられる．\n\n\n予測手法と使い分け\n古典的な予測手法は statsmodelsに入っている。種々の指数平滑法の拡張や、ARIMAなどは簡単にできる。 いずれも高速なので、補助的なデータがない時系列データに対して、簡易的な予測をしたい場合には便利だが、予測精度は期待すべきではない。\n機械（深層）学習を用いた予測は、 scikit-learn（最近ではpycaret）やfastaiのTabularモデルやLSTMを用いることによって容易にできる。 補助的なデータが豊富な場合には、これらの手法が推奨される。\n補助的なデータが少ない時系列データの場合には、prophetを用いたベイズ推論が推奨される。ベイズ推論だと、予測を点でするのではなく、不確実性の範囲まで得られるので、 サプライ・チェインのモデルと相性が良い。例えば、在庫モデルにおいては、不確実性の情報が不可欠だからだ。\n\n# hide_input\nfrom IPython.display import Image\n\nImage(\"../figure/how_to_select.PNG\", width=700, height=400)"
  },
  {
    "objectID": "10forecast.html#ベイズ推論",
    "href": "10forecast.html#ベイズ推論",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "ベイズ推論",
    "text": "ベイズ推論\nここでは， ベイズ推論 (Bayesian inference) をPyMCパッケージ https://www.pymc.io/ を用いて解説する． PyMCはGoogle Colabにプレインストールされている．\n\nベイズの公式\nベイズ推論は，高校で学習するベイズの公式を利用する．\n\n\\(P(A | B)\\) ： \\(B\\) という事象が発生したきに \\(A\\) が起こる条件付き確率\n\\(P( A \\cap B)\\): 事象 \\(A\\) と \\(B\\) が同時に発生する確率\n\n\\[\nP(A|B) = \\frac{P( A \\cap B)}{P(B)}\n\\]\n同様に， \\[\nP(B|A) = \\frac{P( A \\cap B)}{P(A)}\n\\]\nベン図を使うと簡単に理解できる．\n\n#hide\n#Image(\"../figure/bayes1.PNG\", width=700, height=400)\n\n\n\n事前分布と事後分布と尤度関数\n\n\\(P(\\bar{B})\\): 事象 \\(B\\) が発生しない確率\n\\(P(B) + P(\\bar{B}) =1\\)\n\nベイズの公式から， \\[\nP(B|A) = \\frac{P(A|B) P(B) }{P(A)} =  \\frac{P( A|B) p(B) }{P(A|B) P(B) + P(A|\\bar{B}) P(\\bar{B})  }\n\\]\n\\(B\\) をモデルのパラメータ \\(\\theta\\)， \\(A\\) を（観測された）データとすると， \\[\nP(\\theta| data) = \\frac{P( data|\\theta) P(\\theta) }{P(data)}\n\\]\n\n\\(P(\\theta)\\): 事前分布 (prior)\n\\(P( data|\\theta)\\) : パラメータ \\(\\theta\\) に対する尤度関数 \\(L(\\theta)\\) (likelihood)\n\\(P(\\theta| data)\\) : 事後分布 (posterior)\n\n事後分布は，尤度と事前分布の積に比例する．\n\\[\nP(\\theta| data) \\propto P( data|\\theta) P(\\theta) = Likelyhood \\times Prior\n\\]\n\n\nベイズ線形回帰\n例として， 通常の線形回帰をベイズ推論を用いて行う． 以下の2変数の線形モデルを仮定し，データを生成する．\n\\[\n\\begin{array}{l l}\nY  &\\sim N(\\mu, \\sigma^2) \\\\\n\\mu &= \\alpha + \\beta_1 X_1 + \\beta_2 X_2\n\\end{array}\n\\]\n\nimport arviz as az\nimport numpy as np\nimport pymc as pm\n\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\nsize = 100\n\nX1 = np.random.randn(size)\nX2 = np.random.randn(size) * 0.2\n\nY = alpha + beta[0] * X1 + beta[1] * X2 + np.random.normal(size=size) * sigma\n\n\n# hide\n#Image(\"../figure/bayes2.PNG\", width=700, height=400)\n\n\n\nベイズ推論\nベイズ推論は， 事前分布と尤度関数によって定義された生成モデルがあれば，ベイズの公式を用いて，データが与えられた条件下での事後分布の推定が可能であることを利用する．\n線形回帰の例では，事前分布は \\(\\alpha, \\beta\\) は正規分布， \\(\\sigma\\) は負の部分を除いた半正規分布とし， 生成モデルは \\(\\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2\\)， 尤度関数は \\(Y \\sim N(\\mu, \\sigma^2)\\) とする．\n\n# hide_input\nImage(\"../figure/bayes3.PNG\", width=700, height=400)\n\n\n\n\n\nmodel = pm.Model()\nwith model:\n    # 事前分布\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n    beta = pm.Normal(\"beta\", mu=0, sigma=10, shape=2)\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n\n    # 生成モデル\n    mu = alpha + beta[0] * X1 + beta[1] * X2\n\n    # 尤度関数\n    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=Y)\n\n\n\nMCMC (Markov chain Monte Carlo)法\n実際の分布の推定には， MCMC (Markov chain Monte Carlo)法を利用する． \nMCMC法は，以下に示すように改悪も許した確率的な探索を行うことによって，分布のサンプリングを行う．\n\n# hide_input\nImage(\"../figure/mcmc.PNG\", width=800, height=500)\n\n\n\n\nベイズ線形回帰の事後分布をサンプリングによって生成する．\n\nwith model:\n    # 事後分布を 1000 個サンプル（時間がかかるのでしばらく待つ）\n    idata = pm.sample(draws=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:02&lt;00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:01&lt;00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\n\n\n結果の表示\n\naz.plot_trace(idata, combined=True, figsize=(10,10));\n\n\n\n\n\naz.summary(idata, round_to=2)\n\n\n  \n    \n      \n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nalpha\n1.03\n0.09\n0.86\n1.21\n0.00\n0.00\n3101.07\n1585.09\n1.0\n\n\nbeta[0]\n1.06\n0.09\n0.89\n1.22\n0.00\n0.00\n2642.38\n1633.52\n1.0\n\n\nbeta[1]\n2.44\n0.45\n1.64\n3.31\n0.01\n0.01\n3164.36\n1788.72\n1.0\n\n\nsigma\n0.94\n0.07\n0.81\n1.07\n0.00\n0.00\n2338.74\n1457.89\n1.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n元のデータは，以下のコードによって生成していたので，良い近似になっていることが確認できる．\n\nalpha, sigma = 1, 1\nbeta = [1, 2.5]"
  },
  {
    "objectID": "10forecast.html#諸パッケージのインポート",
    "href": "10forecast.html#諸パッケージのインポート",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "諸パッケージのインポート",
    "text": "諸パッケージのインポート\nProphetで予測するために必要なパッケージをインポートしておく． vega_datasetsのデータを用いるので，インストールしておく．\nhttps://github.com/altair-viz/vega_datasets\n\n# Google Colabの場合\n#import plotly.io as pio\n#pio.renderers.default = \"colab\"\n#!pip install prophet\n#!pip install -U vega_datasets\n\nimport pandas as pd\nfrom prophet import Prophet\nfrom vega_datasets import data\nimport plotly.express as px\nimport prophet.plot as fp\nimport plotly\n\n\n# export\nimport pandas as pd\nfrom prophet import Prophet\nfrom vega_datasets import data\nimport plotly.express as px\nimport prophet.plot as fp\nimport plotly"
  },
  {
    "objectID": "10forecast.html#prophetの基本",
    "href": "10forecast.html#prophetの基本",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "Prophetの基本",
    "text": "Prophetの基本\nProphetをPythonから呼び出して使う方法は，機械学習パッケージscikit-learnと同じである。\n\nProphetクラスのインスタンスmodelを生成\nfitメソッドで学習（引数はデータフレーム）\npredictメソッドで予測（引数は予測したい期間を含んだデータフレーム）\n\n\n例題：Wikiアクセス数\n例としてアメリカンフットボールプレーヤのPayton ManningのWikiアクセス数のデータを用いる。\n\ndf = pd.read_csv(\"http://logopt.com/data/peyton_manning.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n2007-12-10\n9.590761\n\n\n1\n2007-12-11\n8.519590\n\n\n2\n2007-12-12\n8.183677\n\n\n3\n2007-12-13\n8.072467\n\n\n4\n2007-12-14\n7.893572\n\n\n\n\n\n\n\nProphetモデルのインスタンスを生成し，fitメソッドで学習（パラメータの最適化）を行う．fitメソッドに渡すのは，上で作成したデータフレームである．このとき、ds(datestamp)列に日付（時刻）を、y列に予測したい数値を入れておく必要がある （この例題では，あらかじめそのように変更されている）．\n\nmodel = Prophet()\nmodel.fit(df)\n\n09:28:44 - cmdstanpy - INFO - Chain [1] start processing\n09:28:45 - cmdstanpy - INFO - Chain [1] done processing\n\n\n&lt;prophet.forecaster.Prophet&gt;\n\n\nmake_future_dataframeメソッドで未来の時刻を表すデータフレームを生成する。既定値では、予測で用いた過去の時刻も含む。 引数は予測をしたい期間数periodsであり，ここでは、１年後（365日分）まで予測することにする。\n\nfuture = model.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n\n\n\nds\n\n\n\n\n3265\n2017-01-15\n\n\n3266\n2017-01-16\n\n\n3267\n2017-01-17\n\n\n3268\n2017-01-18\n\n\n3269\n2017-01-19\n\n\n\n\n\n\n\npredict メソッドに予測したい時刻を含んだデータフレームfuture を渡すと、予測値を入れたデータフレームforecastを返す。このデータフレームは、予測値yhatの他に、予測の幅などの情報をもった列を含む。以下では，予測値yhatの他に，予測の上限と下限（yhat_lowerとyhat_upper）を表示している．\n\nforecast = model.predict(future)\nforecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]].tail()\n\n\n\n\n\n\n\n\nds\nyhat\nyhat_lower\nyhat_upper\n\n\n\n\n3265\n2017-01-15\n8.212625\n7.490936\n8.952181\n\n\n3266\n2017-01-16\n8.537635\n7.801412\n9.285417\n\n\n3267\n2017-01-17\n8.325071\n7.604013\n9.018835\n\n\n3268\n2017-01-18\n8.157723\n7.458736\n8.901977\n\n\n3269\n2017-01-19\n8.169677\n7.469157\n8.876939\n\n\n\n\n\n\n\nmatplotlibを用いた描画は，plotメソッドで行う．\n\nmodel.plot(forecast)\n\n\n\n\n\n\n\n\n\n一般化加法モデル\nProphetにおける予測は一般化加法モデルを用いて行われる．これは，傾向変動，季節変動，イベント情報などの様々な因子の和として予測を行う方法である．\n\\[\ny_t =g_t + s_t + h_t + \\epsilon_t\n\\]\n\n\\(y_t\\) : 予測値\n\\(g_t\\) : 傾向変動(trend)；傾向変化点ありの線形もしくはロジスティック曲線\n\\(s_t\\) : 季節変動；年次，週次，日次の季節変動をsin, cosの組み合わせ（フーリエ級数）で表現\n\\(h_t\\) : 休日などのイベント項\n\\(\\epsilon_t\\) : 誤差項\n\n因子ごとに予測値の描画を行うには，plot_componentsメソッドを用いる．既定では，以下のように，上から順に傾向変動，週次の季節変動，年次の季節変動が描画される．また，傾向変動の図（一番上）には，予測の誤差範囲が示される．季節変動の誤差範囲を得る方法については，後述する．\n\nmodel.plot_components(forecast)\n\n\n\n\n\n\n\n対話形式に，拡大縮小や範囲指定ができる動的な図も，Plotlyライブラリを用いて得ることができる．\n\nfig = fp.plot_plotly(model, forecast) \nplotly.offline.plot(fig);\n\n\n# hide_input\nImage(\"../figure/prophet1.PNG\", width=700, height=400)\n\n\n\n\n\n\n例題： \\(CO_2\\) 排出量のデータ\nデータライブラリから二酸化炭素排出量のデータを読み込み，Plotly Expressで描画する．\n\nco2 = data.co2_concentration()\nco2.head()\n\n\n\n\n\n\n\n\nDate\nCO2\n\n\n\n\n0\n1958-03-01\n315.70\n\n\n1\n1958-04-01\n317.46\n\n\n2\n1958-05-01\n317.51\n\n\n3\n1958-07-01\n315.86\n\n\n4\n1958-08-01\n314.93\n\n\n\n\n\n\n\n\nfig = px.line(co2,x=\"Date\",y=\"CO2\")\nplotly.offline.plot(fig);\n\n\n                                                \n\n\n\n# hide_input\nImage(\"../figure/prophet2.PNG\", width=700, height=400)\n\n\n\n\n列名の変更には，データフレームのrenameメソッドを用いる．引数はcolumnsで，元の列名をキーとし，変更後の列名を値とした辞書を与える．また，元のデータフレームに上書きするために，inplace引数をTrueに設定しておく．\n\nco2.rename(columns={\"Date\":\"ds\",\"CO2\":\"y\"},inplace=True)\nco2.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1958-03-01\n315.70\n\n\n1\n1958-04-01\n317.46\n\n\n2\n1958-05-01\n317.51\n\n\n3\n1958-07-01\n315.86\n\n\n4\n1958-08-01\n314.93\n\n\n\n\n\n\n\nmake_future_dataframeメソッドで未来の時刻を表すデータフレームを生成する。既定値では、（予測で用いた）過去の時刻も含む。 ここでは、200ヶ月先まで予測することにする。\nそのために，引数 periods を200に，頻度を表す引数 freq をMonthを表す M に設定しておく\npredict メソッドに予測したい時刻を含んだデータフレームfuture を渡すと、予測値を入れたデータフレームforecastを返す。このデータフレームは、予測値yhatの他に、予測の幅などの列を含む。\n最後にplotメソッドで表示する．\n\nmodel = Prophet()\nmodel.fit(co2)\nfuture = model.make_future_dataframe(periods=200, freq=`M`)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n09:49:04 - cmdstanpy - INFO - Chain [1] start processing\n09:49:04 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n予測は一般化加法モデルを用いて行われる．\nこれは，傾向変動，季節変動，イベント情報などの様々な因子の和として予測を行う方法である．\n上に表示されているように，週次と日次の季節変動は無視され，年次の季節変動のみ考慮して予測している．\n因子ごとに予測値の描画を行うには，plot_componentsメソッドを用いる．既定では，以下のように，上から順に傾向変動，週次の季節変動，年次の季節変動が描画される．また，傾向変動の図（一番上）には，予測の誤差範囲が示される．季節変動の誤差範囲を得る方法については，後述する．\n\nmodel.plot_components(forecast);\n\n\n\n\nPlotlyで描画すると， 一部を拡大，期の選択などが可能になる．\n\nfig = fp.plot_plotly(model, forecast)\nplotly.offline.plot(fig);\n\n\n# hide_input\nImage(\"../figure/prophet3.PNG\", width=700, height=400)\n\n\n\n\n\n\n例題：航空機乗客数のデータ\nProphetの既定値では季節変動は加法的モデルであるが、問題によっては乗法的季節変動の方が良い場合もある。 例として、航空機の乗客数を予測してみよう。最初に既定値の加法的季節変動モデルで予測し，次いで乗法的モデルで予測する．\n\npassengers = pd.read_csv(\"http://logopt.com/data/AirPassengers.csv\")\npassengers.head()\n\n\n\n\n\n\n\n\nMonth\n#Passengers\n\n\n\n\n0\n1949-01\n112\n\n\n1\n1949-02\n118\n\n\n2\n1949-03\n132\n\n\n3\n1949-04\n129\n\n\n4\n1949-05\n121\n\n\n\n\n\n\n\n\nfig = px.line(passengers,x=\"Month\",y=\"#Passengers\")\nplotly.offline.plot(fig);\n\n\n# hide_input\nImage(\"../figure/prophet4.PNG\", width=700, height=400)\n\n\n\n\n\npassengers.rename(inplace=True,columns={\"Month\":\"ds\",\"#Passengers\":\"y\"})\npassengers.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1949-01\n112\n\n\n1\n1949-02\n118\n\n\n2\n1949-03\n132\n\n\n3\n1949-04\n129\n\n\n4\n1949-05\n121\n\n\n\n\n\n\n\n季節変動を乗法的に変更するには， モデルの seasonality_mode 引数を乗法的を表す multiplicative に設定する．\nなお，以下のデータは月次のデータであるので，make_future_dataframeの freq 引数を M (Month)に設定する．\n\nmodel = Prophet().fit(passengers)\nfuture = model.make_future_dataframe(periods=20, freq=\"M\")\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n09:58:45 - cmdstanpy - INFO - Chain [1] start processing\n09:58:45 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\nmodel = Prophet(seasonality_mode=\"multiplicative\").fit(passengers)\nfuture = model.make_future_dataframe(periods=20, freq=\"M\")\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n09:56:17 - cmdstanpy - INFO - Chain [1] start processing\n09:56:17 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n結果から，乗法的季節変動の方が，良い予測になっていることが確認できる．\n\n#hide\n#model.plot_components(forecast);\n\n\n\n問題（小売りの需要データ）\n以下の，小売りの需要データを描画し，予測を行え． ただし，モデルは乗法的季節変動で，月次で予測せよ．\n\nretail = pd.read_csv(`http://logopt.com/data/retail_sales.csv`)\nretail.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1992-01-01\n146376\n\n\n1\n1992-02-01\n147079\n\n\n2\n1992-03-01\n159336\n\n\n3\n1992-04-01\n163669\n\n\n4\n1992-05-01\n170068\n\n\n\n\n\n\n\n\n#export\n# retail = pd.read_csv(`http://logopt.com/data/retail_sales.csv`)\n# model = Prophet(seasonality_mode=`multiplicative`).fit(retail)\n# future = model.make_future_dataframe(periods=20, freq=`M`)\n# forecast = model.predict(future)\n# model.plot(forecast);\n# #model.plot_components(forecast);\n\n\n\n例題： 1時間ごとの気温データ\nここではシアトルの気温の予測を行う．\n\nclimate = data.seattle_temps()\nclimate.head()\n\n\n\n\n\n\n\n\ndate\ntemp\n\n\n\n\n0\n2010-01-01 00:00:00\n39.4\n\n\n1\n2010-01-01 01:00:00\n39.2\n\n\n2\n2010-01-01 02:00:00\n39.0\n\n\n3\n2010-01-01 03:00:00\n38.9\n\n\n4\n2010-01-01 04:00:00\n38.8\n\n\n\n\n\n\n\nこのデータは， date 列に日付と1時間ごとの時刻が， temp 列に気温データが入っている．\nProphetは， 日別でないデータも扱うことができる。 date列のデータ形式は、日付を表すYYYY-MM-DDの後に時刻を表すHH:MM:SSが追加されている。 未来の時刻を表すデータフレームは、make_future_dataframeメソッドで生成するが、このとき引数freqで時間の刻みを指定する。 ここでは1時間を表す H を指定する。\n\nclimate[\"Date\"] = pd.to_datetime(climate.date)\n\n\nclimate.rename(columns={\"Date\":\"ds\",\"temp\":\"y\"},inplace=True)\n\n\nmodel = Prophet().fit(climate)\nfuture = model.make_future_dataframe(periods=200, freq=\"H\")\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n10:06:08 - cmdstanpy - INFO - Chain [1] start processing\n10:06:11 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n因子ごとに予測値を描画すると，傾向変動と週次の季節変動の他に，日次の季節変動（1日の気温の変化）も出力される．\n\nmodel.plot_components(forecast);\n\n\n\n\n\n\n問題（サンフランシスコの気温データ）\n以下のサンフランシスコの気温データを描画し，時間単位で予測を行え．\n\nsf = data.sf_temps()\nsf.head()\n\n\n\n\n\n\n\n\ntemp\ndate\n\n\n\n\n0\n47.8\n2010-01-01 00:00:00\n\n\n1\n47.4\n2010-01-01 01:00:00\n\n\n2\n46.9\n2010-01-01 02:00:00\n\n\n3\n46.5\n2010-01-01 03:00:00\n\n\n4\n46.0\n2010-01-01 04:00:00\n\n\n\n\n\n\n\n\n#export\n# sf = data.sf_temps()\n# sf[\"Date\"] = pd.to_datetime(sf.date)\n# sf.rename(columns={\"Date\":\"ds\",\"temp\":\"y\"},inplace=True)\n# model = Prophet().fit(sf)\n# future = model.make_future_dataframe(periods=200, freq=`H`)\n# forecast = model.predict(future)\n# model.plot(forecast);\n\n#hide \n\n#hide\ngithub = data.github()\ngithub.tail()\n\n\n\n\n\n\n\n\ntime\ncount\n\n\n\n\n950\n2015-05-29 17:00:00\n1\n\n\n951\n2015-05-29 19:00:00\n1\n\n\n952\n2015-05-30 00:00:00\n10\n\n\n953\n2015-05-30 09:00:00\n1\n\n\n954\n2015-05-30 11:00:00\n2\n\n\n\n\n\n\n\n\n#hide\n# fig = px.line(github,x=\"time\",y=\"count\")\n# plotly.offline.plot(fig);\n\n#hide\n\n\n#hide\n#github[\"cumsum\"] = np.log(github[\"count\"]+0.1)\ngithub[\"cumsum\"] = github[\"count\"].cumsum()\ngithub.rename(inplace=True,columns={\"time\":\"ds\",\"cumsum\":\"y\"})\ngithub.head()\n\n\n\n\n\n\n\n\nds\ncount\ny\n\n\n\n\n0\n2015-01-01 01:00:00\n2\n2\n\n\n1\n2015-01-01 04:00:00\n3\n5\n\n\n2\n2015-01-01 05:00:00\n1\n6\n\n\n3\n2015-01-01 08:00:00\n1\n7\n\n\n4\n2015-01-01 09:00:00\n3\n10\n\n\n\n\n\n\n\n\n#hide\nmodel = Prophet().fit(github)\nfuture = model.make_future_dataframe(periods=200, freq=\"H\")\nforecast = model.predict(future)\nmodel.plot(forecast);\nmodel.plot_components(forecast);\n\n11:20:22 - cmdstanpy - INFO - Chain [1] start processing\n11:20:22 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n#hide\n#forecast[\"error\"] = np.exp(forecast.yhat)-np.exp(github.y)\n# forecast[\"error\"] = forecast.yhat - github.y\nforecast[\"error\"].plot();\n\n\n\n\n\n\n傾向変化点\n「上昇トレンドの株価が，下降トレンドに移った」というニュースをよく耳にするだろう．このように，傾向変動は，時々変化すると仮定した方が自然なのだ．Prophetでは，これを傾向の変化点として処理する．再び，Peyton Manningのデータを使う．\nadd_changepoints_to_plotを使うと、変化した点（日次）と傾向変動を図に追加して描画できる。引数は軸(axis)，モデル(model)，予測データフレーム(forecast)であり， 軸は図オブジェクトのgca(get current axis)メソッドで得る．\n\ndf = pd.read_csv(`http://logopt.com/data/peyton_manning.csv`)\nmodel = Prophet().fit(df)\nfuture = model.make_future_dataframe(periods=366)\nforecast = model.predict(future)\nfig = model.plot(forecast)\na = fp.add_changepoints_to_plot(fig.gca(),model,forecast);\n\n11:27:07 - cmdstanpy - INFO - Chain [1] start processing\n11:27:08 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n変化点の数を制御するための引数は changepoint_prior_scale であり、既定値は \\(0.05\\) である。これを増やすと変化点が増え、予測の自由度が増すため予測幅が大きくなる。\n\nmodel = Prophet(changepoint_prior_scale=0.5).fit(df)\nfuture = model.make_future_dataframe(periods=366)\nforecast = model.predict(future)\nfig = model.plot(forecast)\nfp.add_changepoints_to_plot(fig.gca(), model, forecast);\n\n11:28:49 - cmdstanpy - INFO - Chain [1] start processing\n11:28:51 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n傾向変化点のリストをchangepoints引数で与えることもできる。以下の例では、1つの日だけで変化するように設定している。\n\nmodel = Prophet(changepoints=[`2014-01-01`]).fit(df)\nfuture = model.make_future_dataframe(periods=366)\nforecast = model.predict(future)\nfig = model.plot(forecast)\nfp.add_changepoints_to_plot(fig.gca(), model, forecast);\n\n11:28:59 - cmdstanpy - INFO - Chain [1] start processing\n11:28:59 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n例題 SP500データ\n株価の予測を行う．\n傾向変化点の候補は自動的に設定される。既定値では時系列の最初の80%の部分に均等に設定される。これは、モデルのchangepoint_range引数で設定する． この例では，期間の終わりで変化点を設定したいので，0.95に変更する．\n年次の季節変動の変化の度合いは、yearly_seasonality（既定値は \\(10\\)) で制御できる。この例では，このパラメータを \\(5\\) に変更することによって年間の季節変動を抑制して予測を行う．\n\nsp500 = data.sp500()\nsp500.tail()\n\n\n\n\n\n\n\n\ndate\nprice\n\n\n\n\n118\n2009-11-01\n1095.63\n\n\n119\n2009-12-01\n1115.10\n\n\n120\n2010-01-01\n1073.87\n\n\n121\n2010-02-01\n1104.49\n\n\n122\n2010-03-01\n1140.45\n\n\n\n\n\n\n\n\n#hide\n# fig = px.line(sp500,x=\"date\",y=\"price\") \n# plotly.offline.plot(fig);\n\n\nsp500.rename(inplace=True,columns={\"date\":\"ds\",\"price\":\"y\"})\n\n\nmodel = Prophet(changepoint_prior_scale=0.5, changepoint_range=0.95,yearly_seasonality=5).fit(sp500)\nfuture = model.make_future_dataframe(periods=200, freq=`D`)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:30:30 - cmdstanpy - INFO - Chain [1] start processing\n11:30:30 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\nmodel.plot_components(forecast);\n\n\n\n\n\nfrom prophet.plot import add_changepoints_to_plot\nfig = model.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), model, forecast)\n\n\n\n\n\n\n例題： 個別銘柄の株価の予測\nstocksデータでは，symbol列に企業コードが入っている．\n\nAAPL アップル\nAMZN アマゾン\nIBM IBM\nGOOG グーグル\nMSFT マイクロソフト\n\nまずは可視化を行う。\n\nstocks = data.stocks()\nstocks.tail()\n\n\n\n\n\n\n\n\nsymbol\ndate\nprice\n\n\n\n\n555\nAAPL\n2009-11-01\n199.91\n\n\n556\nAAPL\n2009-12-01\n210.73\n\n\n557\nAAPL\n2010-01-01\n192.06\n\n\n558\nAAPL\n2010-02-01\n204.62\n\n\n559\nAAPL\n2010-03-01\n223.02\n\n\n\n\n\n\n\n\nfig = px.line(stocks,x=\"date\",y=\"price\",color=\"symbol\")\nplotly.offline.plot(fig);\n\n\n                                                \n\n\n\n# hide_input\nImage(\"../figure/prophet5.PNG\", width=700, height=400)\n\n\n\n\n以下では，マイクロソフトの株価を予測してみる．\n\nmsft = stocks[ stocks.symbol == \"MSFT\"]\nmsft.head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nprice\n\n\n\n\n0\nMSFT\n2000-01-01\n39.81\n\n\n1\nMSFT\n2000-02-01\n36.35\n\n\n2\nMSFT\n2000-03-01\n43.22\n\n\n3\nMSFT\n2000-04-01\n28.37\n\n\n4\nMSFT\n2000-05-01\n25.45\n\n\n\n\n\n\n\n\nmsft = msft.rename(columns={\"date\":\"ds\",\"price\":\"y\"})\nmsft.head()\n\n\n\n\n\n\n\n\nsymbol\nds\ny\n\n\n\n\n0\nMSFT\n2000-01-01\n39.81\n\n\n1\nMSFT\n2000-02-01\n36.35\n\n\n2\nMSFT\n2000-03-01\n43.22\n\n\n3\nMSFT\n2000-04-01\n28.37\n\n\n4\nMSFT\n2000-05-01\n25.45\n\n\n\n\n\n\n\n\nmodel = Prophet(changepoint_prior_scale=0.5, changepoint_range=0.95,yearly_seasonality=5).fit(msft)\nfuture = model.make_future_dataframe(periods=200, freq=`D`)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:33:31 - cmdstanpy - INFO - Chain [1] start processing\n11:33:31 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\nmodel.plot_components(forecast);\n\n\n\n\n\n\n問題（株価）\n上の株価データのマイクロソフト以外の銘柄を1つ選択し，予測を行え．\n\nstocks = data.stocks()\nstocks.head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nprice\n\n\n\n\n0\nMSFT\n2000-01-01\n39.81\n\n\n1\nMSFT\n2000-02-01\n36.35\n\n\n2\nMSFT\n2000-03-01\n43.22\n\n\n3\nMSFT\n2000-04-01\n28.37\n\n\n4\nMSFT\n2000-05-01\n25.45\n\n\n\n\n\n\n\n\n#export\n# stocks = data.stocks()\n# amzn = stocks[ stocks.symbol == \"AMZN\"]\n# amzn = amzn.rename(columns={\"date\":\"ds\",\"price\":\"y\"})\n# model = Prophet(changepoint_prior_scale=0.5, changepoint_range=0.95, yearly_seasonality=5).fit(amzn)\n# future = model.make_future_dataframe(periods=200, freq=`D`)\n# forecast = model.predict(future)\n# model.plot(forecast);"
  },
  {
    "objectID": "10forecast.html#発展編",
    "href": "10forecast.html#発展編",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "発展編",
    "text": "発展編\n以下では，Prophetの高度な使用法を解説する．\n\nロジスティック曲線による予測\nProphetによる予測の既定値は線形モデルであるが、ロジスティック曲線を用いることもできる。これによって，上限や下限に漸近する時系列データの予測を行うことができる．\n上限を規定するためには、データフレームのcap列に上限値（容量(capacity)の略でcap）を入力する。(下限値を設定する場合には、floor列に下限値を入力する.) これは行(データ）ごとに設定しなければならない．\n次いで、引数growthをlogisticに設定してProphetモデルを生成すると、ロジスティック曲線に当てはめを行う。\n\nlogistic = pd.read_csv(\"http://logopt.com/data/logistic.csv\")\n\n\n#hide\nfig = px.line(logistic,x=\"ds\",y=\"y\")\nplotly.offline.plot(fig);\n\n\n                                                \n\n\n\nmodel = Prophet(growth=\"logistic\") \nlogistic[\"cap\"] = 8.5 \nmodel.fit(logistic)    \nfuture = model.make_future_dataframe(periods=1826)\nfuture[`cap`] = 8.5\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:34:53 - cmdstanpy - INFO - Chain [1] start processing\n11:34:53 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n外れ値の影響\n外れ値(outlier)を除去すると予測の精度が向上する場合がある。以下の例では、2010年あたりに大きな変化があるため、予測の幅が広がっている。\n\noutliers1 = pd.read_csv(\"http://logopt.com/data/outliers1.csv\")\n\n\n#hide\nfig = px.line(outliers1,x=\"ds\",y=\"y\")\nplotly.offline.plot(fig);\n\n\n                                                \n\n\n\nmodel = Prophet()\nmodel.fit(outliers1)\nfuture = model.make_future_dataframe(periods=1096)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:35:45 - cmdstanpy - INFO - Chain [1] start processing\n11:35:47 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n2010年のデータを除外することによって、予測が改善される。\n\noutliers1.loc[(outliers1[`ds`] &gt; `2010-01-01`) & (outliers1[`ds`] &lt; `2011-01-01`), `y`] = None\nmodel =Prophet()\nmodel.fit(outliers1)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:35:52 - cmdstanpy - INFO - Chain [1] start processing\n11:35:52 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n上では、外れ値を除外することによって予測が改善されたが、これがいつでも成立するとは限らない。以下の例では2015年6月に外れ値が観察される。　\n\noutliers2 = pd.read_csv(\"http://logopt.com/data/outliers2.csv\")\n\n\n#hide\nfig = px.line(outliers2,x=\"ds\",y=\"y\")\nplotly.offline.plot(fig);\n\n\n                                                \n\n\n\nmodel = Prophet()\nmodel.fit(outliers2)\nfuture = model.make_future_dataframe(periods=1096)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:36:21 - cmdstanpy - INFO - Chain [1] start processing\n11:36:21 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n上の予測では2015年6月のデータを予測に用いず，外れ値として処理しているので，外れ値を除外すると予測の幅が広がる。\n\noutliers2.loc[(outliers2[`ds`] &gt; `2015-06-01`) & (outliers2[`ds`] &lt; `2015-06-30`), `y`] = None\nmodel = Prophet()\nmodel.fit(outliers2)\nfuture = model.make_future_dataframe(periods=1096)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:36:26 - cmdstanpy - INFO - Chain [1] start processing\n11:36:27 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n休日（特別なイベント）を考慮した予測\n休日や特別なイベントをモデルに追加することを考える。そのためには、holiday と ds(datestamp)を列名としたデータフレームを準備する必要がある。holiday列にはイベント名を、dsにはそのイベントが発生する日時を入力する。\n以下では、holiday 列に superbowl と playoff の2種類を入れる。\nまた、イベントの影響が指定した日時の前後何日まで影響を与えるかを示す2つの列lower_windowと upper_windowを追加することができる。\n例としてPeyton Manningの例題のデータを用いる．\n\ndf = pd.read_csv(`http://logopt.com/data/peyton_manning.csv`)\ndf.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n2007-12-10\n9.590761\n\n\n1\n2007-12-11\n8.519590\n\n\n2\n2007-12-12\n8.183677\n\n\n3\n2007-12-13\n8.072467\n\n\n4\n2007-12-14\n7.893572\n\n\n\n\n\n\n\n\nplayoffs = pd.DataFrame({\n  `holiday`: `playoff`,\n  `ds`: pd.to_datetime([`2008-01-13`, `2009-01-03`, `2010-01-16`,\n                        `2010-01-24`, `2010-02-07`, `2011-01-08`,\n                        `2013-01-12`, `2014-01-12`, `2014-01-19`,\n                        `2014-02-02`, `2015-01-11`, `2016-01-17`,\n                        `2016-01-24`, `2016-02-07`]),\n  `lower_window`: 0,\n  `upper_window`: 1,\n})\nsuperbowls = pd.DataFrame({\n  `holiday`: `superbowl`,\n  `ds`: pd.to_datetime([`2010-02-07`, `2014-02-02`, `2016-02-07`]),\n  `lower_window`: 0,\n  `upper_window`: 1,\n})\nholidays = pd.concat((playoffs, superbowls))\nholidays.head()\n\n\n\n\n\n\n\n\nholiday\nds\nlower_window\nupper_window\n\n\n\n\n0\nplayoff\n2008-01-13\n0\n1\n\n\n1\nplayoff\n2009-01-03\n0\n1\n\n\n2\nplayoff\n2010-01-16\n0\n1\n\n\n3\nplayoff\n2010-01-24\n0\n1\n\n\n4\nplayoff\n2010-02-07\n0\n1\n\n\n\n\n\n\n\n引数holidaysで休日を表すデータフレームを与えることによって、特別なイベントを考慮した予測を行うことができる。\n\nmodel= Prophet(holidays=holidays)\nmodel.fit(df)\nfuture = model.make_future_dataframe(periods=365)\nforecast = model.predict(future)\n\n11:37:41 - cmdstanpy - INFO - Chain [1] start processing\n11:37:42 - cmdstanpy - INFO - Chain [1] done processing\n\n\nプレーオフやスーパーボールなどのイベント効果がある日だけ抜き出してデータフレームを表示する．\n\nforecast[(forecast[`playoff`] + forecast[`superbowl`]).abs() &gt; 0][\n        [`ds`, `playoff`, `superbowl`]][-10:]\n\n\n\n\n\n\n\n\nds\nplayoff\nsuperbowl\n\n\n\n\n2190\n2014-02-02\n1.231269\n1.189638\n\n\n2191\n2014-02-03\n1.900381\n1.461279\n\n\n2532\n2015-01-11\n1.231269\n0.000000\n\n\n2533\n2015-01-12\n1.900381\n0.000000\n\n\n2901\n2016-01-17\n1.231269\n0.000000\n\n\n2902\n2016-01-18\n1.900381\n0.000000\n\n\n2908\n2016-01-24\n1.231269\n0.000000\n\n\n2909\n2016-01-25\n1.900381\n0.000000\n\n\n2922\n2016-02-07\n1.231269\n1.189638\n\n\n2923\n2016-02-08\n1.900381\n1.461279\n\n\n\n\n\n\n\n因子別に描画を行うと，イベントによって変化した量が描画される（上から2番目）．\n\nmodel.plot_components(forecast);\n\n\n\n\n\n\n国（州）別の休日\nadd_country_holidaysを用いて，各国（州）の休日データを追加することができる。日本のデータもあるが、天皇誕生日がずれていたりするので、注意を要する。\n\nmodel = Prophet(holidays=holidays)\nmodel.add_country_holidays(country_name=`US`)\nmodel.fit(df)\n\n11:38:05 - cmdstanpy - INFO - Chain [1] start processing\n11:38:05 - cmdstanpy - INFO - Chain [1] done processing\n\n\n&lt;prophet.forecaster.Prophet&gt;\n\n\n追加された休日名をtrain_holiday_names属性で確認しておく．\n\nmodel.train_holiday_names\n\n0                         playoff\n1                       superbowl\n2                  New Year's Day\n3      Martin Luther King Jr. Day\n4           Washington's Birthday\n5                    Memorial Day\n6                Independence Day\n7                       Labor Day\n8                    Columbus Day\n9                    Veterans Day\n10                   Thanksgiving\n11                  Christmas Day\n12       Christmas Day (Observed)\n13        Veterans Day (Observed)\n14    Independence Day (Observed)\n15      New Year's Day (Observed)\ndtype: object\n\n\n米国の休日を考慮して予測を行い，因子別に描画してみる．上から2番目が，休日に対する影響を表している．\n\nforecast = model.predict(future)\nmodel.plot_components(forecast);\n\n\n\n\n\n\n予測因子の追加\nadd_regressorメソッドを用いると、モデルに因子を追加できる。以下の例では、オンシーズンの日曜日にだけ影響がでる因子を追加している。\n\ndef nfl_sunday(ds):\n   date = pd.to_datetime(ds)\n   if date.weekday() == 6 and (date.month &gt; 8 or date.month &lt; 2):\n     return 1\n   else:\n     return 0\ndf = pd.read_csv(`http://logopt.com/data/peyton_manning.csv`)\ndf[`nfl_sunday`] = df[`ds`].apply(nfl_sunday)\nmodel = Prophet()\nmodel.add_regressor(`nfl_sunday`)\nmodel.fit(df)\nfuture = model.make_future_dataframe(periods=365)\nfuture[`nfl_sunday`] = future[`ds`].apply(nfl_sunday)\nforecast = model.predict(future)\nmodel.plot_components(forecast);\n\n11:38:22 - cmdstanpy - INFO - Chain [1] start processing\n11:38:22 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\nユーザーが設定した季節変動\nProphetでは既定値の年次や週次の季節変動だけでなく、ユーザー自身で季節変動を定義・追加できる。以下では、週次の季節変動を除き，かわりに周期が30.5日の月次変動をフーリエ次数（seasonalityの別名）5として追加している。\n\nmodel = Prophet(weekly_seasonality=False)\nmodel.add_seasonality(name=`monthly`, period=30.5, fourier_order=5)\nmodel.fit(df)\nfuture = model.make_future_dataframe(periods=365)\nforecast = model.predict(future)\nmodel.plot_components(forecast);\n\n11:38:38 - cmdstanpy - INFO - Chain [1] start processing\n11:38:38 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n他の要因に依存した季節変動\n他の要因に依存した季節変動も定義・追加することができる。以下の例では、オンシーズンとオフシーズンごと週次変動を定義し、追加してみる。\n\ndef is_nfl_season(ds):\n    date = pd.to_datetime(ds)\n    return (date.month &gt; 8 or date.month &lt; 2)\ndf[`on_season`] = df[`ds`].apply(is_nfl_season)\ndf[`off_season`] = ~df[`ds`].apply(is_nfl_season)\n\n\nmodel = Prophet(weekly_seasonality=False)\nmodel.add_seasonality(name=`weekly_on_season`, period=7, fourier_order=3, condition_name=`on_season`)\nmodel.add_seasonality(name=`weekly_off_season`, period=7, fourier_order=3, condition_name=`off_season`)\nmodel.fit(df)\nfuture = model.make_future_dataframe(periods=365)\nfuture[`on_season`] = future[`ds`].apply(is_nfl_season)\nfuture[`off_season`] = ~future[`ds`].apply(is_nfl_season)\nforecast =model.predict(future)\nmodel.plot_components(forecast);\n\n11:38:48 - cmdstanpy - INFO - Chain [1] start processing\n11:38:49 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n休日と季節変動の効果の調整法\n休日の影響を抑制するためには、holidays_prior_scaleを小さくすれば良い。このパラメータの既定値は \\(10\\) であり、これはほとんど正則化を行わないことを意味する。 一般に， prior_scale を大きくするとそのパラメータの柔軟性が増し，小さくすると柔軟性が減る．以下では，holidays_prior_scaleを \\(0.05\\) に設定して予測を行う．\n\ndf = pd.read_csv(`http://logopt.com/data/peyton_manning.csv`)\nmodel= Prophet(holidays=holidays, holidays_prior_scale=0.05)\nmodel.fit(df)\nfuture = model.make_future_dataframe(periods=365)\nforecast = model.predict(future)\n\nforecast[(forecast[`playoff`] + forecast[`superbowl`]).abs() &gt; 0][\n    [`ds`, `playoff`, `superbowl`]][-10:]\n\n11:39:23 - cmdstanpy - INFO - Chain [1] start processing\n11:39:24 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nds\nplayoff\nsuperbowl\n\n\n\n\n2190\n2014-02-02\n1.203527\n0.970955\n\n\n2191\n2014-02-03\n1.851203\n0.993308\n\n\n2532\n2015-01-11\n1.203527\n0.000000\n\n\n2533\n2015-01-12\n1.851203\n0.000000\n\n\n2901\n2016-01-17\n1.203527\n0.000000\n\n\n2902\n2016-01-18\n1.851203\n0.000000\n\n\n2908\n2016-01-24\n1.203527\n0.000000\n\n\n2909\n2016-01-25\n1.851203\n0.000000\n\n\n2922\n2016-02-07\n1.203527\n0.970955\n\n\n2923\n2016-02-08\n1.851203\n0.993308\n\n\n\n\n\n\n\nスーパーボール（superbowl）の効果が抑制されていることが見てとれる． 同様に，季節変動の影響はseasonality_prior_scaleを小さくすることによって抑制できる。\n\n\n不確実性の幅\nProphetは既定では傾向変動に対する不確実性の幅を予測する。このとき、引数interval_widthで予測の幅を設定できる。既定値は \\(0.8\\) である。このパラメータを大きくすると幅が広がり、小さくすると幅が狭くなることが確認できる。\n例として \\(CO_2\\) 排出量のデータを用いる．\n\nco2 = data.co2_concentration()\nco2.rename(columns={\"Date\":\"ds\",\"CO2\":\"y\"},inplace=True)\nco2.head()\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1958-03-01\n315.70\n\n\n1\n1958-04-01\n317.46\n\n\n2\n1958-05-01\n317.51\n\n\n3\n1958-07-01\n315.86\n\n\n4\n1958-08-01\n314.93\n\n\n\n\n\n\n\n\nmodel = Prophet(interval_width=0.95)\nmodel.fit(co2)\nfuture = model.make_future_dataframe(periods=200, freq=`M`)\nforecast = model.predict(future)\nmodel.plot(forecast);\n\n11:39:35 - cmdstanpy - INFO - Chain [1] start processing\n11:39:35 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\nforecast = Prophet(interval_width=0.5).fit(co2).predict(future)\nmodel.plot(forecast);\n\n11:39:37 - cmdstanpy - INFO - Chain [1] start processing\n11:39:37 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n季節変動に対する不確実性を予測するためには、マルコフ連鎖モンテカルロ法を行う必要がある、そのためには、引数 mcmc_samples をシミュレーションの反復回数に設定する。 このパラメータの既定値は \\(0\\) である。\nこれによって、既定値の最大事後確率（MAP）推定の代わりにマルコフ連鎖モンテカルロ法によるサンプリングが行われる。これは、非常に時間がかかることもある。 要因別に図を描画してみると、季節変動に対しても不確実性の幅が示されていることが確認できる。\n\nmodel = Prophet(mcmc_samples=100)\nforecast = model.fit(co2).predict(future)\nmodel.plot_components(forecast);\n\n11:41:11 - cmdstanpy - INFO - CmdStan installation /Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics-v-sH3Dza-py3.8/lib/python3.8/site-packages/prophet/stan_model/cmdstan-2.26.1 missing makefile, cannot get version.\n11:41:11 - cmdstanpy - INFO - Cannot determine whether version is before 2.28.\n11:41:11 - cmdstanpy - INFO - CmdStan start processing\n11:41:18 - cmdstanpy - INFO - CmdStan done processing.\n11:41:18 - cmdstanpy - WARNING - Non-fatal error during sampling:\nException: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Scale vector is 0, but must be positive finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\nException: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Scale vector is 0, but must be positive finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Scale vector is 0, but must be positive finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\nException: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Scale vector is 0, but must be positive finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Scale vector is 0, but must be positive finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\nException: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Matrix of independent variables is inf, but must be finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Scale vector is 0, but must be positive finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\n    Exception: normal_id_glm_lpdf: Scale vector is 0, but must be positive finite! (in '/Users/runner/work/prophet/prophet/python/stan/prophet.stan', line 137, column 2 to line 142, column 4)\nConsider re-running with show_console=True if the above output is unclear!\n11:41:18 - cmdstanpy - WARNING - Some chains may have failed to converge.\n    Chain 1 had 49 divergent transitions (98.0%)\n    Chain 2 had 44 divergent transitions (88.0%)\n    Chain 2 had 6 iterations at max treedepth (12.0%)\n    Chain 3 had 5 divergent transitions (10.0%)\n    Chain 3 had 45 iterations at max treedepth (90.0%)\n    Chain 4 had 46 divergent transitions (92.0%)\n    Use function \"diagnose()\" to see further information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n検証と誤差の評価\nProphetでは、予測の精度を検証するための仕組みが組み込まれている。例として、Peyton Manningのデータセットを用いる。 このデータセットは、全部で \\(2905\\) 日分のデータで構成されている。\n交差検証のためにはcross_validationを用いる。\n引数は以下の通り．\n\nmodel: 予測を行うモデル；事前にfitメソッドで学習しておく必要がある．\nhorizon : 計画期間（予測を行う期間）\nperiod : 予測の間隔；省略するとhorizonの半分が代入される．\ninitial : 交差検証を開始する最初の期；省略するとhorizonの3倍が代入される．\n\n以下の例では、initialが \\(730\\) 日なので、\\(729\\) 日までの情報を用いて、その後の \\(365\\)(horizon)日の予測を行い、本当の値との誤差を評価し、 次いで \\(730+180\\)(period)日までの情報を用いて、その後の \\(365\\) 日の予測を行い評価し、という手順を最後の日まで繰り返す。 \\((2905-730-365)/180 = 10.05\\) であるので、 \\(11\\) 回の予測を行い評価することになる。cross_validationは、交差検証用のデータフレームを返す。\n最初の検証は \\(730\\) 日後である 2010-2-15(cutoff)までのデータを用いて，2010-2-16から \\(365\\) (horizon)日分の予測で行われ、次の検証はその \\(180\\)(period)日後である2010-08-14日から行われる。最後の検証は2015-01-20日までのデータを用いて2016-01-20日まで行われる。\n\ndf = pd.read_csv(`http://logopt.com/data/peyton_manning.csv`)\nmodel = Prophet()\nmodel.fit(df);\n\n11:43:07 - cmdstanpy - INFO - Chain [1] start processing\n11:43:08 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nfrom prophet.diagnostics import cross_validation\ndf_cv = cross_validation(model, initial=`730 days`, period=`180 days`, horizon = `365 days`)\ndf_cv.head()\n\n\n\n\n11:43:17 - cmdstanpy - INFO - Chain [1] start processing\n11:43:17 - cmdstanpy - INFO - Chain [1] done processing\n11:43:17 - cmdstanpy - INFO - Chain [1] start processing\n11:43:17 - cmdstanpy - INFO - Chain [1] done processing\n11:43:18 - cmdstanpy - INFO - Chain [1] start processing\n11:43:18 - cmdstanpy - INFO - Chain [1] done processing\n11:43:19 - cmdstanpy - INFO - Chain [1] start processing\n11:43:19 - cmdstanpy - INFO - Chain [1] done processing\n11:43:19 - cmdstanpy - INFO - Chain [1] start processing\n11:43:19 - cmdstanpy - INFO - Chain [1] done processing\n11:43:20 - cmdstanpy - INFO - Chain [1] start processing\n11:43:20 - cmdstanpy - INFO - Chain [1] done processing\n11:43:20 - cmdstanpy - INFO - Chain [1] start processing\n11:43:21 - cmdstanpy - INFO - Chain [1] done processing\n11:43:21 - cmdstanpy - INFO - Chain [1] start processing\n11:43:21 - cmdstanpy - INFO - Chain [1] done processing\n11:43:22 - cmdstanpy - INFO - Chain [1] start processing\n11:43:22 - cmdstanpy - INFO - Chain [1] done processing\n11:43:23 - cmdstanpy - INFO - Chain [1] start processing\n11:43:23 - cmdstanpy - INFO - Chain [1] done processing\n11:43:24 - cmdstanpy - INFO - Chain [1] start processing\n11:43:24 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nds\nyhat\nyhat_lower\nyhat_upper\ny\ncutoff\n\n\n\n\n0\n2010-02-16\n8.959074\n8.490492\n9.469220\n8.242493\n2010-02-15\n\n\n1\n2010-02-17\n8.725548\n8.253267\n9.210082\n8.008033\n2010-02-15\n\n\n2\n2010-02-18\n8.609390\n8.144968\n9.107764\n8.045268\n2010-02-15\n\n\n3\n2010-02-19\n8.531294\n8.014229\n9.048185\n7.928766\n2010-02-15\n\n\n4\n2010-02-20\n8.273357\n7.775097\n8.779778\n7.745003\n2010-02-15\n\n\n\n\n\n\n\nperformance_metrics を用いてメトリクス（評価尺度）を計算する。評価尺度は、 平均平方誤差(mean squared error: MSE), 平均平方誤差の平方根 (root mean squared error: RMSE), 平均絶対誤差 (mean absolute error: MAE), 平均絶対パーセント誤差 (mean absolute percent error : MAPE), yhat_lower とyhat_upper の間に入っている割合（被覆率: coverage) である。\n既定値では予測期間の最初の\\(10\\)%は除外して示される。これは、引数rolling_window によって変更できる。\n\nfrom prophet.diagnostics import performance_metrics\ndf_p = performance_metrics(df_cv, rolling_window=0.1)\ndf_p.head()\n\n\n\n\n\n\n\n\nhorizon\nmse\nrmse\nmae\nmape\nmdape\nsmape\ncoverage\n\n\n\n\n0\n37 days\n0.494752\n0.703386\n0.505215\n0.058538\n0.049584\n0.058826\n0.677935\n\n\n1\n38 days\n0.500521\n0.707475\n0.510201\n0.059115\n0.049373\n0.059463\n0.675423\n\n\n2\n39 days\n0.522712\n0.722988\n0.516284\n0.059713\n0.049505\n0.060187\n0.672682\n\n\n3\n40 days\n0.529990\n0.728004\n0.519131\n0.060018\n0.049231\n0.060561\n0.673824\n\n\n4\n41 days\n0.537478\n0.733129\n0.520118\n0.060096\n0.049373\n0.060702\n0.681361\n\n\n\n\n\n\n\n評価尺度は plot_cross_validation_metricで可視化できる。以下では平均絶対パーセント誤差(MAPE)を描画している．\n\nfrom prophet.plot import plot_cross_validation_metric\nplot_cross_validation_metric(df_cv, metric=`mape`);"
  },
  {
    "objectID": "10forecast.html#主なパラメータと既定値",
    "href": "10forecast.html#主なパラメータと既定値",
    "title": "PyMCによるベイズ推論とProphetによる時系列データの予測",
    "section": "主なパラメータと既定値",
    "text": "主なパラメータと既定値\n以下にProphetの主要なパラメータ（引数）とその既定値を示す．\n\ngrowth=linear :傾向変動の関数．既定値は線形．ロジスティック曲線にするにはlogisticに設定\nchangepoints=None : 傾向変更点のリスト\nchangepoint_range = \\(0.8\\) : 傾向変化点の候補の幅（先頭から何割を候補とするか）\nn_changepoints=25 : 傾向変更点の数\nyearly_seasonality=auto : 年次の季節変動を考慮するか否か\nweekly_seasonality=auto : 週次の季節変動を考慮するか否か\ndaily_seasonality=auto : 日次の季節変動を考慮するか否か\nholidays=None : 休日のリスト\nseasonality_prior_scale= \\(10.0\\) : 季節変動の事前分布のスケール値（パラメータの柔軟性を表す）\nholidays_prior_scale= \\(10.0\\) : 休日のの事前分布のスケール値（パラメータの柔軟性を表す）\nchangepoint_prior_scale= \\(0.05\\) : 傾向変更点の事前分布のスケール値（パラメータの柔軟性を表す）\nmcmc_samples= \\(0\\) : MCMC法のサンプル数\ninterval_width= \\(0.8\\) : 不確実性の幅\nuncertainty_samples= \\(1000\\) : 不確実性の幅を計算する際のサンプル数"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()"
  },
  {
    "objectID": "11networkx.html",
    "href": "11networkx.html",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "",
    "text": "NetwrokX https://networkx.org/ はグラフ・ネットワーク解析のためのパッケージである．\nここでは実務で用いる機能を中心に解説する．\nimport networkx as nx  # networkxパッケージを nx という別名で読み込み\nimport matplotlib.pyplot as plt  # 描画用パッケージの読み込み\n\n# 図の表示用のマジックコマンド"
  },
  {
    "objectID": "11networkx.html#グラフ理論",
    "href": "11networkx.html#グラフ理論",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "グラフ理論",
    "text": "グラフ理論\nグラフ \\(G=(V,E)\\) とは，点集合 \\(V\\) と枝集合 \\(E\\) から構成される概念である． 点集合の要素を点(vertex, node)とよび， \\(u, v (\\in V)\\) などの記号で表す． 枝集合の要素を枝(edge, arc)とよび， \\(e (\\in E)\\) と表す． 2点間に複数の枝がない場合には，両端点 \\(u,v\\) を決めれば一意に枝が定まるので，枝を両端にある点の組として \\((u,v)\\) もしくは \\(uv\\) と表すことができる．\n枝に「向き」をつけたグラフを有向グラフ(directed graph, digraph)とよび，有向グラフの枝を有向枝(directed edge， arc)とよぶ． 一方，通常の（枝に向きをつけない）グラフであることを強調したいときには，グラフを無向グラフ(undirected graph)とよぶ．\nネットワーク(network)とは，有向グラフに枝上を流れる「もの」（フロー）を付加した概念である． ネットワーク上の最適化理論は 1950年代にはじまり，その実務的な重要性と理論的な美しさから，急速に発展した分野である．\nNetworkXでは，点をnode，枝をedgeとよんでいる．\n\nグラフの生成法\nグラフクラス Graph もしくは有向グラフクラス DiGraph から生成する．\nimport networkx as nx\n\nG = nx.Graph()     #（無向）グラフ G の生成\n\nD = nx.DiGraph()   #有向グラフ D の生成\n\nG = nx.Graph()  # （無向）グラフ G の生成\nG\n\n&lt;networkx.classes.graph.Graph&gt;\n\n\n\n\n点と枝の追加方法\n\n点の追加\n\nG.add_node(n)\nで，グラフGに点nを追加する． n は不変オブジェクトなら何でも良い．\n\n枝の追加\n\nG.add_edge(u,v)\nで，グラフGに枝(u,v)を追加する． 枝を追加すると点は自動的に追加される．\n\nG.add_node(\"Tokyo\")\nG.add_edge(1, 2)\nprint(G)\n\nGraph with 3 nodes and 1 edges\n\n\n\n\nグラフの情報\nG.nodesで点の情報，G.edgesで枝の情報を得ることができる．\n\nprint(G.nodes)\nprint(G.edges)\n\n['Tokyo', 1, 2]\n[(1, 2)]\n\n\n\n\n点，枝に属性を付与\nG.add_node(n,attr_dict)\nで，第2引数 attr_dict に任意の属性を名前付き引数として付加することもできる．以下に例を示す．\nG.add_node(1)\nG.add_node(\"Tokyo\")\nG.add_node(5, demand=500)\nG.add_node(6, product=[\"A\",\"F\",\"D\"])\nG.add_edge(u,v,attr_dict)\nで，第3引数attr_dictに任意の属性を名前付き引数として付加することもできる．以下に例を示す．\nG.add_edge(1,2)\nG.add_edge(1,3)\nG.add_edge(2,3, weight=7, capacity=15.0)\nG.add_edge(1,4, cost=1000)\n\n\n一度にたくさん追加\n\n複数の点を一度に追加\n\nG.add_nodes_from(L)\nはリストL内の各要素を点としてグラフGに追加する．引数はリストLのかわりに集合，辞書，文字列，グラフオブジェクトなども可能である．\n\n複数の枝を一度に追加\n\nG.add_edges_from(L)\nは長さ2のタプル(u,v)を要素としたリストL内の要素を枝(u,v)として追加する． 以下に例を示す．\nG.add_edges_from([(1,2),(1,3),(2,3),(1,4)]\nG.add_weighted_edges_from(L)は長さ3のタプル(u,v,w)を要素としたリストL内の要素を， 枝(u,v)ならびに重みを表す属性wとして追加する．重みの属性名の既定値はweightである． 以下に例を示す．\nG.add_weighted_edges_from([(\"s\",1,5),(\"s\",2,8),(2,1,2),(1,\"t\",8),(2,3,5),(3,\"t\",6)])"
  },
  {
    "objectID": "11networkx.html#グラフの生成と描画",
    "href": "11networkx.html#グラフの生成と描画",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "グラフの生成と描画",
    "text": "グラフの生成と描画\n3点からなる完全グラフ(complete graph: すべての点の間に枝がある無向グラフ）を生成して，描画する．\n以下の3通りの方法を示す．\n\n一番簡単で単純な方法（これが基本）\nforループを使う方法（点の数が増えてきたらこれを使う）\n関数を使って一発\n\nG=nx.complete_graph(3)\n描画はdraw関数を用いる．引数はグラフのインスタンスである．\nJupyter環境で %matplotlib inline と宣言している場合にはmatplotlibの描画関数 plt.show() は省略できる．\n\nG = nx.Graph()  # グラフのインスタンスを生成\nG.add_edge(1, 2)  # 枝を追加\nG.add_edge(1, 3)  # 枝を追加\nG.add_edge(2, 3)  # 枝を追加\nnx.draw(G)  # グラフを描画\nprint(G.nodes)\nprint(G.edges)\n\n[1, 2, 3]\n[(1, 2), (1, 3), (2, 3)]\n\n\n\n\n\n\nG = nx.Graph()  # グラフのインスタンスを生成\nfor i in range(3):\n    for j in range(3):\n        if i &lt; j:  # 無向グラフなので，iより大きいjの場合だけ枝を生成\n            G.add_edge(i, j)\n# nx.draw(G)\n\n\nG = nx.complete_graph(3)  # 完全グラフを生成するcomplete_graph関数を利用\n# nx.draw(G)\n\n\n問題（グラフの生成と描画）\n\n5点の完全グラフを生成し描画せよ． ここで完全グラフとは，すべての点間に枝がある無向グラフである．\n\\(3 \\times 3\\) の格子グラフを生成し，描画せよ． ここで格子グラフ(grid graph)とは，2次元平面上に 座標 \\((i,j) (i=1,2,\\ldots,n; j=1,2,\\ldots,m)\\) をもつように \\(n m\\)個の点を配置し， 座標 \\((i,j)\\) の各点に対して，右の点 \\((i+1,j)\\) もしくは上の点 \\((i,j+1)\\) が存在するなら枝をはることによって得られるグラフであり， grid_2d_graph(n,m)関数で生成できる．\n\n\n# export\n# G=nx.complete_graph(5)\n# nx.draw(G)\n\n\n# export\n# Grid = nx.grid_2d_graph(3,3)\n# nx.draw(Grid)"
  },
  {
    "objectID": "11networkx.html#点枝の情報",
    "href": "11networkx.html#点枝の情報",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "点・枝の情報",
    "text": "点・枝の情報\nここでは，点と枝に関する情報にアクセスする方法について述べる．\n\nG.nodesメソッドはグラフGに含まれる点の集合（実際はViewと呼ばれるオブジェクト）を返す．\nG.nodes[n]は点nの属性の情報を辞書として返す．\nfor n in GはグラフGの点に対する反復を行う．\n\n\nD = nx.DiGraph()\nD.add_node(\"Customer\", demand=500)\nD.add_node(\"Plant\", product=[\"A\", \"F\", \"D\"])\nD.add_edge(\"Plant\", \"Customer\")\n\nprint(D.nodes)\nprint(D.nodes[\"Customer\"], D.nodes[\"Plant\"])\nfor n in D:\n    print(n)\n\n['Customer', 'Plant']\n{'demand': 500} {'product': ['A', 'F', 'D']}\nCustomer\nPlant\n\n\n\nG.edgesメソッドはグラフGに含まれるすべての枝の集合（実際はViewと呼ばれるオブジェクト）を返す．\nfor e in G.edges() でグラフGの枝に対する反復を行うことができる．\nG.neighbors(u)は点uに隣接する（有向グラフの場合には後続する）点のイテレータを返す．\n\n以下の例では，点 \\(1\\) に隣接する点は \\(2,3,4\\) であり，点 \\(2\\) に隣接する点は \\(1,3\\)， 点\\(3\\)に隣接する点は\\(1,2\\)， 点 \\(4\\) に隣接するのは点 \\(1\\) だけである．\n\nG = nx.Graph()\nG.add_edges_from([(1, 2), (1, 3), (2, 3), (1, 4)])\nfor n in G.neighbors(1):\n    print(n)\nfor n in G:\n    print(n, list(G.neighbors(n)))\n\n2\n3\n4\n1 [2, 3, 4]\n2 [1, 3]\n3 [1, 2]\n4 [1]\n\n\n\nG[u]は点uに隣接する点vをキーとし，枝(u,v)に付加された情報を値とした辞書を返す．\n\n以下の例では，点1に隣接する点は2,3,4であり，接続する枝の重みはそれぞれ100,200,50である．\n\nG[u][v]は枝(u,v)に付加された属性の情報を辞書として返す．\n\nつまり，枝(1,2)の重みは100である．\n\nG = nx.Graph()\nG.add_weighted_edges_from([(1, 2, 100), (1, 3, 200), (2, 3, 60), (1, 4, 50)])\nprint(G[1])\nprint(G[1][2])\n\n{2: {'weight': 100}, 3: {'weight': 200}, 4: {'weight': 50}}\n{'weight': 100}\n\n\n\nD.successors(u)は有向グラフDに対する点uの後続点のイテレータを返す．\nD.predecessors(u)は有向グラフDに対する点uの先行点のイテレータを返す．\n\n\nfor n in D.successors(\"Plant\"):\n    print(n)\n\nCustomer\n\n\n\n問題\n\\(3 \\times 3\\) の格子グラフを生成し，枝の重みをランダムに設定せよ．\n\n# export\n# import random\n# Grid = nx.grid_2d_graph(3,3)\n# for (u,v) in Grid.edges():\n#     Grid[u][v][\"weight\"] = random.random()"
  },
  {
    "objectID": "11networkx.html#描画の引数とレイアウト",
    "href": "11networkx.html#描画の引数とレイアウト",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "描画の引数とレイアウト",
    "text": "描画の引数とレイアウト\n関数 draw の代表的な引数は以下の通り．\n\npos：点をキー，座標（\\(x,y\\)のタプル）を値とした辞書を与える．これによって点の座標を指定できる． 省略された場合にはバネ法（後述）によって計算された座標に描画する．\nwith_labels：点の名称の表示の有無を指定できる．既定値はFalse．\nnodelist：描画する点のリストを指定できる．\nedgelist：描画する枝のリストを指定できる．\nnode_size：描画する点の大きさを指定できる．既定値は \\(300\\)．\nnode_color：描画する点の色を指定できる．既定値は “r”（赤）．\nwidth：描画する枝の幅を指定できる．既定値は \\(1.0\\)．\nedge_color：描画する枝の色を指定できる．既定値は “k” （黒）．\n\n描画の際の点の位置posを求めるための関数として，以下のものが準備されている．\n\nspring_layout(G)：隣接する点の間に反発するバネがあると仮定して得られるレイアウト（バネのレイアウト）の座標を返す． これが既定値であるので，引数posを省略して描画した場合には，バネのレイアウトになる．\ncircular_layout(G)：点を円上に配置したレイアウト（円上レイアウト）の座標を返す．\nrandom_layout(G)：点を正方形内にランダムに配置したレイアウト（ランダム・レイアウト）の座標を返す．\nshell_layout(G,nlist)：点を同心円状に配置したレイアウト（同心円レイアウト）の座標を返す． nlistは点のリストのリストであり，各リストは同心円に含まれる点集合を表す．\nspectral_layout(G)：Laplace行列（次数対角行列から隣接行列を減じたもの）の固有値を用いたレイアウト（スペクトル・レイアウト）の座標を返す．\nkamada_kawai_layout(G)：Kamada-Kawaiによる枝の重み（距離）を考慮したレイアウトの座標を返す．\n\n例として円上レイアウトでグラフを描画する．\n\nG = nx.Graph()\nG.add_edges_from([(1, 2), (1, 3), (2, 3), (1, 4), (1, 5)])\npos = nx.circular_layout(G)\nnx.draw(G, pos=pos, edge_color=\"r\", node_color=\"y\", with_labels=True)\n\n\n\n\n\n# hide\n# net.barnes_hut()\n# net.force_atlas_2based()\n# net.hrepulsion()\n# net.repulsion()\n\n\n# hide\n# net= Network(notebook=True)\n# net.from_nx(G)\n# net.show(\"graph.html\")\n\n\n問題\n以下のグラフ生成から好きなものを3つ生成し，描画せよ．\nG = nx.cycle_graph(6)\nG = nx.balanced_tree(2, 2)\nG = nx.complete_graph(5)\nG = nx.complete_bipartite_graph(3, 3)\nG = nx.grid_2d_graph(3, 3)\nG = nx.hypercube_graph(4)\nG = nx.chvatal_graph()\nG = nx.cubical_graph()\nG = nx.octahedral_graph()\nG = nx.dodecahedral_graph()\nG = nx.icosahedral_graph()\nG = nx.petersen_graph()\nG = nx.truncated_cube_graph()\nG = nx.truncated_tetrahedron_graph()\nG = nx.tutte_graph()\nG = nx.fast_gnp_random_graph(30, 0.1)\nG = nx.random_geometric_graph(10, 0.2)\nG = nx.bipartite.random_graph(10, 30, 0.3)\nG = nx.bipartite.gnmk_random_graph(10, 30, 50)\nまた， 各グラフを、円上レイアウト， バネのレイアウト， スペクトル・レイアウトのいずれかを1つずつ選び、描画せよ．\n\n# export\n# G=nx.cycle_graph(6)\n# pos=nx.spring_layout(G)\n# nx.draw(G,pos=pos,node_size=1000,with_labels=True,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.balanced_tree(2,2)\n# pos=nx.spring_layout(G)\n# nx.draw(G,pos=pos,node_size=1000,with_labels=True,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.complete_graph(5)\n# pos=nx.circular_layout(G)\n# nx.draw(G,pos=pos,node_size=1000,with_labels=True,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.hypercube_graph(4)\n# pos=nx.spring_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.chvatal_graph()\n# pos=nx.spring_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.cubical_graph()\n# pos=nx.spring_layout(G)\n# nx.draw(G,pos=pos,node_size=1000,with_labels=True,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.octahedral_graph()\n# #pos=nx.spring_layout(G)\n# pos=nx.spectral_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.dodecahedral_graph()\n# pos=nx.spring_layout(G)\n# #pos=nx.spectral_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.icosahedral_graph()\n# #pos=nx.spring_layout(G)\n# pos=nx.spectral_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.petersen_graph()\n# pos=nx.spring_layout(G)\n# #pos=nx.spectral_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.truncated_cube_graph()\n# #pos=nx.spring_layout(G)\n# pos=nx.spectral_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.truncated_tetrahedron_graph()\n# pos=nx.spring_layout(G)\n# ###pos=nx.spectral_layout(G)\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.tutte_graph()\n# #pos=nx.spring_layout(G)\n# pos=nx.spectral_layout(G)\n# #pos=nx.shell_layout(G,[nodes[5:10],nodes[0:5]])\n# nx.draw(G,pos=pos,node_size=100,with_labels=False,node_color=\"w\",edge_color=\"g\",width=5)\n\n\n# export\n# G=nx.fast_gnp_random_graph(30,0.1)\n# #G=nx.gnm_random_graph(10,18)\n# #G=nx.random_regular_graph(3,10)\n# nx.draw(G)\n\n\n# export\n# G=nx.random_geometric_graph(100,0.2)\n# pos=nx.get_node_attributes(G,'pos')\n# nx.draw(G,pos=pos)"
  },
  {
    "objectID": "11networkx.html#グラフに対する基本操作",
    "href": "11networkx.html#グラフに対する基本操作",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "グラフに対する基本操作",
    "text": "グラフに対する基本操作\nここではグラフに対する基本的な操作を紹介する．\n\ncomplement(G): 補グラフ(complement)を返す． ここで補グラフ \\(G=(V,E)\\) のとは，点集合 \\(V\\) をもち，\\((u,v) \\not\\in E\\) のときに \\(u,v\\) 間に枝をはったグラフである．\nreverse(G): 有向グラフの枝を逆にしたものを返す．\n\n\nG = nx.Graph()\nG.add_edges_from([(0, 1), (0, 2), (0, 3)])\npos = nx.spring_layout(G)\nnx.draw(G, pos=pos, edge_color=\"y\", with_labels=True)\n\nC = nx.complement(G)\nprint(C.edges())\nnx.draw(C, pos=pos, edge_color=\"r\", with_labels=True)\n\n[(1, 2), (1, 3), (2, 3)]\n\n\n\n\n\n\ncompose(G, H): GとHの和グラフ(union graph)を返す．ただしGとHに共通部分があってもよい． ここで\\(G=(V_1,E_1)\\) と \\(H=(V_2,E_2)\\) の和グラフとは， 点集合 \\(V_1 \\cup V_2\\) と枝集合 \\(E_1 \\cup E_2\\) をもつグラフである．\nunion(G, H): グラフGとHの和グラフを返す． ただしGとHに共通部分があってはいけない（もし，共通部分があった場合には例外を返す）．\nintersection(G, H): 同じ点集合をもつグラフGとHに対して，両方に含まれている枝から成るグラフ（交差グラフ）を返す．\ndifference(G, H): 同じ点集合をもつグラフGとHに対して，Gには含まれているがHには含まれていない枝から成るグラフ（差グラフ）を返す．\nsymmetric_difference(G, H): 同じ点集合をもつグラフGとHに対して，GまたはHに含まれており，かつ両者に含まれていない枝から成るグラフ（対称差グラフ）を返す．\n\n\nG = nx.Graph()\nG.add_edges_from([(0, 1), (0, 2), (0, 3)])\npos = nx.spring_layout(G)\nnx.draw(G, pos=pos, edge_color=\"y\", with_labels=True)\n\n\n\n\n\nH = nx.cycle_graph(4)\nnx.draw(H, pos=pos, edge_color=\"r\", with_labels=True)\n\n\n\n\n\n# C = nx.compose(G,H)\n# C = nx.union(G,H)\n# C = nx.intersection(G,H)\n# C = nx.difference(G,H)\n# C = nx.difference(H,G)\nC = nx.symmetric_difference(G, H)\nprint(C.edges())\nnx.draw(C, pos=pos, edge_color=\"g\", with_labels=True)\n\n[(0, 2), (1, 2), (2, 3)]\n\n\n\n\n\n\ncartesian_product(G, H): グラフGとHに対する直積(Cartesian product)グラフを返す． ここで，グラフ \\(G=(V_1,E_1)\\) と \\(H=(V_2,E_2)\\) の直積グラフとは， 点集合 \\(V_1 \\times V_2 = \\{ (v_1,v_2) | v_1 \\in V_1, v_2 \\in V_2 \\}\\) （点集合 \\(V_1,V_2\\) の直積）と， 以下を満たす枝集合から構成されるグラフである．\n\n直積グラフの枝 \\(((u,x),(v,y))\\) が存在 \\(\\Leftrightarrow\\) 「\\(x=y\\) かつ \\((u,v) \\in E_1\\)」もしくは「\\(u=v\\) かつ \\((x,y) \\in E_2\\)」\n\nlexicographic_product(G, H): グラフGとHに対する辞書的積(lexicographic product)グラフを返す． ここで，グラフ \\(G=(V_1,E_1)\\) と \\(H=(V_2,E_2)\\) の辞書的積グラフとは， 点集合 \\(V_1 \\times V_2\\) と， 以下を満たす枝集合から構成されるグラフである．\n\n辞書的積グラフの枝 \\(((u,x),(v,y))\\) が存在 \\(\\Leftrightarrow\\) 「\\((u,v) \\in E_1\\)」もしくは「\\(u=v\\) かつ \\((x,y) \\in E_2\\)」\n\ntensor_product(G, H): グラフGとHに対するテンソル積(tensor product)グラフを返す． ここで，グラフ \\(G=(V_1,E_1)\\) と \\(H=(V_2,E_2)\\) のテンソル積グラフとは， 点集合 \\(V_1 \\times V_2\\) と， 以下を満たす枝集合から構成されるグラフである．\n\nテンソル積グラフの枝 \\(((u,x),(v,y))\\) が存在 \\(\\Leftrightarrow\\) 「\\((u,v) \\in E_1\\) かつ \\((x,y) \\in E_2\\)」\n\nstrong_product(G, H): グラフGとHに対する強積(strong product)グラフを返す． ここで，グラフ \\(G=(V_1,E_1)\\) と \\(H=(V_2,E_2)\\) の強積グラフとは， 点集合 \\(V_1 \\times V_2\\) と， 以下を満たす枝集合から構成されるグラフである．\n\n強積グラフの枝 \\(((u,x),(v,y))\\) が存在 \\(\\Leftrightarrow\\) 「\\(((u,x),(v,y))\\) が直積グラフの枝もしくはテンソル積グラフの枝」\n以下の例では，グラフ \\(G\\) が商品 \\(0,1,2\\) の関連， グラフ \\(H\\) が店舗 \\(A,B,C\\) 間の競合を表すものとする．\n直積グラフの点は，各店舗で売られている各商品を表しており， 同じ店で販売されている関連する商品，もしくは競合する店で販売されている同じ商品に 対して枝がはられていると解釈できる．\n\nG = nx.Graph()\nH = nx.Graph()\nG.add_edges_from([(0, 1), (0, 2)])\nH.add_edges_from([(\"A\", \"B\"), (\"A\", \"C\")])\nProduct = nx.cartesian_product(G, H)\nnx.draw(Product, with_labels=True, node_size=1000, node_color=\"y\")\n\n\n\n\n\n問題\n上の例題に対して辞書的積， テンソル積， 強積を計算し，描画せよ． それぞれ，どのような意味を持つか考察せよ．\n\n# export\n# G=nx.Graph()\n# G.add_edges_from([(0,1),(1,2)])\n# H=nx.Graph()\n# H.add_edges_from([('A','B'),('A','C')])\n# #Product = nx.cartesian_product(G,H)\n# #Product = nx.lexicographic_product(G,H)\n# #Product = nx.tensor_product(G,H)\n# Product = nx.strong_product(G,H)\n\n# pos={}\n# for i in range(3):\n#     for j,item in enumerate([\"B\",\"A\",\"C\"]):\n#         pos[(i,item)]=(i,j)\n\n# #nx.draw(Product,with_labels=True)\n# nx.draw(Product,with_labels=True,pos=pos,\n#         node_color=\"w\",node_size=1500,edge_color=\"g\",width=3)\n\n#hide ### 問題　（難）\n3つの工場 \\(0,1,2\\) で自動車を生産することを考える． 工場 \\(0,1\\) は部品工場であり，そこでは部品 \\(P1, P2, P3\\) を製造している． 工場 \\(2\\) は組み立て工場であり，そこでは部品を組み立てて完成品 \\(P4, P5\\) を製造している．\n部品と完成品の関係は，部品展開表とよばれるグラフによって与えられており， \\(P4\\) を製造するためには，部品 \\(P1, P2\\) が1つずつ必要であり， \\(P5\\) を製造するためには，部品 \\(P2, P3\\) が1つずつ必要であるものとする． 工場 \\(0\\) から工場 \\(2\\) への輸送と，工場 \\(1\\) から工場 \\(2\\) への輸送を表すグラフと，部品展開表を表すグラフの テンソル積をとることによって，各工場における製品の製造を点，可能な輸送経路を枝としたグラフを生成せよ．"
  },
  {
    "objectID": "11networkx.html#マッチングとeuler閉路",
    "href": "11networkx.html#マッチングとeuler閉路",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "マッチングとEuler閉路",
    "text": "マッチングとEuler閉路\n以下のサイト（もしく本）を参照されたい．\n\nマッチング： Pythonによる実務で役立つ最適化問題100+ (2) ―割当・施設配置・在庫最適化・巡回セールスマン― 13章\n\nhttps://scmopt.github.io/opt100/30matching.html\n\nEuler閉路： Pythonによる実務で役立つ最適化問題100+ (3) ―配送計画・パッキング・スケジューリング― 24章\n\nhttps://scmopt.github.io/opt100/64euler.html\n\n問題（マッチングとEuler閉路）\n以下のグラフは奇数の次数をもつ点があるので，Euler閉路（すべての枝をちょうど1回通過する閉路：すべての点の次数が偶数であるのが，Euler閉路をもつための必要十分条件である）をもたない． 奇数の次数をもつ点集合に対して，点間の最短距離を計算し，最小距離のマッチング（点の次数が1以下の部分グラフ)を求めよ． マッチングに含まれる枝をもとのグラフに加えるとEuler閉路をもつようになる（なぜか？理由を考えよ）． マッチングを加えたグラフに対するEuler閉路を求めよ．\n\nG = nx.grid_2d_graph(3, 4)\nnx.draw(G)\n\n\n\n\n\n# export\n# G = nx.grid_2d_graph(3,4)\n# NewG = nx.eulerize(G)\n# for (u,v) in nx.eulerian_circuit(NewG):\n#     print(u,v)"
  },
  {
    "objectID": "11networkx.html#最小木",
    "href": "11networkx.html#最小木",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "最小木",
    "text": "最小木\n最小木問題については，以下のサイト（もしく本）を参照されたい．\n\nPythonによる実務で役立つ最適化問題100+ (1) ―グラフ理論と組合せ最適化への招待― 第４章\n\nhttps://scmopt.github.io/opt100/05mst.html\n\n問題（最小木）\n\n\\(5 \\times 5\\) の格子グラフ（枝の重みはすべて \\(1\\)）の最小木（枝の重みの合計が最小の閉路を含まない連結グラフ）を求めよ．\n\\(5 \\times 5\\) の格子グラフの枝の重みをランダムに設定した上で，最小木を求め，最小木に含まれる枝を異なる色で描画せよ．\n枝上に距離が定義された無向グラフ \\(G=(V,E)\\) を考える． このグラフの点集合 \\(V\\) を \\(k\\)個に分割したとき，分割に含まれる点同士の最短距離を最大化するようにしたい．これは最小木に含まれる枝を距離の大きい順に \\(k-1\\) 本除くことによって得ることができる． ランダムに距離を設定した \\(5 \\times 5\\) の格子グラフに対して \\(k=5\\) の分割を求めよ．\n\n\n# export\n# G = nx.grid_2d_graph(5,5)\n# print(list(nx.minimum_spanning_edges(G)))\n\n# m, n = 5, 5\n# lb, ub = 1, 20\n# G = nx.grid_2d_graph(m, n)\n# for (i,j) in G.edges():\n#     G[i][j][\"weight\"] = random.randint(lb, ub)\n# pos ={(i,j):(i,j) for (i,j) in G.nodes() }\n# edges = list(nx.minimum_spanning_edges(G))\n# plt.figure()\n# nx.draw(G, pos=pos, node_size=100)\n# edge_labels ={}\n# for (i,j) in G.edges():\n#     edge_labels[i,j] = f\"{ G[i][j]['weight'] }\"\n# nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)\n# nx.draw(G, pos=pos, width=5, edgelist= edges, edge_color =\"orange\")\n# plt.show()\n\n# weight =[ ]\n# for (i,j,w) in edges:\n#     weight.append( (w[\"weight\"], i,j) )\n# weight.sort(reverse=True)\n# G1 = nx.Graph()\n# for (w,i,j) in weight[5:]:\n#     G1.add_edge(i,j)\n# nx.draw(G, pos=pos, node_size=100)\n# nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)\n# nx.draw(G1, pos=pos, node_size=100, width=10, edge_color=\"orange\")"
  },
  {
    "objectID": "11networkx.html#最短路",
    "href": "11networkx.html#最短路",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "最短路",
    "text": "最短路\n最短路問題の詳細については，以下のサイト（もしくは本）を参照されたい．\n\nPythonによる実務で役立つ最適化問題100+ (1) ―グラフ理論と組合せ最適化への招待― 第2章\n\nhttps://scmopt.github.io/opt100/03sp.html\n簡単な通勤（通学）の例を示そう．\n八千代緑が丘から越中島までの電車による経路を求めたい． 枝ごとの移動時間と移動費用を入力して，最短時間パスと最小費用パスを求めよ． 乗り換えの待ち時間は無視してよいが，徒歩の時間は考慮せよ．\n\nG = nx.Graph()\nG.add_edge(\"八千代緑が丘\", \"西船橋\", weight=15, cost=490)\nG.add_edge(\"西船橋\", \"門前仲町\", weight=20, cost=230)\nG.add_edge(\"門前仲町\", \"越中島\", weight=10, cost=0)\nG.add_edge(\"西船橋\", \"越中島\", weight=24, cost=380)\n\npath = nx.dijkstra_path(G, \"八千代緑が丘\", \"越中島\")\nprint(\"最短時間パス\", path)\n\npath = nx.dijkstra_path(G, \"八千代緑が丘\", \"越中島\", weight=\"cost\")\nprint(\"最小費用パス\", path)\n\n最短時間パス ['八千代緑が丘', '西船橋', '越中島']\n最小費用パス ['八千代緑が丘', '西船橋', '門前仲町', '越中島']\n\n\n\n問題\n自宅から大学（もしくは職場）までの最短時間と最小費用のパスを求めるためのネットワークを作成し，最短時間パスと最小費用パスを求めよ． ただし大学（職場）から徒歩圏の場合は，親戚の家からのパスを求めよ．\n\n# export\n# G = nx.Graph()\n# G.add_edge(\"八千代緑が丘\", \"西船橋\", weight=15, cost=490)\n# G.add_edge(\"西船橋\", \"門前仲町\", weight=20, cost=230)\n# G.add_edge(\"門前仲町\", \"越中島\", weight=10, cost=0)\n# G.add_edge(\"西船橋\", \"越中島\", weight=24, cost=380)\n\n# path = nx.dijkstra_path(G, \"八千代緑が丘\", \"越中島\")\n# print(\"最短時間パス\", path)\n\n# path = nx.dijkstra_path(G, \"八千代緑が丘\", \"越中島\", weight=\"cost\")\n# print(\"最小費用パス\", path)\n\n\n\n問題\n\\(3\\times 3\\) の格子グラフを生成するプログラムを作成し，枝の重みをランダムに設定した上で， 左上の点から右下の点までの最短路を求め，最短路を異なる色で描画せよ．\n\n# export\n# m, n = 3, 3\n# lb, ub = 1, 300\n# G = nx.grid_2d_graph(m, n)\n# for (i,j) in G.edges():\n#     G[i][j][\"weight\"] = random.randint(lb, ub)\n# path = nx.dijkstra_path(G, source=(0,0), target=(2,2))\n# edges =[]\n# for i in range(len(path)-1):\n#     edges.append( (path[i],path[i+1]) )\n# plt.figure()\n# nx.draw(G, pos=pos, node_size=100)\n# edge_labels ={}\n# for (i,j) in G.edges():\n#     edge_labels[i,j] = f\"{ G[i][j]['weight'] }\"\n# nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)\n# nx.draw(G, pos=pos, width=5, edgelist= edges, edge_color =\"orange\")\n# plt.show()\n\n\n\n問題 (PERT)\nあなたは航空機会社のコンサルタントだ．あなたの仕事は，着陸した航空機をなるべく早く離陸させるためのスケジュールをたてることだ． 航空機は，再び離陸する前に幾つかの作業をこなさなければならない． まず，乗客と荷物を降ろし，次に機内の掃除をし，最後に新しい乗客を搭乗させ，新しい荷物を積み込む． 当然のことであるが， 乗客を降ろす前に掃除はできず，掃除をした後でないと新しい乗客を入れることはできず， 荷物をすべて降ろし終わった後でないと，新しい荷物は積み込むことができない． また，この航空機会社では， 乗客用のゲートの都合で，荷物を降ろし終わった後でないと新しい乗客を搭乗させることができないのだ．\n作業時間は，乗客降ろし \\(13\\) 分，荷物降ろし \\(25\\) 分，機内清掃 \\(15\\) 分，新しい乗客の搭乗 \\(27\\) 分， 新しい荷物積み込み \\(22\\) 分とする． さて，最短で何分で離陸できるだろうか？ （ヒント： 最短離陸時間を出すにはグラフの最長路を求める必要がある． 枝の重みを負にして最短路を解けば良い． 枝の重みが負なので， Dijkstra法でなくBellman-Ford法を使う）\n\n# export\n# duration = {1: 13, 2: 25, 3: 15, 4: 27, 5: 22}\n# G = nx.DiGraph()\n# G.add_weighted_edges_from([(0,1,-13),(1,2,-15),(2,3,-27),(0,4,-25),(4,2,0),(4,3,-22)])\n# pred, distance = nx.bellman_ford_predecessor_and_distance(G,source=0)\n# distance\n\n#hide ### 問題 （難）\nDijkstra法を自分で実装せよ．\nNetworkXパッケージのDijkstra法と自分で作成したDijkstra法を大規模な格子グラフで実験し，計算時間を比較せよ．"
  },
  {
    "objectID": "11networkx.html#フロー問題",
    "href": "11networkx.html#フロー問題",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "フロー問題",
    "text": "フロー問題\n以下のサイト（もしく本）を参照されたい．\n\n最大流問題： Pythonによる実務で役立つ最適化問題100+ (1) ―グラフ理論と組合せ最適化への招待― 第8章\n\nhttps://scmopt.github.io/opt100/10maxflow.html\n\n割当問題： Pythonによる実務で役立つ最適化問題100+ (2) ―割当・施設配置・在庫最適化・巡回セールスマン― 14章\n\nhttps://scmopt.github.io/opt100/33ap.html\n\n最小費用流問題： Pythonによる実務で役立つ最適化問題100+ (1) ―グラフ理論と組合せ最適化への招待― 第7章\n\nhttps://scmopt.github.io/opt100/09mcf.html\n\n例題：最大流問題\nランダムな2部グラフを作成し，最大マッチングを最大流問題(maximum flow problem)を解くことによって求めよ．\nここで最大流問題とは，以下に定義される問題である．\n\\(n\\) 個の点から構成される点集合 \\(V\\) および \\(m\\) 本の枝から構成される枝集合 \\(E\\)， 有向グラフ \\(G=(V,E)\\)， 枝上に定義される非負の容量関数 \\(C: E \\rightarrow R_+\\)， 始点 \\(s \\in V\\) および終点 \\(t \\in V\\) が与えられたとき， 始点 \\(s\\) から終点 \\(t\\) までの「フロー」で， その量が最大になるものを求めよ．\nここでフロー(flow)とは枝上に定義された実数値関数 \\(x: E \\rightarrow R\\) で， 以下の性質を満たすものを指す．\n\nフロー整合条件: \\[\n\\sum_{v: vu \\in E} x_{vu} - \\sum_{v: uv \\in E} x_{uv} =0  \\ \\ \\ \\forall u \\in V \\setminus \\{s,t\\}\n\\]\n容量制約と非負制約: \\[\n0 \\leq x_{e} \\leq C_{e} \\ \\ \\  \\forall e \\in E\n\\]\n\nダミーの始点と終点を追加することによって，最大流問題に帰着する．\n\n# ランダムな２部グラフの生成例\nBG = nx.bipartite.random_graph(10, 5, 0.5)\ntop = nx.bipartite.sets(BG)[0]\nother = nx.bipartite.sets(BG)[1]\npos = nx.bipartite_layout(BG, top)\nnx.draw(BG, pos=pos)\n\nAmbiguousSolution: Disconnected graph: Ambiguous solution for bipartite sets.\n\n\n\nG = nx.DiGraph()\nG.add_nodes_from([\"source\",\"sink\"])\nG.add_edges_from([ (\"source\",j) for j in top])\nG.add_edges_from([ (i, \"sink\") for i in other])\nG.add_edges_from(BG.edges())\nfor (i,j) in G.edges():\n    G[i][j][\"capacity\"] = 1\nvalue, flow = nx.maximum_flow(G, _s=\"source\", _t=\"sink\")\nprint(\"value=\",value)\nprint(flow)\n\nNameError: name 'top' is not defined\n\n\n\n\n例題： 割当問題\n4人の作業員 A,B,C,D を4つの仕事 \\(0,1,2,3\\) を1つずつ割り当てることを考える． 作業員が仕事を割り当てられたときにかかる費用が，以下のようになっているとき，最小費用の割当を求めよ．\n\\[\n\\begin{array}{ l    l l l l}\n       & 0 & 1 & 2    &3    \\\\ \\hline\nA      & 25 & 20 & 30 & 27  \\\\\nB      & 17 & 15 & 12 & 16  \\\\\nC      & 25 & 21 & 23 & 19  \\\\\nD      & 16 & 26 & 21 & 22  \n\\end{array}\n\\]\nこの問題は，最小費用流問題(minimum cost flow problem)に帰着できる．\n最小費用流問題は，以下のように定義される問題である．\n有向グラフ \\(G=(V,E)\\)， 枝上に定義される重み（費用）関数 \\(w: E \\rightarrow R\\)， 枝上に定義される非負の容量関数 $C: E R_+ { } $， 点上に定義される流出量関数 \\(b: V \\rightarrow R\\) が与えられたとき， 「実行可能フロー」で，費用の合計が最小になるものを求めよ． ただし，\\(\\sum_{v \\in V} b_v =0\\) を満たすものとする．\n作業員を表す点から仕事を表す点へ \\(1\\) 単位のフローを最小費用で流すことによって求解する．\n\ncost = [[25,20,30,27],\n        [17,15,12,16],\n        [25,21,23,19],\n        [16,26,21,22]]\nG = nx.DiGraph()\nn = len(cost)\nfor i in range(n):\n    G.add_node(i, demand=-1)\n    G.add_node(n+i, demand=1)\nG.add_weighted_edges_from([(i,n+j,cost[i][j]) for i in range(n) for j in range(n)])\nval, flow = nx.algorithms.flow.network_simplex(G)\nprint(val,flow)\n\n67 {0: {4: 0, 5: 1, 6: 0, 7: 0}, 4: {}, 1: {4: 0, 5: 0, 6: 1, 7: 0}, 5: {}, 2: {4: 0, 5: 0, 6: 0, 7: 1}, 6: {}, 3: {4: 1, 5: 0, 6: 0, 7: 0}, 7: {}}\n\n\n\n\n問題（輸送問題）\nあなたは，スポーツ用品販売チェインのオーナーだ． あなたは，店舗展開をしている5つの顧客 （需要地点）に対して， 3つの自社工場で生産した製品を運ぶ必要がある． 調査の結果， 工場での生産可能量（容量）， 顧客への輸送費用，ならびに各顧客における需要量は， 以下のようになっていることが分かった． さて，どのような輸送経路を選択すれば，総費用が最小になるであろうか？\n顧客の需要量，工場から顧客までの輸送費用，ならびに工場の生産容量： \\[\n\\begin{array}{c  c c c c c   c  } \\hline\n顧客 & 1  &  2  & 3  & 4 & 5 &  \\\\  \\hline\n需要量   &80 & 270 & 250 & 160 & 180 &   \\\\ \\hline\n工場  &   輸送費用  &   &    &    &  & 容量  \\\\ \\hline\n1      & 4 & 5 & 6 & 8 & 10 &  500 \\\\\n2      & 6  &4 & 3 & 5 & 8 &  500 \\\\\n3      & 9  & 7 & 4 & 3 & 4 &  500 \\\\ \\hline\n\\end{array}\n\\]\n（ヒント： 最小費用流問題においては，需要（供給は負の需要とみなす）の合計が \\(0\\) でなくてはいけない． 仮想の点を追加することによって，最小費用流問題に帰着せよ）\n\n# export\n# d = {1:80, 2:270, 3:250 , 4:160, 5:180} # demand\n# M = {1:500, 2:500, 3:500}              # capacity\n# c = {(1,1):4,    (1,2):6,    (1,3):9,  # cost\n#      (2,1):5,    (2,2):4,    (2,3):7,\n#      (3,1):6,    (3,2):3,    (3,3):4,\n#      (4,1):8,    (4,2):5,    (4,3):3,\n#      (5,1):10,   (5,2):8,    (5,3):4,\n#      }\n# G = nx.DiGraph()\n# for i in M:\n#     G.add_node(f\"plant{i}\", demand=-M[i])\n# for j in d:\n#     G.add_node(f\"customer{j}\", demand=d[j])\n# total_demand = sum(d[j] for j in d)\n# total_supply = sum(M[i] for i in M)\n# G.add_node(\"dummy\", demand = total_supply-total_demand)\n# G.add_weighted_edges_from([(f\"plant{i}\",f\"customer{j}\",c[j,i]) for i in M for j in d] )\n# G.add_weighted_edges_from([ (f\"plant{i}\",\"dummy\",0) for i in M] )\n# val, flow = nx.algorithms.flow.network_simplex(G)\n# print(val,flow)\n\n\n\n問題（多期間生産計画問題）\n1つの製品の生産をしている工場を考える． 在庫費用は1日あたり，1トンあたり1万円とする． いま7日先までの需要が分かっていて，7日分の生産量と在庫量を決定したい． 各日の需要は，以下の表のようになっている． 工場の1日の稼働時間は \\(8\\)時間，製品1トンあたりの製造時間は\\(1\\)時間としたとき， 稼働時間上限を満たした最小費用の生産・在庫量を決定せよ．\n\n\n\n日\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n需要量\n5\n7\n8\n2\n9\n1\n3\n\n\n\n\n# export\ndemand = [5, 7, 8, 2, 9, 1, 3]  # 需要量\nT = len(demand)  # 計画期間\nh = 1  # 在庫費用\ncapacity = 8  # 最大稼働時間(=最大生産量；製造時間=1だから)\nG = nx.DiGraph()\nfor i in range(T):\n    G.add_node(f\"period{i}\", demand=demand[i])\nG.add_node(\"dummy\", demand=-sum(demand))\nG.add_weighted_edges_from([(\"dummy\", f\"period{i}\", 0) for i in range(T)])\nG.add_weighted_edges_from([(f\"period{i}\", f\"period{i+1}\", h) for i in range(T - 1)])\nfor i in range(T):\n    G[\"dummy\"][f\"period{i}\"][\"capacity\"] = capacity\nfor i in range(T - 1):\n    G[f\"period{i}\"][f\"period{i+1}\"][\"capacity\"] = sum(demand)\nval, flow = nx.algorithms.flow.network_simplex(G)\nprint(val, flow)\n\n1 {'period0': {'period1': 0}, 'period1': {'period2': 0}, 'period2': {'period3': 0}, 'period3': {'period4': 1}, 'period4': {'period5': 0}, 'period5': {'period6': 0}, 'period6': {}, 'dummy': {'period0': 5, 'period1': 7, 'period2': 8, 'period3': 3, 'period4': 8, 'period5': 1, 'period6': 3}}\n\n\n\n\n問題（下限制約）\n以下の図に示す最小費用流問題を考える．枝上の数値は「単位フローあたりの費用(容量)」であり，点 \\(0\\) から点 \\(4\\) に \\(10\\) 単位のものを最小費用で流したい．\nただし，点2から点3へ向かう枝のフロー量が \\(4\\)以上でなければならないものとする．\nこのフロー量の下限制約を取り除くことを考える． 下限 \\(4\\) を超過した量を新たなフロー量として \\(x'_{23}\\) と記す．元のフロー量 \\(x_{23}\\) とは \\(x_{23}= 4+x'_{23}\\) の関係がある． 変数 \\(x_{23}\\) を \\(x'_{23}\\) に置き換えることによって， 点2におけるフロー整合条件から点2の需要量は \\(4\\) 増え， 点3におけるフロー整合条件から点3の需要量は \\(4\\)減る． また，枝\\((2,3)\\) の容量（フロー量上限）は，\\(1(=5-4)\\)に変更される．\nこの観察を用いて，下限制約付きの最小費用流を求めよ．\n\n#export\nG = nx.DiGraph()\nG.add_node(0, demand=-10)\nG.add_node(4, demand=10)\ncapacity = {(0, 1): 5, (0, 2): 8, (1, 4): 8, (2, 1): 2, (2, 3): 5, (3, 4): 6}\nG.add_weighted_edges_from(\n    [(0, 1, 10), (0, 2, 5), (1, 4, 1), (2, 1, 3), (2, 3, 1), (3, 4, 6)]\n)\nfor (i, j) in G.edges():\n    G[i][j][\"capacity\"] = capacity[i, j]\npos = {0: (0, 1), 1: (1, 2), 2: (1, 0), 3: (2, 0), 4: (2, 2)}\nedge_labels = {}\nfor (i, j) in G.edges():\n    edge_labels[i, j] = f\"{ G[i][j]['weight'] }({ G[i][j]['capacity']})\"\nplt.figure()\nnx.draw(G, pos=pos, with_labels=True, node_size=1000)\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\nplt.show()\n\n\n\n\n\n\n問題（ナプキンのクリーニング） （難）\nあなたはホテルの宴会係だ．あなたは1週間に使用するナプキンを手配する必要がある． 各日の綺麗なナプキンの需要量は平日は \\(100\\)枚，土曜日と日曜日は \\(125\\)枚だ． 新しいナプキンを購入するには \\(100\\)円かかる． 使用したナプキンはクリーニング店で洗濯して綺麗なナプキンにすることができるが， 早いクリーニング店だと1日で1枚あたり \\(30\\)円かかり， 遅いクリーニング店だと2日で1枚あたり \\(10\\)円かかる． 月曜の朝のナプキンの在庫が \\(0\\) としたとき，需要を満たす最適なナプキンの購入ならびにクリーニング計画をたてよ．\nヒント： この問題は下限付きの最小費用流問題に帰着できる．\nさらに、上のナプキンのクリーニング問題において， 日曜末の在庫を月曜の朝に使うことができると仮定したときの最適なナプキンのクリーニング計画をたてよ．\n\n# export\n# G=nx.DiGraph()\n\n# Demand=[100,100,100,100,100,125,125]\n# U=sum(Demand)\n# T=len(Demand)\n# G.add_edge(\"t\",\"s\")\n# for t in range(T):\n#     G.add_node((\"clean\",t),demand=Demand[t])\n#     G.add_node((\"dirty\",t),demand=-Demand[t])\n#     G.add_edge(\"s\",(\"clean\",t),weight=10)         #buy new napkins\n#     G.add_edge((\"clean\",t),(\"dirty\",t),capacity=U-Demand[t]) #use napkins\n#     G.add_edge((\"dirty\",t),\"t\")         #discard napkins\n\n# for t in range(T-1):\n#     G.add_edge((\"clean\",t),(\"clean\",t+1)) #invenory of clean napkins\n#     G.add_edge((\"dirty\",t),(\"clean\",t+1),weight=3) #clean using fast laundry\n\n# for t in range(T-2):\n#     G.add_edge((\"dirty\",t),(\"clean\",t+2),weight=1) #clean using slow laundry\n\n# cost,flow = nx.network_simplex(G)\n\n# print(\"cost=\",cost)\n# for i in G.nodes():\n#     for j in flow[i]:\n#         if flow[i][j]&gt;0:\n#             print(i,j,flow[i][j])\n\n# G=nx.DiGraph()\n\n# Demand=[100,100,100,100,100,125,125]\n# U=sum(Demand)\n# T=len(Demand)\n# G.add_edge(\"t\",\"s\")\n# for t in range(T):\n#     G.add_node((\"clean\",t),demand=Demand[t])\n#     G.add_node((\"dirty\",t),demand=-Demand[t])\n#     G.add_edge(\"s\",(\"clean\",t),weight=10)         #buy new napkins\n#     G.add_edge((\"clean\",t),(\"dirty\",t),capacity=U-Demand[t]) #use napkins\n#     G.add_edge((\"dirty\",t),\"t\")         #discard napkins\n\n# for t in range(T):\n#     G.add_edge((\"clean\",t),(\"clean\",(t+1) % T )) #invenory of clean napkins\n#     G.add_edge((\"dirty\",t),(\"clean\", (t+1)% T ),weight=3) #clean using fast laundry\n\n# for t in range(T):\n#     G.add_edge((\"dirty\",t),(\"clean\",(t+2) % T),weight=1) #clean using slow laundry\n\n# cost,flow = nx.network_simplex(G)\n\n# print(\"cost=\",cost)\n# for i in G.nodes():\n#     for j in flow[i]:\n#         if flow[i][j]&gt;0:\n#             print(i,j,flow[i][j])"
  },
  {
    "objectID": "11networkx.html#最大安定集合問題と最大クリーク問題",
    "href": "11networkx.html#最大安定集合問題と最大クリーク問題",
    "title": "グラフ・ネットワーク解析パッケージ NetworkX",
    "section": "最大安定集合問題と最大クリーク問題",
    "text": "最大安定集合問題と最大クリーク問題\n最大安定集合問題(maximum stable set problem)は，以下のように定義される問題である．\n点数 \\(n\\) の無向グラフ \\(G=(V,E)\\) が与えられたとき，点の部分集合 \\(S (\\subseteq V)\\) は， すべての \\(S\\) 内の点の間に枝がないとき安定集合（stable set）とよばれる． 最大安定集合問題とは，集合に含まれる要素数（位数）\\(|S|\\) が最大になる安定集合 \\(S\\) を求める問題である.\nこの問題のグラフの補グラフ（枝の有無を反転させたグラフ）を考えると， 以下に定義される最大クリーク問題（maximum clique problem）になる．\n無向グラフ \\(G=(V,E)\\) が与えられたとき，点の部分集合 \\(C (\\subseteq V)\\)は， \\(C\\) によって導かれた誘導部分グラフが完全グラフになるときクリーク（clique）とよばれる （完全グラフとは，すべての点の間に枝があるグラフである）． 最大クリーク問題とは，位数 \\(|C|\\) が最大になるクリーク \\(C\\) を求める問題である．\nこれらの2つの問題は（お互いに簡単な変換によって帰着されるという意味で）同値である．\n最大安定集合問題と最大クリーク問題の詳細については，以下のサイト（もしくは本）を参照されたい．\n\nPythonによる実務で役立つ最適化問題100+ (1) ―グラフ理論と組合せ最適化への招待― 第11章\n\nhttps://scmopt.github.io/opt100/18clique.html\n\n問題 （\\(8\\)-クイーン問題)\n\\(8 \\times 8\\) のチェス盤に \\(8\\)個のクイーンを置くことを考える． チェスのクイーンとは，将棋の飛車と角の両方の動きができる最強の駒である． クイーンがお互いに取り合わないように置く配置を1つ求めよ．\n将棋を知らない人のために言い換えると，\\(8 \\times 8\\) のマス目に，同じ行（列）には高々1つのクイーンを置き， 左下（右下）斜めにも高々1つのクイーンを置くような配置を求める問題である （ヒント： 実は，この問題は安定集合問題の特殊形である． グラフは \\(i\\) 行，\\(j\\) 列のマス目を点とみなして，クイーンが取り合うとき点の間に枝をはれば良い）．\nちなみに斜めでクイーンが取り合うかどうかを判定するのは，\\(i-j\\) が同じ（右下の斜め）か \\(i+j\\) が同じか（左下の斜め）で判定すれば良い．"
  },
  {
    "objectID": "16fastai.html",
    "href": "16fastai.html",
    "title": "fastaiによる深層学習",
    "section": "",
    "text": "# hide_input\n\nYouTubeVideo(\"3w7896ke-To\", width=200, height=150)\n# hide_input\nYouTubeVideo(\"VKc7NGZru-Y\", width=200, height=150)\n# hide_input\nYouTubeVideo(\"iJVkwofNY9E\", width=200, height=150)"
  },
  {
    "objectID": "16fastai.html#深層学習とは",
    "href": "16fastai.html#深層学習とは",
    "title": "fastaiによる深層学習",
    "section": "深層学習とは",
    "text": "深層学習とは\n深層学習とは多くの隠れ層（後で説明する）をもつニューラルネットである． ニューラルネットとは（機械学習のところで簡単に触れたように），\n\n訓練データを複数の階層から構成されるモデルに入力，\n上層からの重み付き和（線形変換）に活性化関数を適用して，下層に流す，\n最下層では損出関数によって誤差を評価，\n誤差の情報から勾配を計算，\n勾配の情報を用いて，重み（パラメータ）の更新，\n\nを繰り返すだけである．これは単に，入力を出力に変換する関数とも考えられるが， できるだけ与えられたデータに適合するように近似することを目標としている点が特徴である．\n1ニューロンのニューラルネットは単なる古典的な線形回帰（もしくは分類問題の場合にはロジスティック回帰）である．\n以下で用いる用語を整理しておこう．\n\n人工知能: 機械に知能を持たせるための技術．\n機械学習：（教師ありに限定だが）入力データと出力データから，モデルのパラメータを調整する方法．\nニューラルネット：単なる関数近似器．\n深層学習：単なる多次元分散型関数近似器．\nfastai：PyTorchのラッパー"
  },
  {
    "objectID": "16fastai.html#深層学習の歴史",
    "href": "16fastai.html#深層学習の歴史",
    "title": "fastaiによる深層学習",
    "section": "深層学習の歴史",
    "text": "深層学習の歴史\nいま流行の深層学習(deep learning)はニューラルネットから生まれ，そのニューラルネットはパーセプトロンから生まれた．起源であるパーセプトロンまで遡ろう．\n1958年に，コーネル大学の心理学者であったFrank Rosenblattがパーセプトロンの概念を提案した．これは1層からなるニューラルネットであり，極めて単純な構成をもつが，当時は部屋いっぱいのパンチカード式の計算機が必要であった．\n隠れ層のない2層のニューラルネットでの出力誤差からの確率的勾配降下法は1960年にB. Widrow と M.E. Hoff, Jr. らが Widrow-Hoff 法（デルタルール）という名称で発表した． 隠れ層のある3層以上のニューラルネットは、1967年に甘利俊一が発表した．\n1969年に，MITのMarvin Minsky（人工知能の巨人として知られる）が，ニューラルネットの限界についての論文を発表した．彼の名声による影響のためか，その後ニューラルネットの研究は徐々に下火になっていく．\n2006年に，トロント大学のGeoffrey Hinton（ニューラルネットの父として知られる）は，多階層のニューラルネットでも効率よく学習できるような方法に関する論文を発表する． この手法はオートエンコーダーと呼ばれ，その後の深層学習の爆発的な研究のもとになったものである．\n2011年にマイクロソフト社は，言語認識のためにニューラルネットを使うようになる．その後も言語認識や機械翻訳は，画像認識ととともに，深層学習の応用分野として定着している．\n2012年の7月にGoogle社は猫を認識するためのニューラルネットであるGoogle Brainを開始し，8月には言語認識に用いるようになる．同年の10月には，Hintonの2人の学生が，ImageNetコンテストで断トツの成績で1位になる．これをきっかけに，深層学習が様々な応用に使われるようになる．\n2015年の12月には，マイクロソフト社のチームが，ImageNetコンテストで人間を超える結果を出し，2016年3月には，AlphaGoが碁の世界チャンピオンでLee Sedolを打ち負かす（ただしこれは深層学習というより強化学習の成果とも言える）．\n最近では，拡散モデル(diffusion model)を用いた高精度の画像の生成や， ChatGPT(Generative Pre-trained Transformer)に代表される自己アテンション(self attention)を用いた自然言語処理への応用が進み，技術の民主化が進んでいる．"
  },
  {
    "objectID": "16fastai.html#なぜ深層学習がうまくいくようになったのか",
    "href": "16fastai.html#なぜ深層学習がうまくいくようになったのか",
    "title": "fastaiによる深層学習",
    "section": "なぜ深層学習がうまくいくようになったのか？",
    "text": "なぜ深層学習がうまくいくようになったのか？\nデータ量の増大に伴い，それをうまく利用できる手法である深層学習が有効になってきている．層の数を増やしても大丈夫なようなアーキテクチャ（モデル）が開発されたことも，重要な要因である．つまり，データと新しいモデルが両輪となって，様々な分野への応用を後押ししているのである．小さなデータしかないときには，ニューラルネットは線形回帰やサポートベクトル機械(SVM)と同じ程度の性能である．しかし，データが大規模になると，ニューラルネットはSVMより高性能になり，小規模なニューラルネットより大規模なニューラルネットの方が良い性能を出すようになる．\nさらには，GPUの低価格化によって単純な計算の反復が必要な深層学習が高速に実行できるようになったことも普及を後押ししている．深層学習がうまく動くことが知られるにつれて，研究も加速している．古典的なシグモイド関数からReLU（ならびにその亜種）への移行，ドロップアウト，バッチ正規化など，実際にうまく動くアルゴリズムの開発も重要な要因である．さらに，応用に応じた様々なモデル（アーキテクチャ）が提案され，問題に応じて適切なモデルを使い分けることができるようになってきたのも，理由の1つである．\n多くの人材が深層学習の分野に参入したことも重要な要因であるように感じている．ハイパーパラメータの適正化は，最適化における実験的解析と同様に，膨大な系統的な実験と，それを解析するマンパワーが必要となる．データを公開し，開発したソフトウェアをオープンソースにして配布するといったこの分野の風土も研究を加速している．\nデータやソフトウェアを非公開にする風土をもつ他の研究分野は，深層学習をお手本にする必要があるだろう．特に，日本の企業との共同研究では，データや開発したソフトウェアは非公開にしがちである．深層学習を牽引するコミュニティーのパワーは，そういった秘密主義がないことに起因している．"
  },
  {
    "objectID": "16fastai.html#fastaiとは",
    "href": "16fastai.html#fastaiとは",
    "title": "fastaiによる深層学習",
    "section": "fastaiとは",
    "text": "fastaiとは\n深層学習のためのパッケージとしては， tensorflow (+Keras), PyTorchなどが有名であるが，ここではfastai https://www.fast.ai を用いる．\nfastaiは、最先端の深層学習を実務家が気軽に適用できるようにするためのパッケージである．\n開発者が「AIをもう一度uncoolに」を標語にしているように，専門家でなくても（Pythonを知っていれば）ある程度（というか数年前の世界新記録程度）の深層学習を使うことができる．\n特徴は以下の通り。\n\nコードが短くかける（Kerasよりも短い）．\n速い．\n最新の工夫が取り入れられている．\nPyTorchの足りない部分を補完してくれる．\n無料の（広告なしの）講義ビデオがある．\nテキストのソースも無料公開されている． https://github.com/fastai/fastbook"
  },
  {
    "objectID": "16fastai.html#fastaiのインストール",
    "href": "16fastai.html#fastaiのインストール",
    "title": "fastaiによる深層学習",
    "section": "fastaiのインストール",
    "text": "fastaiのインストール\n自分のマシンへのfastaiのインストールは本家サイトを参照されたい．\nGoogle Colab上にはすでにインストールされているので，以下の操作だけを行えば良い．\n\n上部メニューのランタイム/ランタイプの種類を変更でGPUをオンにする．\n\n割り当てられたGPUを，以下のコマンドで確認しておく．\n\n!nvidia-smi\n\nTue Aug 23 01:10:22 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   65C    P0    31W /  70W |   2084MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n\n#hide google driveにアクセスする権限を与える必要があるので，リンクを開いてauthentification codeをコピペする．\n\n#![ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n\n#hide \n\n# hide\n# from fastai.vision.all import *\n# path = untar_data(URLs.PETS)/\"images\"\n\n# def is_cat(x): return x[0].isupper()\n# dls = ImageDataLoaders.from_name_func(\n#     path, get_image_files(path), valid_pct=0.2, seed=42,\n#     label_func=is_cat, item_tfms=Resize(224))\n\n# learn = cnn_learner(dls, resnet34, metrics=error_rate)\n# #learn.fine_tune(1)\n\n\n# hide\n# import fastai.vision.widgets as widgets\n# uploader = widgets.FileUpload()\n# uploader\n\n\n# hide\n# img = PILImage.create(uploader.data[0])\n# is_cat,_,probs = learn.predict(img)\n# print(f\"Is this a cat?: {is_cat}.\")\n# print(f\"Probability it\"s a cat: {probs[1].item():.6f}\")"
  },
  {
    "objectID": "16fastai.html#mnist_sample",
    "href": "16fastai.html#mnist_sample",
    "title": "fastaiによる深層学習",
    "section": "MNIST_SAMPLE",
    "text": "MNIST_SAMPLE\n深層学習における ”Hello World” は、MNISTの手書き文字認識である。ここでは、さらに簡単なMNISTの一部（\\(３\\)と\\(７\\)だけの画像）を認識するためのニューラルネットを作成する。 これは、2値分類問題と呼ばれ、似た例をあげると，与えられた写真に猫が写っているか否か，受け取ったメイルがスパムか否か，などを判定することがあげられる。 2値分類問題は、独立変数（ニューラルネットの入力，特徴ベクトル）に対する従属変数（ターゲット）が \\(0\\)か\\(1\\)の値をとる問題であると言える．\nこの簡単な例を用いて、fastaiを用いた訓練 (training) のコツを伝授する．\nまず、fastaiで準備されているMNIST_SAMPLEのデータを読み込む．\npathはデータを展開するフォルダ（ディレクトリ）名であり、dlsはデータローダーと名付けられた画像用データローダー (ImageDataLoader)のインスタンスである。\nデータローダーには，様々なファクトリメソッド（インスタンスを生成するためのメソッド）がある．ここでは，フォルダから生成するfrom_folderメソッドを用いる．\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST_SAMPLE)\ndls = ImageDataLoaders.from_folder(path)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:01&lt;00:00]\n    \n    \n\n\ndoc()でドキュメントをみることができる．\n\ndoc(ImageDataLoaders)\n\n\nImageDataLoaders\nImageDataLoaders(*loaders, path:str|Path='.', device=None)Basic wrapper around several `DataLoader`s with factory methods for computer vision problems\nShow in docs\n\n\n読み込んだデータの1バッチ分は，データローダーのshow_batchメソッドでみることができる．\n\ndls.show_batch()\n\n\n\n\n畳み込みニューラルネットモデルのインスタンスを生成し，データとあわせて学習器 learn を生成する．メトリクス（評価尺度）はerror_rateを指定しておく．\n学習器はresnet34を用い，学習済みのデータを用いた転移学習を行う．\n\nResNet\nResNetは残差ブロックとよばれる層の固まりを，何層にも重ねたものである．残差ブロックでは，ブロックへの入力，線形層，ReLU(rectified linear unit; \\(\\max (0,x)\\))，線形層の流れに，入力そのものを加えたものに，ReLUを行うことによって出力を得る． 入力をそのまま最終の活性化関数の前に繋げることによって，必要のないブロックを跳ばして計算することができるようになり，これによって多層のニューラルネットで発生する勾配消失や勾配爆発を避けることが可能になる． 残差ブロックは，畳み込み層の間に「近道（ショートカット）」を入れたものに他ならない．この「近道」を入れることによって，最適化が楽になることが知られている．局所解が減少し， 滑らかな空間（ランドスケープ）での最適化になるのだ．\n残差ネットワークの学習器learnを作成してからlearn.summaryをみると、その構造がわかる。 以下で用いるresnet34は34層の大規模な畳み込みニューラルネットである。実行すると、学習済みの重みが読み込まれ，この重みをもとに転移学習を行うことができる．\n\n\n転移学習\n通常の訓練においては，初期のパラメータ（重み）はランダムに設定される．しかし，ランダムな初期パラメータからの学習は，アーキテクチャが大規模になると膨大な時間がかかることがある．そこで，特定のアーキテクチャに対して，事前に訓練されたパラメータ（重み）を用いることが行われるようになってきた．これが転移学習 (transfer learning) である．\n多層の畳み込みニューラルネットで発生を用いて画像の分類をするケースを考えよう．学習が進むにつれて，最初の方の層では線や角などの簡単な形状を抽出するようになり，層が深まるにつれて徐々に複雑な形状を学習するようになる．たとえば，猫のふわふわした毛に反応するニューロンや，猫の目に反応するニューロンが出てくる．最終層の直前では，分類したい物の特徴を抽出するニューロンがある．転移学習では，他の目的のために訓練されたパラメータを用い，判別を行う最終層だけに対して訓練（パラメータの調整）を行う．線や角の判別は共通であるが，最終的な分類は，対象とするものに依存して再訓練をしなければならないからだ．\n最終層のパラメータが十分に訓練されたら，上層のパラメータに対しても訓練を行う方が良い．fine_tuneメソッドは，これを自動的にしてくれる．\n\n\n学習率の調整\n深層学習で最も重要なパラメータは，学習率(learning rate: lrと略される）である．深層学習では，重み（パラメータ）調整のために非線形最適化を行う．\nつまり，勾配に適当なステップサイズを乗じて現在の値から減じる操作を繰り返す．この非線形最適化におけるステップサイズのことを，学習率と呼んでいる．\nこれをチューニングするために，fastaiでは学習器オブジェクトにlr_find() というメソッドを準備している．\n評価尺度(metrics）に誤差率を指定した学習器learnを作成してlearn.lr_find()とする．\nlr_findは，学習率を小さな値から1反復ごとに2倍にしたときの損出関数（目的関数のこと）をプロットしてくれる． 目安だが，最小値をもつ谷に入るあたりの学習率が良いと言われている．\n\nlearn = vision_learner(dls,resnet34, metrics=error_rate, cbs=ShowGraphCallback())\nlearn.lr_find()\n\n/Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /Users/mikiokubo/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|███████████████████████████████████████████████████████████████████| 83.3M/83.3M [00:02&lt;00:00, 40.8MB/s]\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0020892962347716093)\n\n\n\n\n\n損出関数が最小になるのは，学習率が0.2あたりだが，最も大きな谷の下り坂に入るあたりが良いとされている．ここでは，学習率を1e-2 (0.01)に設定して訓練してみる．\nこれには学習器インスタンスのfit_tuneメソッドを用いる．引数はエポック数（最適化の反復回数；データ全体を何回使うかを表す）と学習率である．\nなお，実際の反復ごとの学習率は，学習器のcbs引数をShowGraphCallback()とすると，見ることができる．\n\ndoc(learn.fine_tune)\n\nLearner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, *, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, start_epoch=0)\nFine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\n\nTo get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev\n\n\n\nlearn.fine_tune(2, base_lr=0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.200721\n0.140300\n0.038273\n00:14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.034825\n0.004138\n0.001472\n00:24\n\n\n1\n0.005775\n0.003441\n0.000981\n00:21\n\n\n\n\n\n\n\n\n評価尺度の誤差率は非常に小さく、図から訓練はうまく行われているようだ。 結果を表示してみる。大体当たっているようだ。\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\nfine_tuneでは、最終層以外を固定して（既定値では１回）訓練を行い、その後、fit_one_cycleを用いて、指定したエポック数だけ訓練する。 fit_one_cycleは，学習率を小さな値から最大学習率まで増やし，その後徐々に減少させていく．同時に，慣性項を徐々に下げて，その後増加させていく最適化法で，これを使うと収束が速くなると言われている．\nfine_tuneメソッドの引数はエポック数と基本学習率 base_lr である．\n分類モデルの結果を解釈は、ClassificationInterpretation()クラスのfrom_learnerメソッドを用いてできる。 plot_top_lossesを用いると，損出関数が悪かったデータを描画してくれる． 引数は画像数と画像のサイズである．\n\ninterp = ClassificationInterpretation.from_learner(learn)\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(9, figsize=(7,7))\n\n\n\n\n\n\n\n\n\n\n\n正解と外れを表す表（混同行列とよばれる）を出力するには，plot_confusion_matrixを使う．\n\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "16fastai.html#ｍｎｉｓｔ",
    "href": "16fastai.html#ｍｎｉｓｔ",
    "title": "fastaiによる深層学習",
    "section": "ＭＮＩＳＴ",
    "text": "ＭＮＩＳＴ\n今度は \\(３\\)と\\(７\\)だけでなく， \\(0\\)から\\(9\\)の数字の画像ファイルから数字を当ててみよう． 手順は同じだ．\n\npath = untar_data(URLs.MNIST)\n\n\ndls = ImageDataLoaders.from_folder(path)\n\n今回は，学習器の評価尺度に正解率(accuracy)を加えておく．\n\nlearn = vision_learner(dls, resnet34, metrics=[error_rate,accuracy],cbs=ShowGraphCallback())\nlr = learn.lr_find() \nprint(lr)\n\n/usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.002511886414140463)\n\n\n\n\n\n\nlearn.fine_tune(2, base_lr=0.002)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n0.587376\n0.408506\n0.127900\n0.872100\n01:26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n0.094730\n0.050687\n0.014500\n0.985500\n01:33\n\n\n1\n0.045115\n0.027908\n0.009200\n0.990800\n01:33\n\n\n\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(9, figsize=(15,10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "16fastai.html#cifar10",
    "href": "16fastai.html#cifar10",
    "title": "fastaiによる深層学習",
    "section": "Cifar10",
    "text": "Cifar10\nCifar10は粗い画像から，10種類の物体を当てるデータセットである．\nImageDataLoaderのfrom_forder()メソッドでデータローダーを生成する． 検証（テスト）データは10%に設定する．\n\n\npath = untar_data(URLs.CIFAR)\n\n\n\n\n\n\n    \n      \n      100.00% [168173568/168168549 00:01&lt;00:00]\n    \n    \n\n\n\ndls = ImageDataLoaders.from_folder(path,valid_pct=0.1)\ndls.show_batch()\n\n\n\n\n\nlearn = vision_learner(dls, resnet50, metrics=[error_rate,accuracy])\nlr= learn.lr_find() \nprint(lr)\n\n/usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.001737800776027143)\n\n\n\n\n\nデータとアーキテクチャ（モデル：RESNET）をあわせて学習器を生成する．\nメトリクスは正解率(accuracy)とする．\n\nlearn.fine_tune(10, base_lr=1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n1.720446\n1.487195\n0.504833\n0.495167\n01:36\n\n\n\n\n\n\n\n\n\n\n    \n      \n      10.00% [1/10 01:36&lt;14:28]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n1.095862\n0.945130\n0.328167\n0.671833\n01:36\n\n\n\n\n\n    \n      \n      56.35% [475/843 00:49&lt;00:38 0.8759]\n    \n    \n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n1.095862\n0.945130\n0.328167\n0.671833\n01:36\n\n\n1\n0.819684\n0.715804\n0.244000\n0.756000\n01:33\n\n\n2\n0.614426\n0.609761\n0.205833\n0.794167\n01:33\n\n\n3\n0.429096\n0.574650\n0.193333\n0.806667\n01:34\n\n\n4\n0.293377\n0.609565\n0.186167\n0.813833\n01:34\n\n\n5\n0.157066\n0.681249\n0.180667\n0.819333\n01:34\n\n\n6\n0.088063\n0.761130\n0.183500\n0.816500\n01:34\n\n\n7\n0.047697\n0.804281\n0.181500\n0.818500\n01:34\n\n\n8\n0.023899\n0.817061\n0.181333\n0.818667\n01:34\n\n\n9\n0.026840\n0.833239\n0.180333\n0.819667\n01:33\n\n\n\n\n\n損出関数の大きい順に5つのデータを出力する．\n\ninterp = Interpretation.from_learner(learn)\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5)"
  },
  {
    "objectID": "16fastai.html#pets",
    "href": "16fastai.html#pets",
    "title": "fastaiによる深層学習",
    "section": "PETS",
    "text": "PETS\n画像ファイルから犬か猫かを判別する．\nモデル（アーキテキクチャ）は画像ファイルなのでResNetを用いる．\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.PETS)\npath\n\nPath('/Users/mikiokubo/.fastai/data/oxford-iiit-pet')\n\n\n\npath.ls()\n\n(#2) [Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/annotations')]\n\n\n\npath_anno = path/\"annotations\"\npath_img = path/\"images\"\n\n\nfnames = get_image_files(path_img)\nfnames[:5]\n\n(#5) [Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/pug_52.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/basset_hound_112.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/Siamese_193.jpg'),Path('/Users/mikiokubo/.fastai/data/oxford-iiit-pet/images/shiba_inu_122.jpg')]\n\n\n\nfiles = get_image_files(path/\"images\")\nlen(files)\n\n7390\n\n\n犬か猫かはファイル名の最初の文字が大文字か小文字かで判別できる．\nImageDataLoadersクラスのfrom_name_func()メソッドを用いてデータローダーを生成する．\n引数は順に，\n\nデータセットのパス path\nファイル名のリスト files\nラベル名を判定する関数 label_func\nデータ変換（ここでは画像ファイルのサイズの変更） item_tfms\n\nである．\n\ndef label_func(f): return f[0].isupper() #犬猫の判定\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))\n\n\ndls.show_batch()\n\n\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n/usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.156178\n0.016154\n0.004060\n00:52\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.054338\n0.005073\n0.001353\n00:54\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n今度は，同じデータセットを用いて，\\(37\\)種類のPETの種類を判別する．\nデータの読み込みには正規表現を用いる．\nImageDataLoaderクラスのfrom_name_re()メソッドは，正規表現を用いてデータを生成する．\n引数は順に，\n\nデータセットのパス path\nファイル名のリスト files\nクラス名をファイル名から抽出するための正規表現 pat\nデータ変換（ここでは画像ファイルのサイズの変更） item_tfms\naug_transformsによるデータ増大 batch_tfms\n\nである．\n\npat = r\"^(.*)_\\d+.jpg\"\ndls = ImageDataLoaders.from_name_re(path, files, pat, item_tfms=Resize(460),\n                                    batch_tfms=aug_transforms(size=224))\ndls.show_batch()\n\n\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\nlearn.fine_tune(4, 0.001)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.070160\n0.406866\n0.123139\n01:07\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.576859\n0.279217\n0.085250\n01:10\n\n\n1\n0.409450\n0.238351\n0.067659\n01:10\n\n\n2\n0.260191\n0.211842\n0.070365\n01:10\n\n\n3\n0.193102\n0.204086\n0.064953\n01:10\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\ninterp = Interpretation.from_learner(learn)\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(9, figsize=(15,10))"
  },
  {
    "objectID": "16fastai.html#表形式データ",
    "href": "16fastai.html#表形式データ",
    "title": "fastaiによる深層学習",
    "section": "表形式データ",
    "text": "表形式データ\n\nfrom fastai.tabular.all import *\n\n\n例題： サラリーの分類\nADULT_SAMPLEは，小規模な表形式データであり，$50k以上の収入があるかどうかを当てるのが目的だ．\n\npath = untar_data(URLs.ADULT_SAMPLE)\npath\n\n\n\n\n\n\n    \n      \n      100.69% [974848/968212 00:00&lt;00:00]\n    \n    \n\n\nPath('/Users/mikiokubo/.fastai/data/adult_sample')\n\n\n\ndf = pd.read_csv(path / \"adult.csv\")\ndf.head().T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nage\n49\n44\n38\n38\n42\n\n\nworkclass\nPrivate\nPrivate\nPrivate\nSelf-emp-inc\nSelf-emp-not-inc\n\n\nfnlwgt\n101320\n236746\n96185\n112847\n82297\n\n\neducation\nAssoc-acdm\nMasters\nHS-grad\nProf-school\n7th-8th\n\n\neducation-num\n12.0\n14.0\nNaN\n15.0\nNaN\n\n\nmarital-status\nMarried-civ-spouse\nDivorced\nDivorced\nMarried-civ-spouse\nMarried-civ-spouse\n\n\noccupation\nNaN\nExec-managerial\nNaN\nProf-specialty\nOther-service\n\n\nrelationship\nWife\nNot-in-family\nUnmarried\nHusband\nWife\n\n\nrace\nWhite\nWhite\nBlack\nAsian-Pac-Islander\nBlack\n\n\nsex\nFemale\nMale\nFemale\nMale\nFemale\n\n\ncapital-gain\n0\n10520\n0\n0\n0\n\n\ncapital-loss\n1902\n0\n0\n0\n0\n\n\nhours-per-week\n40\n45\n32\n40\n50\n\n\nnative-country\nUnited-States\nUnited-States\nUnited-States\nUnited-States\nUnited-States\n\n\nsalary\n&gt;=50k\n&gt;=50k\n&lt;50k\n&gt;=50k\n&lt;50k\n\n\n\n\n\n\n\n表形式データの基本クラスは TabularDataLoaders であり，これはfrom_csvメソッドやfrom_dfを用いて作ることができる．\nfrom_csvの主な引数の意味は以下の通り．\n\ncsv: csvファイル\npath: ファイルの置き場所\ny_names: 従属変数（ターゲット）の列名（のリスト）\nvalid_idx: 検証用データのインデックス\nproc: 前処理の方法を入れたリスト\ncat_names: カテゴリーデータの列名のリスト\ncont_names: 連続量データの列名のリスト\nカテゴリーデータと連続量データを自動的に分けてくれる以下の関数も準備されている．\n\ncont_names, cat_names = cont_cat_split(df=データフレーム, dep_var=従属変数の列名)\n\nprocs: 前処理の指定\n\n前処理には以下のものがある．\n\nCategorify: cat_names引数で与えた列リストをカテゴリー変数とする．\nFillMissing： cont_namesに含まれる連続変数に対して欠損値処理を行う．\n\n引数のFillStrategyには[MEDIAN, COMMON, CONSTANT]があり，順にメディアン，最頻値，定数（fill_valで指定）である． また，add_col引数がTrueのときには，欠損値であることを表す列を追加する．\n\nNormalize: cont_namesに含まれる連続変数の正規化を行う．(平均を引いて標準偏差+微少量で割る．）\n\n時刻型の列を自動的に幾つかの変数に変換する以下の関数が準備されている．\nadd_datepart(df, fldname, drop=True, time=False)\nfldnameは時刻型が含まれている列名であり，dropがTrueのとき元の列を削除する．またtimeがTrueのときには，日付だけでなく時，分，秒の列も追加する．\n\ndls = TabularDataLoaders.from_csv(\n    path / \"adult.csv\",\n    path=path,\n    y_names=\"salary\",\n    cat_names=[\n        \"workclass\",\n        \"education\",\n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n    ],\n    cont_names=[\"age\", \"fnlwgt\", \"education-num\"],\n    procs=[Categorify, FillMissing, Normalize],\n)\n\n\ndls.show_batch()\n\n\n\n\n\nworkclass\neducation\nmarital-status\noccupation\nrelationship\nrace\neducation-num_na\nage\nfnlwgt\neducation-num\nsalary\n\n\n\n\n0\nPrivate\nBachelors\nMarried-civ-spouse\nSales\nHusband\nWhite\nFalse\n45.000000\n162186.999591\n13.0\n&gt;=50k\n\n\n1\nPrivate\nSome-college\nMarried-civ-spouse\nTech-support\nHusband\nWhite\nFalse\n50.000000\n226496.999148\n10.0\n&gt;=50k\n\n\n2\nSelf-emp-inc\nBachelors\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nFalse\n49.000000\n197038.000301\n13.0\n&gt;=50k\n\n\n3\nPrivate\nHS-grad\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nFalse\n35.000000\n54317.004480\n9.0\n&lt;50k\n\n\n4\nPrivate\nHS-grad\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nFalse\n45.000000\n197730.999852\n9.0\n&lt;50k\n\n\n5\nPrivate\n1st-4th\nNever-married\n#na#\nNot-in-family\nWhite\nTrue\n22.000000\n464102.988691\n10.0\n&lt;50k\n\n\n6\nPrivate\nBachelors\nMarried-civ-spouse\nProf-specialty\nWife\nBlack\nFalse\n28.000000\n338408.999201\n13.0\n&lt;50k\n\n\n7\nPrivate\nSome-college\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nFalse\n57.000001\n171242.000203\n10.0\n&gt;=50k\n\n\n8\nState-gov\nHS-grad\nMarried-civ-spouse\nOther-service\nWife\nWhite\nFalse\n30.000000\n234823.998613\n9.0\n&lt;50k\n\n\n9\nSelf-emp-not-inc\nHS-grad\nMarried-civ-spouse\nSales\nHusband\nWhite\nFalse\n55.000000\n184701.999856\n9.0\n&lt;50k\n\n\n\n\n\n\ncont_names, cat_names = cont_cat_split(df, max_card=50, dep_var=\"salary\")\ncat_names\n\n['workclass',\n 'education',\n 'marital-status',\n 'occupation',\n 'relationship',\n 'race',\n 'sex',\n 'native-country']\n\n\n\nprocs = [FillMissing, Categorify, Normalize]  # 前処理の種類を準備．\nvalid_idx = range(len(df) - 2000, len(df))  # 検証用データのインデックスを準備．\ndep_var = \"salary\"  # 従属変数名とカテゴリー変数が格納されている列リストを準備．\ncat_names = [\n    \"workclass\",\n    \"education\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"native-country\",\n]\ncont_names = [\n    \"age\",\n    \"fnlwgt\",\n    \"education-num\",\n    \"capital-gain\",\n    \"capital-loss\",\n    \"hours-per-week\",\n]\n\ntabular_learner関数で表形式データの深層学習器を作ることができる．\n主な引数の意味は以下の通り． - dls: データローダー - layers: レイヤの数を指定したリスト - emb_szs: カテゴリーデータの列名をキー，埋め込みサイズを値とした辞書 - metrics: 評価尺度(accuracyなど） - emb_drop: 埋め込みレイヤのdrop out率\n\n# 深層学習(PyTorch)の学習器インスタンスlearnを生成し，fitメソッドで訓練．引数はエポック数と学習率．\nlearn = tabular_learner(dls, metrics=accuracy)\n\n\nlearn.fit_one_cycle(3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.378957\n0.361153\n0.829853\n00:03\n\n\n1\n0.355874\n0.355134\n0.835995\n00:03\n\n\n2\n0.352343\n0.352415\n0.835688\n00:03\n\n\n\n\n\nsummary属性をみると，学習器は，埋め込み層に続いて2つの線形層を配置したニューラルネットになっていることが確認できる．\n\nlearn.summary()\n\n\n\n\n\n\n\n\nTabularModel (Input shape: 64 x 7)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 6              \nEmbedding                                 60         True      \n____________________________________________________________________________\n                     64 x 8              \nEmbedding                                 136        True      \n____________________________________________________________________________\n                     64 x 5              \nEmbedding                                 40         True      \n____________________________________________________________________________\n                     64 x 8              \nEmbedding                                 128        True      \n____________________________________________________________________________\n                     64 x 5              \nEmbedding                                 35         True      \n____________________________________________________________________________\n                     64 x 4              \nEmbedding                                 24         True      \n____________________________________________________________________________\n                     64 x 3              \nEmbedding                                 9          True      \nDropout                                                        \nBatchNorm1d                               6          True      \n____________________________________________________________________________\n                     64 x 200            \nLinear                                    8400       True      \nReLU                                                           \nBatchNorm1d                               400        True      \n____________________________________________________________________________\n                     64 x 100            \nLinear                                    20000      True      \nReLU                                                           \nBatchNorm1d                               200        True      \n____________________________________________________________________________\n                     64 x 2              \nLinear                                    202        True      \n____________________________________________________________________________\n\nTotal params: 29,640\nTotal trainable params: 29,640\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nModel unfrozen\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n\n\n\n\n例題：住宅価格の予測\nBostonの住宅価格の予測を深層学習を用いた回帰分析で行う．\nmedvが住宅の価格で，他のデータ（犯罪率や人口などの数値データ）から予測する．\nただし，訓練データとテストデータのインデックス（train_idx,valid_idx）を生成するには， 以下に示すように，scikit-learnのtrain_test_splitを用いる．\n連続データとカテゴリーデータの列は， cont_cat_split関数を用いる． 引数の max_card \\(=50\\) は \\(50\\)以下の種類しかもたない列はカテゴリー変数とみなすことを意味する． また，引数の dep_varは従属変数名である．\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nboston = pd.read_csv(\"http://logopt.com/data/Boston.csv\",index_col=0)\nprocs = [Categorify, FillMissing, Normalize] #前処理の種類を準備．\ntrain_idx, valid_idx = train_test_split(range(len(boston)), test_size=0.3) #検証用データのインデックスを準備．\ndep_var = \"medv\" #従属変数名を準備．\n\ncont_names, cat_names = cont_cat_split(boston, max_card = 50, dep_var=dep_var)\nprint(cat_names, cont_names)\n\n['chas', 'rad'] ['crim', 'zn', 'indus', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'black', 'lstat']\n\n\n準備ができたので， TabularDataLoadersのfrom_dfメソッドでデータローダーを生成し，それをもとにtabular_learner関数で学習器 learn を作る． 評価尺度(metrics)には rmse （rooted mean square error)を用いる．\nfit_one_cycle法を用いて， 30エポック，最大学習率0.001で訓練する．\n\ndls = TabularDataLoaders.from_df(boston, y_names=dep_var, procs = procs, cont_names=cont_names, cat_names=cat_names)\nlearn = tabular_learner(dls, metrics=rmse)\nlearn.fit_one_cycle(30,1e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n_rmse\ntime\n\n\n\n\n0\n583.483521\n612.155334\n24.741774\n00:00\n\n\n1\n581.810120\n611.932373\n24.737268\n00:00\n\n\n2\n578.824707\n608.098999\n24.659664\n00:00\n\n\n3\n574.245300\n598.170288\n24.457520\n00:00\n\n\n4\n567.828918\n579.099182\n24.064480\n00:00\n\n\n5\n556.611816\n545.330933\n23.352322\n00:00\n\n\n6\n537.237671\n484.291077\n22.006615\n00:00\n\n\n7\n508.969635\n396.657959\n19.916273\n00:00\n\n\n8\n469.514343\n293.581970\n17.134233\n00:00\n\n\n9\n424.950714\n204.514008\n14.300838\n00:00\n\n\n10\n377.579315\n132.951584\n11.530462\n00:00\n\n\n11\n334.218781\n91.368095\n9.558666\n00:00\n\n\n12\n295.503296\n62.303730\n7.893271\n00:00\n\n\n13\n261.120300\n45.621571\n6.754374\n00:00\n\n\n14\n231.821823\n36.233948\n6.019464\n00:00\n\n\n15\n205.355896\n31.576481\n5.619295\n00:00\n\n\n16\n182.979019\n28.494129\n5.337989\n00:00\n\n\n17\n163.683594\n26.984529\n5.194664\n00:00\n\n\n18\n146.885498\n25.802696\n5.079636\n00:00\n\n\n19\n132.069229\n23.790033\n4.877503\n00:00\n\n\n20\n119.163567\n23.443287\n4.841827\n00:00\n\n\n21\n107.618279\n22.275869\n4.719732\n00:00\n\n\n22\n97.619194\n21.489685\n4.635697\n00:00\n\n\n23\n88.492821\n20.707258\n4.550523\n00:00\n\n\n24\n80.453087\n20.948822\n4.576989\n00:00\n\n\n25\n73.967003\n20.161036\n4.490104\n00:00\n\n\n26\n68.017082\n20.298531\n4.505389\n00:00\n\n\n27\n62.846817\n20.439318\n4.520987\n00:00\n\n\n28\n58.536152\n20.634672\n4.542541\n00:00\n\n\n29\n53.941723\n20.251295\n4.500144\n00:00\n\n\n\n\n\n\n\n問題（スパム）\nメールがスパム（spam；迷惑メイル）か否かを，深層学習を用いて判定せよ．\nデータは，様々な数値情報から，is_spam列が1 （スパム）か，0（スパムでない）かを判定するデータである．\n評価尺度はaccuracyとする．\n\nimport pandas as pd\nspam = pd.read_csv(\"http://logopt.com/data/spam.csv\")\nspam.head()\n\n\n\n\n\n\n\n\nword_freq_make\nword_freq_address\nword_freq_all\nword_freq_3d\nword_freq_our\nword_freq_over\nword_freq_remove\nword_freq_internet\nword_freq_order\nword_freq_mail\nword_freq_receive\nword_freq_will\nword_freq_people\nword_freq_report\nword_freq_addresses\nword_freq_free\nword_freq_business\nword_freq_email\nword_freq_you\nword_freq_credit\nword_freq_your\nword_freq_font\nword_freq_000\nword_freq_money\nword_freq_hp\nword_freq_hpl\nword_freq_george\nword_freq_650\nword_freq_lab\nword_freq_labs\nword_freq_telnet\nword_freq_857\nword_freq_data\nword_freq_415\nword_freq_85\nword_freq_technology\nword_freq_1999\nword_freq_parts\nword_freq_pm\nword_freq_direct\nword_freq_cs\nword_freq_meeting\nword_freq_original\nword_freq_project\nword_freq_re\nword_freq_edu\nword_freq_table\nword_freq_conference\nchar_freq_;\nchar_freq_(\nchar_freq_[\nchar_freq_!\nchar_freq_$\nchar_freq_#\ncapital_run_length_average\ncapital_run_length_longest\ncapital_run_length_total\nis_spam\n\n\n\n\n0\n0.21\n0.28\n0.50\n0.0\n0.14\n0.28\n0.21\n0.07\n0.00\n0.94\n0.21\n0.79\n0.65\n0.21\n0.14\n0.14\n0.07\n0.28\n3.47\n0.00\n1.59\n0.0\n0.43\n0.43\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.07\n0.0\n0.0\n0.00\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.132\n0.0\n0.372\n0.180\n0.048\n5.114\n101\n1028\n1\n\n\n1\n0.06\n0.00\n0.71\n0.0\n1.23\n0.19\n0.19\n0.12\n0.64\n0.25\n0.38\n0.45\n0.12\n0.00\n1.75\n0.06\n0.06\n1.03\n1.36\n0.32\n0.51\n0.0\n1.16\n0.06\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.06\n0.0\n0.0\n0.12\n0.0\n0.06\n0.06\n0.0\n0.0\n0.01\n0.143\n0.0\n0.276\n0.184\n0.010\n9.821\n485\n2259\n1\n\n\n2\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n0.31\n0.31\n0.31\n0.00\n0.00\n0.31\n0.00\n0.00\n3.18\n0.00\n0.31\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.137\n0.0\n0.137\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n3\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n0.31\n0.31\n0.31\n0.00\n0.00\n0.31\n0.00\n0.00\n3.18\n0.00\n0.31\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.135\n0.0\n0.135\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n4\n0.00\n0.00\n0.00\n0.0\n1.85\n0.00\n0.00\n1.85\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.223\n0.0\n0.000\n0.000\n0.000\n3.000\n15\n54\n1\n\n\n\n\n\n\n\n\n#export\n# spam = pd.read_csv(\"http://logopt.com/data/spam.csv\")\n# procs = [Categorify, FillMissing, Normalize] #前処理の種類を準備．\n# train_idx, valid_idx = train_test_split(range(len(spam)), test_size=0.3) #検証用データのインデックスを準備．\n# dep_var = \"is_spam\" #従属変数名を準備．\n# cont_names, cat_names = cont_cat_split(spam, max_card = 50, dep_var=dep_var)\n# dls = TabularDataLoaders.from_df(spam, y_names=dep_var, procs = procs, cont_names=cont_names, cat_names=cat_names)\n# learn = tabular_learner(dls, metrics=accuracy)\n# learn.fit_one_cycle(30,1e-3)\n\n\n\n問題（毒キノコ）\nデータから毒キノコか否かを，深層学習を用いて判定せよ．\ntarget列がターゲット（従属変数）であり，edibleが食用，poisonousが毒である．\n評価尺度はaccuracyとする．\n\nmashroom = pd.read_csv(\n    \"http://logopt.com/data/mashroom.csv\",\n    dtype={\"shape\": \"category\", \"surface\": \"category\", \"color\": \"category\"},\n)\nmashroom.head()\n\n\n\n\n\n\n\n\ntarget\nshape\nsurface\ncolor\n\n\n\n\n0\nedible\nconvex\nsmooth\nyellow\n\n\n1\nedible\nbell\nsmooth\nwhite\n\n\n2\npoisonous\nconvex\nscaly\nwhite\n\n\n3\nedible\nconvex\nsmooth\ngray\n\n\n4\nedible\nconvex\nscaly\nyellow\n\n\n\n\n\n\n\n\n# export\n# mashroom = pd.read_csv(\"http://logopt.com/data/mashroom.csv\", dtype = {\"shape\":\"category\", \"surface\":\"category\", \"color\":\"category\"})\n# procs = [Categorify, FillMissing, Normalize] #前処理の種類を準備．\n# train_idx, valid_idx = train_test_split(range(len(mashroom)), test_size=0.3) #検証用データのインデックスを準備．\n# dep_var = \"target\" #従属変数名を準備．\n# cont_names, cat_names = cont_cat_split(mashroom, max_card = 50, dep_var=dep_var)\n# #print(cat_names, cont_names)\n# dls = TabularDataLoaders.from_df(mashroom, y_names=dep_var, procs = procs, cont_names=cont_names, cat_names=cat_names)\n# learn = tabular_learner(dls, metrics=accuracy)\n# learn.fit_one_cycle(30,1e-3)\n\n\n\n問題（タイタニック）\ntitanicデータに対して深層学習を行い，死亡確率の推定を行え．\n\ntitanic = pd.read_csv(\"http://logopt.com/data/titanic.csv\")\ntitanic.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n# export\n# titanic = pd.read_csv(\"http://logopt.com/data/titanic.csv\")\n# train_idx, valid_idx = train_test_split(range(len(titanic)), test_size=0.2) #検証用データのインデックスを準備．\n# dep_var = \"Survived\" #従属変数名\n# cat_names = [\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Cabin\",\"Embarked\"] #カテゴリー変数が格納されている列リスト．\n# cont_names = [\"Age\",\"Fare\"] #連続変数名\n# procs = [FillMissing, Categorify, Normalize] #前処理の種類を準備．\n# dls = TabularDataLoaders.from_df(titanic, y_names=dep_var, procs = procs, cont_names=cont_names, cat_names=cat_names)\n# learn = tabular_learner(dls, metrics=accuracy)\n# learn.fit_one_cycle(30,1e-3)\n\n\n\n問題（胸部癌）\nhttp://logopt.com/data/cancer.csv にある胸部癌か否かを判定するデータセットを用いて，深層学習による分類を行え．\n最初の列diagnosisが癌か否かを表すものであり，“M”が悪性（malignant），“B”が良性（benign）を表す．\n\ncancer = pd.read_csv(\"http://logopt.com/data/cancer.csv\", index_col=0)\ncancer.head()\n\n\n\n\n\n\n\n\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n# export\n# cancer = pd.read_csv(\"http://logopt.com/data/cancer.csv\", index_col=0)\n# train_idx, valid_idx = train_test_split(range(len(cancer)), test_size=0.2) #検証用データのインデックスを準備．\n# dep_var = \"diagnosis\" #従属変数名\n# procs = [FillMissing, Normalize] #前処理の種類を準備．\n# dls = TabularDataLoaders.from_df(cancer, y_names=dep_var, procs = procs)\n# learn = tabular_learner(dls, metrics=accuracy)\n# learn.fit_one_cycle(30,1e-3)\n\n\n\n問題（部屋）\n以下の部屋が使われているか否かを判定するデータに対して，深層学習による分類を行え．\noccupancy列が部屋が使われているか否かを表す情報であり，これをdatetime列以外の情報から分類せよ．\n\noccupancy = pd.read_csv(\"http://logopt.com/data/occupancy.csv\")\noccupancy.head()\n\n\n\n\n\n\n\n\ndatetime\ntemperature\nrelative humidity\nlight\nCO2\nhumidity\noccupancy\n\n\n\n\n0\n2015-02-04 17:51:00\n23.18\n27.2720\n426.0\n721.25\n0.004793\n1\n\n\n1\n2015-02-04 17:51:59\n23.15\n27.2675\n429.5\n714.00\n0.004783\n1\n\n\n2\n2015-02-04 17:53:00\n23.15\n27.2450\n426.0\n713.50\n0.004779\n1\n\n\n3\n2015-02-04 17:54:00\n23.15\n27.2000\n426.0\n708.25\n0.004772\n1\n\n\n4\n2015-02-04 17:55:00\n23.10\n27.2000\n426.0\n704.50\n0.004757\n1\n\n\n\n\n\n\n\n\n# export\n# occupancy = pd.read_csv(\"http://logopt.com/data/occupancy.csv\")\n# occupancy.drop(\"datetime\", axis=1, inplace=True)\n# procs = [Categorify, FillMissing, Normalize] #前処理の種類を準備．\n# train_idx, valid_idx = train_test_split(range(len(occupancy)), test_size=0.3) #検証用データのインデックスを準備．\n# dep_var = \"occupancy\" #従属変数名を準備．\n# cont_names, cat_names = cont_cat_split(occupancy, max_card = 50, dep_var=dep_var)\n# print(cat_names, cont_names)\n# dls = TabularDataLoaders.from_df(occupancy, y_names=dep_var, procs = procs, cont_names=cont_names, cat_names=cat_names)\n# learn = tabular_learner(dls, metrics=accuracy)\n# learn.fit_one_cycle(30,1e-3)"
  },
  {
    "objectID": "16fastai.html#画像データ",
    "href": "16fastai.html#画像データ",
    "title": "fastaiによる深層学習",
    "section": "画像データ",
    "text": "画像データ\nデータ一覧は Data Externalにある．\nhttp://docs.fast.ai/data.external#download_url\n\nfrom fastai.vision import *\n\n\n複数のラベルを生成する分類\nPASCAL_2007データを読み込み，複数のラベルの予測を行う．\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.PASCAL_2007)\n\n\n\n\n\ndf = pd.read_csv(path/\"train.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n\n\n\n\n\nデータブロック DataBlockを始めに生成して，それからデータローダーを作る．\nデータブロックは，以下の引数をもつ．\n\nblocks: データブロックを構成するブロックのタプル； 画像ブロックと（複数の）カテゴリーブロック\nsplitter: 訓練データと検証データのインデックスを返す関数\nget_x: 独立変数（特徴ベクトル）を返す関数\nget_y: 従属変数（ターゲット）を返す関数\nitem_tfms: 個々のデータの変換の指示\nbatch_tfms: バッチに対する変換の指示\n\n\ndef get_x(r): return path/\"train\"/r[\"fname\"]\ndef get_y(r): return r[\"labels\"].split(\" \")\ndef splitter(df):\n    train = df.index[df[\"is_valid\"]].tolist()\n    valid = df.index[df[\"is_valid\"]].tolist()\n    return train,valid\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\ndls.show_batch(nrows=1, ncols=3)\n\n\n\n\n評価尺度には多ラベル用の正解率 accuracy_multiを用いる． また，関数partialで，引数の閾値（thresh)を0.2に固定して渡す． partialは標準モジュールのfunctoolsに含まれているが，fastaiではすでにimportした状態になっている．\nfine_tuneで訓練をするが，最終層以外を固定して（freezeして）４エポック訓練し，その後，最終層以外も自由にして3エポック訓練する．\n\nlearn = vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.939143\n0.686238\n0.235538\n00:28\n\n\n1\n0.824816\n0.574355\n0.280578\n00:27\n\n\n2\n0.605865\n0.203421\n0.807829\n00:28\n\n\n3\n0.359789\n0.127288\n0.937928\n00:28\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.138787\n0.124408\n0.941434\n00:29\n\n\n1\n0.118910\n0.108459\n0.948426\n00:29\n\n\n2\n0.097468\n0.105211\n0.952430\n00:29\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n画像から人の頭の中心を当てる回帰\n画像データは分類だけでなく，回帰を行うこともできる． BIWIデータを読み込み，サンプル画像を表示する．\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f\"{str(x)[:-7]}pose.txt\")\nimg2pose(img_files[0])\n\nPath('/root/.fastai/data/biwi_head_pose/06/frame_00079_pose.txt')\n\n\n\ncal = np.genfromtxt(path/\"01\"/\"rgb.cal\", skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name==\"13\"),\n    batch_tfms=[*aug_transforms(size=(240,320)), \n                Normalize.from_stats(*imagenet_stats)]\n)\n\n\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(lr_min=0.004786301031708717, lr_steep=1.3182567499825382e-06)\n\n\n\n\n\n\nlearn.fine_tune(4, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.052732\n0.005105\n02:07\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.005605\n0.001754\n02:17\n\n\n1\n0.003696\n0.001718\n02:17\n\n\n2\n0.002149\n0.000066\n02:16\n\n\n3\n0.001374\n0.000060\n02:18\n\n\n\n\n\n正解と予測を表示\n\nlearn.show_results()"
  },
  {
    "objectID": "16fastai.html#協調フィルタリング",
    "href": "16fastai.html#協調フィルタリング",
    "title": "fastaiによる深層学習",
    "section": "協調フィルタリング",
    "text": "協調フィルタリング\n\nfrom fastai.tabular.all import *\nfrom fastai.collab import *\n\n協調フィルタリング(collaborative filtering)とは，推奨システム(recommender system)の一種で，ユーザーとアイテムの両方の潜在因子を考慮して，レーティングを決める手法だ．\n推奨システムでよく見かけるのは，「この商品を買った人はこの商品も買っています」とか「最も良く売れているのはこの商品です」などの猿でもできるタイプのものだ．このような単純なものではなく，あなたに似た潜在因子をもつ人が，高いレーティングをつけている（もしくは良く購入する）商品に近い潜在因子をもった商品を紹介するのが，協調フィルタリングである．\n機械学習の中で（Andrew Ngが実務家から聞いた話だが）実務で最も役に立つ，もしくは期待されているのがこれだ． 背景にある理論を簡単に紹介しよう．\nいま，顧客と商品の集合とともに，商品 \\(i\\) に対して顧客 \\(j\\) が評価を行ったデータが与えられているものとする． ただし，顧客が評価をつけた商品は通常は少なく，データは極めて疎な行列として与えられている． 商品 \\(i\\) に対して顧客 \\(j\\) が評価を行っているとき \\(1\\)，それ以外のとき \\(0\\) のパラメータを \\(r(i,j)\\) とする． \\(r(i,j)=1\\) の場合には，顧客 \\(j\\) は商品 \\(i\\) に対して離散値の（たとえば \\(1\\) から \\(5\\) の整数などで）評価点をつける． この評価点を表すデータを \\(y^{(i,j)}\\) とする．これがトレーニングデータになる． これをもとに，評価点がつけられていない（\\(r(i,j)=0\\) の）場所の評価点を推定することが問題の目的となる．\n推奨システム設計のための手法は， コンテンツベース推奨(contents based recommendation)と協調フィルタリング推奨(collaborative filtering recommendation)の2つに分類できる．\nコンテンツベース推奨では，商品 \\(i\\) に対する特徴ベクトル \\(x^{(i)} \\in R^n\\) が与えられていると仮定する． たとえば，商品を映画としたときに，特徴ベクトルは映画の種別（アクション，SF，ホラー，恋愛もの，スリラー ，ファンタジーなど）の度合いを表す． たとえば，スターウォーズはSF度 \\(0.8\\)，恋愛度 \\(0.1\\)，アクション度 \\(0.1\\) と採点される．\n顧客 \\(j\\) の特徴に対する重みベクトルを \\(w^{(j)} \\in R^n\\) とする．これは顧客がどういった映画の種別を好むのかを表す． これを線形回帰を用いて求めることを考えると仮説関数は， \\[\n  h_w (x)=w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n  \n\\] となる． 最適な重みを計算するには，以下に定義される費用関数を最小にする重みベクトル \\(w^{(j)}\\) を求めればよい． \\[\n\\frac{1}{2} \\sum_{i: r(i,j) = 1 } \\left(  (w^{(j)})^T (x^{(i)}) -y^{(i,j)} \\right)^2\n\\]\n映画ごとに特徴を見積もることは実際には難しい． そこで，協調フィルタリング推奨では，商品ごとの特徴ベクトル \\(x^{(i)} \\in R^n\\) を定数として与えるのではなく， 変数とみなして顧客ごとの重みと同時に最適化を行う． すべての顧客と商品に対するトレーニングデータとの誤差の2乗和を最小化する問題は，以下のように書ける． \\[\n\\min_{w, x} \\frac{1}{2} \\sum_{(i,j): r(i,j) = 1 } \\left(  (w^{(j)})^T (x^{(i)}) -y^{(i,j)} \\right)^2\n\\]\nこの問題を直接最適化してもよいが，\\(x\\) と \\(w\\) を交互に線形回帰を用いて解く簡便法も考えられる． すなわち，適当な特徴ベクトルの推定値 \\(x^{(i)}\\) を用いて顧客 \\(j\\) に対する重みベクトル \\(w^{(j)}\\) を求めた後に， 今度は \\(w^{(j)}\\) を用いて \\(x^{(i)}\\) を求めるのである．この操作を収束するまで繰り返せばよい．\n上のアルゴリズムを用いて得られた商品 \\(i\\) の特徴ベクトル \\(x^{(i)}\\) を用いると， 類似の商品を抽出することができる． たとえば，\\(x^{(i)}\\) を \\(n\\)次元空間内の点とみなしてクラスタリングを行うことによって， 商品のクラスタリングができる．同様に顧客 \\(j\\) の重みベクトル \\(w^{(j)}\\) を用いることによって顧客のクラスタリングができる．\n有名な例題（映画の評価値を当てる）であるMovieLensのデータを読み込む．\nデータにはtimestamp列がついているが，とりあえずこれは無視してレーティング(rating)を予測してみる．\n\npath = untar_data(URLs.ML_100k)\nratings = pd.read_csv(path/\"u.data\", delimiter=\"\\t\", header=None,\n                      usecols=(0,1,2), names=[\"user\",\"movie\",\"rating\"])\nratings.head()\n\n\n\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\n\n\n\n\n0\n196\n242\n3\n\n\n1\n186\n302\n3\n\n\n2\n22\n377\n1\n\n\n3\n244\n51\n2\n\n\n4\n166\n346\n1\n\n\n\n\n\n\n\n\nmovies = pd.read_csv(path/\"u.item\",  delimiter=\"|\", encoding=\"latin-1\",\n                     usecols=(0,1), names=(\"movie\",\"title\"), header=None)\nmovies.head()\n\n\n\n\n\n\n\n\nmovie\ntitle\n\n\n\n\n0\n1\nToy Story (1995)\n\n\n1\n2\nGoldenEye (1995)\n\n\n2\n3\nFour Rooms (1995)\n\n\n3\n4\nGet Shorty (1995)\n\n\n4\n5\nCopycat (1995)\n\n\n\n\n\n\n\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\ntitle\n\n\n\n\n0\n196\n242\n3\nKolya (1996)\n\n\n1\n63\n242\n3\nKolya (1996)\n\n\n2\n226\n242\n5\nKolya (1996)\n\n\n3\n154\n242\n3\nKolya (1996)\n\n\n4\n306\n242\n5\nKolya (1996)\n\n\n\n\n\n\n\nCollabDataLoadersクラスのfrom_dfメソッドにデータフレームを入れるとデータオブジェクトを作成してくれる．\n引数はデータフレーム(ratings)，検証データの比率(pct_val)，ユーザー，アイテム，レーティングを表す列名だ．\n\ndls = CollabDataLoaders.from_df(ratings, item_name=\"title\", bs=64)\n\n作成したデータオブジェクトをcollab_learner関数に入れると学習器（誤差を最小にする潜在因子行列の重みの最適化が目的）を作ってくれる．予測したいレーティングは，星５つまでなので，y_rangeで指定する．\nデータオブジェクト(data)，潜在因子の数(n_factors)を指定しているが，他にもmetricsは評価尺度，wdはweight decayで正則化のためのパラメータなどを指定できる．\ndef collab_learner(data, n_factors:int=None, use_nn:bool=False, metrics=None,\n                  emb_szs:Dict[str,int]=None, wd:float=0.01, **kwargs)-&gt;Learner\n\ndls.show_batch()\n\n\n\n\n\nuser\ntitle\nrating\n\n\n\n\n0\n348\nJack (1996)\n4\n\n\n1\n346\nTwelve Monkeys (1995)\n2\n\n\n2\n110\nSimple Twist of Fate, A (1994)\n2\n\n\n3\n72\nConan the Barbarian (1981)\n2\n\n\n4\n864\nDeath and the Maiden (1994)\n4\n\n\n5\n347\nTwelve Monkeys (1995)\n4\n\n\n6\n731\nSabrina (1954)\n4\n\n\n7\n751\nRaising Arizona (1987)\n3\n\n\n8\n344\nLeaving Las Vegas (1995)\n4\n\n\n9\n577\nDevil in a Blue Dress (1995)\n4\n\n\n\n\n\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.935071\n0.929474\n00:06\n\n\n1\n0.812040\n0.860315\n00:06\n\n\n2\n0.631619\n0.864089\n00:06\n\n\n3\n0.400996\n0.885280\n00:06\n\n\n4\n0.289997\n0.891847\n00:06\n\n\n\n\n\n検証の損出関数をみると、過剰適合しているようだ（途中まで下がっているが、最後は上昇している）。\nL2正則化関数を入れてみよう。fastaiでは、重み減衰 (weight decay: wd) という引数で指定する。\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.952608\n0.940679\n00:06\n\n\n1\n0.839875\n0.867949\n00:06\n\n\n2\n0.742433\n0.825799\n00:06\n\n\n3\n0.591568\n0.814826\n00:06\n\n\n4\n0.479914\n0.816404\n00:06\n\n\n\n\n\n訓練ロスは悪化しているが、検証ロスは改善していることが確認できる。\nトップ1000の映画を抽出し、埋め込み層の重みを主成分分析で2次元に落として描画してみる。\nニューラルネットが、自動的に映画の特徴を抽出していることが確認できる。\n\ng = ratings.groupby(\"title\")[\"rating\"].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes[\"title\"].o2i[m] for m in top_movies])\nmovie_w = learn.model.i_weight.weight[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = np.random.choice(len(top_movies), 50, replace=False)\nidxs = list(range(100))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()"
  },
  {
    "objectID": "16fastai.html#意味分割",
    "href": "16fastai.html#意味分割",
    "title": "fastaiによる深層学習",
    "section": "意味分割",
    "text": "意味分割\n以下のデータセットは、与えられた画像の分割（各ピクセルがどの物体に属するのかを分類すること；これを意味分割(semantic segmentation)と呼ぶ）に用いられる。\n\nCamvid: Motion-based Segmentation and Recognition Dataset (CAMVID, CAMVID_TINY)\n\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/\"labels\"/f\"{o.stem}_P{o.suffix}\",\n    codes = np.loadtxt(path/\"codes.txt\", dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n2.760665\n5.930563\n01:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n2.219755\n1.865447\n01:15\n\n\n1\n1.819075\n1.280244\n01:20\n\n\n2\n1.559273\n1.177241\n01:17\n\n\n3\n1.374030\n0.868488\n01:16\n\n\n4\n1.210444\n0.832017\n01:16\n\n\n5\n1.082514\n0.755146\n01:16\n\n\n6\n0.980371\n0.728570\n01:15\n\n\n7\n0.902825\n0.727522\n01:16\n\n\n\n\n\n\nlearn.show_results(max_n=6, figsize=(7,8))"
  },
  {
    "objectID": "16fastai.html#テキストデータ",
    "href": "16fastai.html#テキストデータ",
    "title": "fastaiによる深層学習",
    "section": "テキストデータ",
    "text": "テキストデータ\nfastaiでは，Wikipediaの膨大なテキストデータを用いた学習済みの言語モデルであるAWD_LSTMを準備している． 映画の批評データを用いて，fastaiの自然言語処理を試してみる．\n\nfrom fastai.text.all import *\n\n\npath = untar_data(URLs.IMDB)\n\nget_imdb = partial(get_text_files, folders=[\"train\", \"test\", \"unsup\"])\n\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n\n\n\n\n\n\n\n言語モデルのデータブロック dls_lm をもとに，学習済のパラメータAWD_LSTMを読み込んで学習器をつくる．\n\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3, \n    metrics=[accuracy, Perplexity()]).to_fp16()\n\n\n\n\n適当な文章TEXT を入れて，その後の文章を作らせる．tempertureは文章にランダム性を付与するために用いられる．\n\nTEXT = \"This is a pen. That is an\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]\nprint(preds)\n\n\n\n\n\n\n\n[\"This is a pen . That is an allusion to what i ' ve known as The Radio Times . BBC Radio 1 ! is an example of how a different composer can work with an orchestra and which contains over half a\",\n 'This is a pen . That is an especially unusual phrase for an artist who has been ascribed to the term , and is sometimes referred to as the \" Artist Generation \" . The term is sometimes defined as defining the evolution of the']\n\n\n言語モデルを用いて，映画の批評のテキストが，ネガティブかパシティブかを判別する学習器をつくる．\n映画批評のデータセットIMDBを読み込んで，言語モデル AWD_LSTM を用いて訓練する．\n\nfrom fastai.text.all import *\n\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=\"test\")\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.598281\n0.404613\n0.822120\n04:11\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.318746\n0.245778\n0.898840\n07:56\n\n\n1\n0.249433\n0.221797\n0.908000\n08:07\n\n\n2\n0.183325\n0.190910\n0.926360\n08:19\n\n\n3\n0.153639\n0.194017\n0.926240\n08:11\n\n\n\n\n\n予測してみる．\n\nprint( learn.predict(\"I really liked that movie!\") )\n\n\n\n\n('pos', tensor(1), tensor([3.4028e-04, 9.9966e-01]))"
  },
  {
    "objectID": "16fastai.html#画像生成",
    "href": "16fastai.html#画像生成",
    "title": "fastaiによる深層学習",
    "section": "画像生成",
    "text": "画像生成\nちょっと前までは GAN (generative adversarial network; 敵対的生成ネットワーク)が流行していたが， 最近では 拡散モデル (diffusion model)を用いて，高精度な画像を高速に生成することができるようになってきた．\n\n問題（DALL・E 2）\nDALL・E2 https://openai.com/dall-e-2/ に登録して，オリジナルの画像を生成せよ．\n（注意： 生成できる画像数に制限がある（毎月リセットされる）\n\n\n問題（Hagging Face Diffusers）\nDiffusers https://github.com/huggingface/diffusers/ のQuickstartにある Getting started with Diffusers をGoogle Colab で開いて，ドライブにコピーを保存してから，最初の画像生成までを実行せよ．\n（注意： ランタイムでGPUをオンにしてから実行する． 有料版のColab Pro(+)に登録する必要はない）\n\n\n問題 （Hagging Face)\nHagging Face のモデル https://huggingface.co/models のTasks（+22 Taskを押すとたくさん出てくる)から好きなものを選び，試してみよ． また，どのようなモデルが使われているか解説を読み， （できれば） Google Colabで動かしてみよ．\n（注意： しばらくはloginなしで使えるが，時間制限を超えるとloginが必要になる）\n\nfrom transformers import pipeline\n\n\n#pip install -U sentence-transformers\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\"I'm looking forward to seeing you！\")\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'label': 'POSITIVE', 'score': 0.9998549222946167}]\n\n\n\n#hide\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\nNo model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n/Users/mikiokubo/Library/Caches/pypoetry/virtualenvs/analytics2-0ZiTWol9-py3.9/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'generated_text': 'In this course, we will teach you how to create a custom app or web app that displays on your server - not just show results to your friends on Facebook (or Instagram)\\n\\nIt works great on mobile like the iPhone and iPad\\n\\n'}]"
  },
  {
    "objectID": "16fastai.html#深層学習の基礎を図で解説",
    "href": "16fastai.html#深層学習の基礎を図で解説",
    "title": "fastaiによる深層学習",
    "section": "深層学習の基礎を図で解説",
    "text": "深層学習の基礎を図で解説\n通常の（完全結合層から成る）ニューラルネットについては，scikit-learn を用いた機械学習の章で述べた． 以下では，本章で紹介した幾つかのアーキテクチャについて解説する．\n\n畳み込みニューラルネット\n画像データに対して完全結合層だけのニューラルネットを使うことは，膨大な量のパラメータを必要とするので，適当な選択ではない． 完全結合層のかわりに畳み込み(convolusion)を用いた層を用いる方法が，畳み込みニューラルネットである．\n畳み込みニューラルネットでは，以下の図に示すように，畳み込み（一種のパラメータ行列の乗算）を行った後に，活性化関数としてLeLUを用い，さらにマックスプールでデータを小さくする操作を繰り返していく． そして，最後の層だけを完全結合層とし，分類もしくは回帰を行う．\n\nfrom IPython.display import Image\n\n\n#hide_input\n\nImage(\"../figure/cnn1.PNG\",width=1000,height=500)\n\n\n\n\n\n#hide_input\nImage(\"../figure/cnn2.PNG\",width=1000,height=500)\n\n\n\n\n\n\n回帰型ニューラルネット\n文字列や音声などの時系列データを扱うためのニューラルネットとして，回帰型ニューラルネット(recurrent neural nettwork)がある．\n長さ \\(T\\) の時系列データ \\(x^{&lt;1&gt;},x^{&lt;2&gt;},\\ldots,x^{&lt;T&gt;}\\) が与えられたとき， 回帰型ニューラルネットは，初期状態 \\(a^{&lt;0&gt;}\\) から状態の列 \\(a^{&lt;1&gt;},a^{&lt;2&gt;},\\ldots,a^{&lt;T&gt;}\\) を順次生成していく．\n各時刻 \\(t=1,2,\\ldots,T\\) において，データ \\(x^{&lt;t&gt;}\\) と前の状態 \\(a^{&lt;t-1&gt;}\\) を結合したものを入力とし活性化関数（通常は\\(\\tanh\\)）を用いて，次の状態 \\(a^{&lt;t&gt;}\\) を生成していく．\n\n#hide_input\nImage(\"../figure/rnn.PNG\",width=1000,height=500)\n\n\n\n\n\n\n長短期記憶\n回帰型ニューラルネットは，誤差逆伝播の際に勾配が無限大に発散したり消失してしまうという弱点をもっている． その弱点を克服するためのアーキテクチャとして，長短期記憶(long short-term memory: LSTM)がある．\nLSTMの特徴は，長期の記憶のためのセル(cell) \\(c^{&lt;t&gt;}\\) と，通常の状態（短期記憶に相当する） \\(a^{&lt;t&gt;}\\) の両者を保持することである． この2種類の記憶情報を，シグモイド関数 \\(\\sigma\\) の出力（\\(0\\) と \\(1\\) の間になる）と乗じることによって， そのまま保持するかリセットするかを決め， 勾配発散（消失）を避けることができる．\n\n#hide_input\nImage(\"../figure/lstm.PNG\",width=1000,height=500)\n\n\n\n\n\n\n埋め込みニューラルネット\nカテゴリーデータをより次元の低い特徴に写像するために埋め込み層(embedding layer)が使われる．以下の例では，10のカテゴリーをもつデータを，\\(10\\times5\\)の重み行列を用いて5次元の特徴に埋め込んでいる．\n\n#hide_input\nImage(\"../figure/embed.PNG\",width=1000,height=500)\n\n\n\n\n\n\n自己アテンション\n自己アテンション(self attention)は，最近注目を浴びているアーキテクチャであり， 自然言語処理に大きな進歩をもたらした．\n自己アテンションでは，入力された文章をLSTMのように順番に入力するのではなく，一度に読み込む． まず，入力された文字の埋め込みに，文字の位置情報を正弦・余弦曲線を用いて付加し， それに対してクエリー，キー，値を表す3つの全結合層を適用し，3つの行列（テンソル） \\(Q,K,V\\) を得る． 次に，クエリ行列 \\(Q\\) とキー行列 \\(K\\) の内積をとり，それにスケーリングとソフトマックスを適用することによって，入力された文字同士の関係を表すアテンションを得る． 最後に，アテンションに値行列 \\(V\\) を乗じることによって出力を得る．\n\n#hide_input\nImage(\"../figure/selfattention.PNG\",width=1000,height=500)"
  },
  {
    "objectID": "16fastai.html#実践的な深層学習のレシピと背景にある理論",
    "href": "16fastai.html#実践的な深層学習のレシピと背景にある理論",
    "title": "fastaiによる深層学習",
    "section": "実践的な深層学習のレシピと背景にある理論",
    "text": "実践的な深層学習のレシピと背景にある理論\n上では例を示すことによって深層学習の「雰囲気」を学んだが、実際問題を解くためには、プロジェクトの進めるためのコツや、背景にある理論も理解する必要がある。 以下では、それらについて簡単に述べる。\n\n訓練、検証、テスト集合\n今までの例題では，データセットをトレーニング（訓練）集合とテスト集合の2つに分けていた． 研究や勉強のためには，この2つに分けるだけで十分であるが， 実際問題に適用する際には，訓練集合(training set)，検証集合(validation set)（開発集合(development set)とよばれることもある），テスト集合(test set)の3つにデータを分けて実験を行う必要がある． 訓練集合でパラメータ（重み）をチューニングし，検証集合で実際のデータでうまく動くようにハイパーパラメータをチューニングし，最後にテスト集合で評価する． テスト集合は実験では使用できないように隠しておく．検証集合は訓練集合から適当な割合で抽出しても良い．\n昔は訓練集合は\\(6\\)割、検証集合は \\(2\\)割、テスト集合は \\(2\\) 割と言われていた。しかし、最近はデータセットが大規模化しており、 検証とテストには一定数のデータセットがあれば十分である。例えば、超大規模データセットに対しては、 \\(98\\)%を訓練、検証とテストには残りの\\(1\\)%ずつとしても良い。\n\n\nバイアスとバリアンス / 過剰適合と不足適合\n深層学習の例題（MNISTとかCifarとか）では，現在の世界記録がどのあたりなのかが分かるが，実際問題においてどこまで学習を進めればよいのかは，一般には分からない．より一般的な最適化理論では，適当な緩和問題を用いた限界値（最小化の場合には下界）を得ることができるので，誤差を評価することができる．しかし，深層学習では，それが難しい場合が多い．そのような場合に，人でテストをしてみることによって，可能な誤差を推測することが推奨される．\n人間がどんなに頑張っても出ないくらいの誤差（エラー率）をBayes最適誤差(Bayes” optimal error)とよぶ．\nバイアス  = 訓練誤差 - Bays最適誤差（もしくは人間水準誤差）\nバリアンス = 検証誤差 - 訓練誤差\nバリアンスが大きい状態を過剰適合(overfit)とよぶ．バイアスが大きい状態を過小適合(underfit)とよび，これは訓練データに対する最適化が十分でないことを表す．\nまずは訓練集合での正解率を上げることを目標に実験をするのだが，訓練データでの性能がいまいちな状態を「高バイアス」とよぶ．これを改善するには，ネットワークの規模を大きくして学習容量を大きくしたり，訓練時間を長くしたり，最適化の方法を変えたりすることが考えられる．\n訓練データでそこそこの成績をあげられたら，今度は検証集合に対して性能を評価する．これがいまいちな状態が「高バリアンス」である． これを改善するには，データの量を増やしたり，正則化・正規化を行ったりすることが考えられる．これには色々な方法が考えられるが，深層学習で手軽なのはドロップアウトを追加したり，（確率的降下法の場合にはL2ノルムのかけ具合を表す）重み減衰パラメータを大きくしたり，バッチ正規化を行うことである．\n\n\n深層学習のレシピ\n深層学習プロジェクトを正しい方向に導くためには，多少のコツがある．ここでは，そのようなコツを伝授する．\n検証集合におけるメトリクスが不十分なときに何をすれば良いだろうか？以下のような様々な方法が思いつく．\n\nより多くのデータを集める．\nより多くの訓練集合を集める．\n訓練により多くの時間をかける．\n様々な最適化アルゴリズムを試す．\nより大きなアーキテキクチャにしてみる．\nアーキテキクチャを変更してみる．\nドロップアウト層を追加してみる．\nL2正則化(regularization；重み減衰(weight decay)と同義語)を追加する．\nバッチ正規化(batch normalization)を追加する．\n\nしかし，これらを場当たり的に適用しても時間がかかるばかりで，効果は上がらない．重要なのはアーキテクチャの選択とハイパーパラメータ（ニューラルネットでは調整する重みのことをパラメータとよび，それ以外のパラメータをハイパーパラメータとよぶ）の設定である．これには，以下の手順が推奨される．\n\n画像の場合には解像度を落とした小さなデータから始めて、徐々に解像度を上げていく。\n訓練集合に対して損出関数を最小化する．できれば下限（人間の水準）に近づくようにする．これがうまくいかない場合には，学習率を適正な値に設定する。学習率が小さすぎると過少適合になり、大きすぎる最適化の探索が発散する。学習率を適正な値にしても、損出関数の値が想定よりも大きい場合には、より大規模なアーキテキクチャを試すか，異なる最適化手法を試す．バッチ正規化をアーキテクチャに追加し、最適化しやすいアーキテクチャ（残差ネットワークなど）を選択することも忘れてはならない。 メモリや計算速度が十分でないときには、単精度計算をするか、より大きなメモリをもつGPUに変更することを検討する。\n転移学習を行っている場合には、固定していたパラメータを自由に変更できるようにしてから、再び訓練を行う。\n（上の手順と並行して）検証集合に対して目的とする評価尺度（メトリクス）を達成するように訓練を行う．これがうまくいかない（過剰適合している）場合には，ドロップアウトを追加するか，重み減衰のパラメータを増やすか，訓練集合を増やすか，データ増大を行う．\nテスト集合に対して良い結果が出るように訓練を行う．これがうまくいかない場合には，検証集合を増やすか，データ増大を行う．\n実問題に対する性能評価を行う．これがうまくいかない場合には，検証集合やテスト集合が実問題を反映しているかどうかを調べ，適宜増やす．\n\n上ではいささか抽象的に手順を紹介したが，具体的なパラメータの適正化は以下の手順が推奨されている．\n\nモデル（アーキテキクチャ）を選択する際には，解きたい問題に似た問題のベンチマークでの成績を https://dawn.cs.stanford.edu/benchmark/ やhttps://benchmarks.ai/ で調べて，そこで上位の（かつ簡単な）ものを選択する．たとえば，画像から物体を当てたい場合には，画像によるクラス分けならResNet（メモリに余裕があるならDenseNetやWide ResNet）をベースにしたもの，画像分類（セグメンテーション）ならUNETを選ぶ．\nただし競技会で上位のものは大規模なモデルを使用している場合が多い．モデルの規模が増大するにしたがい誤差は小さくなる（精度が上がる）が，その一方で計算時間が増加する．解くべき問題の複雑さを考慮してなるべく小さなモデルから始めて，十分な精度が得られなかったときに，大きめのモデルを試すという方法が推奨される．\nすでに学習済みの重みがあるモデル（たとえばresnet34）を用い，転移学習を行う場合には，最終層以外の重みを変えないような状態で訓練を行う．学習率をlr_find()で可視化し，損出関数が下降している途中の範囲を求める．\n得られた適正な学習率を用いて，最終層だけを数エポック訓練する．損出関数や精度の推移を可視化し，正しく訓練されていることを確認する．\n上層も訓練できるように設定し，再びlr_find()で適切な学習率を探索する．\n最下層を上で求めた学習率とし，上層の固まりは下層の固まりよりやや（画像の場合には10分の1，テキストの場合には0.26倍）小さめになるように設定し，過剰適合になるまで訓練する．（fastaiでは層のブロックを3層になるようにまとめている．）\nすぐに過剰適合になっている場合には，それを抑止する方法を取り入れる必要がある．以下の順に試す.\n\nもっとデータを集める．\nデータ増大を行う．\nアーキテクチャ（モデル）にドロップアウト層やバッチ正規化を追加する．\n正則化のためのハイパーパラメータ（重み減衰率: weight decay(wd)）を大きめにする．\nアーキテクチャを単純化する．\n\n画像データの場合には，上の手順を解像度を下げて行い，徐々に解像度を上げて繰り返す．他の形式のデータの場合には，データの一部を用いて上の手順を行い，適切な結果が出たら大きなデータを入れて本実験を行う．\n\n\n\nL2正則化（重み減衰）が過剰適合を削減する直感的な理由\n\n重み減衰率が大きくなると，重み \\(w\\) は小さくなり，0になるものが増える．それによってニューラルネットがより疎になり，（ドロップアウトと同様に）過剰適合を削減する．\ntanhなどの非線形な活性化関数を使っている場合，重み減衰率 lambda が大きくなると \\(w\\) が小さくなるので，ニューロンへの入力も小さくなる（0に近くなる）．tanhなどの活性化関数を可視化すると分かるように0付近では線形関数に近い形をしている．したがって，非線形な活性化関数も線形関数と同じような働きをするようになり，これによって過剰適合が削減できる．\n\n\n\nハイパーパラメータのチューニング\nハイパーパラメータは、以下の順で重要である。\n\n学習率 (learning rate: lr)\n慣性項（モーメント）(momentum)\nミニバッチの大きさ\n隠れ層のユニット数\n層の数\n学習率の減らし方\n正則化パラメータ (weight decay: wd)\n活性化関数\nAdamのパラメータ\n\n#hide ### データセット\nfastaiで使用できるデータセットは、https://docs.fast.ai/data.external.html にまとめられている。ここでは、それらのうち重要なものを解説する。\nこれらのデータセットは、全てAWS(Amazon Web Service)のOpen Dataとして公開されている（https://registry.opendata.aws/）。 以下では，おおまかな分類ごとに\nデータセット名（読み込むときの文字列名；SAMPLE, TINYが最後につくものは小規模版）：解説\nの形式で示す．\n画像分類(image classification)\n\nMNIST(MNIST, MNIST_SAMPLE, MNIST_TINY, MNIST_VAR_SIZE_TINY)：\\(28 \\times 28\\) の手書き文字画像。画像分類で最初に扱われるデータセットである。\nCIFAR10 (CIFAR) : 60000個の \\(32 \\times 32\\) のカラー画像であり、10種類の物体に分類する。\nCIFAR100 (CIFAR_100): CIFAR10と同じであるが、100種類の物体に分類する。\nCaltech-UCSD Birds-200-2011(CUB_200_2011)：200種類の鳥の種類を分類する。 物体検出(localization)にも使用できる。\nCaltech 101 (CALTECH_101): 101種類のカテゴリーに分類する。物体検出にも使用できる。\n\nOxford-IIIT Pet (PETS)：27種類のペットを分類する。物体検出にも使用できる。\nOxford 102 Flowers (FLOWERS): 102種類の花の名前を分類する。画像は解像度が高い。\n\nFood-101 (FOOD)：101種類の食べ物を分類する。\nStanford cars (CARS)：196種類の車を分類する。\n\n自然言語処理(natural language processing: NLP)\n\nIMDb Large Movie Review Dataset (IMDB, IMDB_SAMPLE)：映画の批評のテキストファイルを元にした感情分類用(sentiment classification)のデータセット。\nWikitext-103 (WIKITEXT WIKITEXT_TINY): Wikipediaから抽出された1億個のトークンから構成されるデータセット。言語モデリングで用いられる。\n(AG_NEWS)\n\n(AMAZON_REVIEWS)\n\n(AMAZON_REVIEWS_POLARITY)\n(DBPEDIA)\n\n(MT_ENG_FRA)\n\n(SOGOU_NEWS)\n\n(YAHOO_ANSWERS)\n\n(YELP_REVIEWS)\n\n(YELP_REVIEWS_POLARITY)\n\n意味分割(semantic segmentation)\n以下のデータセットは、与えられた画像の分割（各ピクセルがどの物体に属するのかを分類すること）に用いられる。\n\nCamvid: Motion-based Segmentation and Recognition Dataset (CAMVID, CAMVID_TINY)\nPASCAL Visual Object Classes (VOC)\nCOCO - Common Objects in Context (COCO_SAMPLE, COCO_TINY) その他（サンプルや講義内で用いるデータ）\nPlanet (PLANET_SAMPLE, PLANET_TINY): 衛星画像から複数の説明を当てるデータ\nAdult (ADULT_SAMPLE): 年収が500Kドル以上かどうかを当てる分類問題用の表形式データ\nBIWI (BIWI_HEAD_POSE)：頭の中心を当てる回帰分析用のデータ\nMovie Lens (ML_SAMPLE)：協調フィルタリング用の映画の評価値データ\nHuman Numbers (HUMAN_NUMBERS): RNN用の数字を英語で書いたデータ\nBedroom (LSUN_BEDROOMS) : GAN用の寝室画像データ\n\n\n\n評価尺度（メトリクス）\nここではfastaiで使われる代表的な評価尺度（メトリクス）について解説する．\n\n正解率 (accuracy)\n\n入力の中で最大値のクラスが正解クラスと一致している割合．正答率，精度と訳されることもある． 正解が1つのクラスに属しているとき（これを1ラベル問題とよぶ）に用いられる．\n例： 入力として3つのクラスから成る5つのデータを与える．正解はすべてクラス1とする．入力の中で値が最大のものは，上のコードの中にあるようにargmaxメソッドで求めることができる．得られた(1,0,1,0,0)が正解(1,1,1,1,1)と一致している割合は 0.4 と計算できる．\n    from fastai.metrics import *\n    in_ = torch.Tensor([ [0.3,0.5,0.2],\n                         [0.6,0.2,0.2],\n                         [0.1,0.6,0.3],\n                         [0.9,0.0,0.1],\n                         [0.8,0.1,0.1],\n                        ])\n    targs = torch.Tensor([1,1,1,1,1]).long()\n    print(in_.argmax(dim=-1).view(5,-1))\n    print(accuracy(in_, targs))\n\n&gt;&gt;&gt;\n    tensor([[1],\n            [0],\n            [1],\n            [0],\n            [0]])\n    tensor(0.4000)\n2値分類の場合には， 正解か否か(true/false)と陽性と予測したか否か(positive/negative)があるので，以下の4通りの場合がある．\n\nTN : 真陰性 (true negative)\nFP : 偽陽性 (false positive)\nFN : 偽陰性 (false negative)\nTP : 真陽性 (true positive)\n\n正解率は以下のように定義される．\n\\[\n\\mathrm{accuracy} = \\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{FN}+\\mathrm{FP}+\\mathrm{TN}}\n\\]\n\n閾値付き正解率\n\n予測値に対するシグモイド関数の値が，与えた閾値（規定値は0.5）より大きいときに1，それ以外のとき0と計算し，その結果と正解を比較したときの正解率．\n例： ランダムな標準正規分布として与えた5つの予測値に対してシグモイド関数で[0,1]の値に変換し，閾値0.5 より大きいものと正解を比較することによって，閾値付き正解率0.6を得る．\n    y_pred = torch.randn(5)\n    y_true = torch.Tensor([1,1,1,1,1]).long()\n    print(\"y_pred = \", y_pred)\n    print(\"sigmoid = \", y_pred.sigmoid())\n    print(accuracy_thresh(y_pred, y_true))\n\n&gt;&gt;&gt;\n    y_pred =  tensor([ 0.8596,  0.3210, -0.1176,  1.0431, -0.7974])\n    sigmoid =  tensor([0.7026, 0.5796, 0.4706, 0.7394, 0.3106])\n    tensor(0.6000)\n\nトップ \\(k\\) 正解率\n\n入力値が大きいものからk個選択し，それらを正解と比較したときの正解率．\n例： 正解率と同じ例題を用いる．トップ2のクラスを出力すると，正解の1は2番目には入っていないので，トップ1の正解率と同じ0.4を得る．\n    print(in_.topk(k=2, dim=-1)[1])\n    print(top_k_accuracy(in_,targs,k=2))\n\n&gt;&gt;&gt;\n    tensor([[1, 0],\n            [0, 2],\n            [1, 2],\n            [0, 2],\n            [0, 2]])\n    tensor(0.4000)\n\nダイス係数(dice coefficient)\n\n分割問題で用いられる集合の類似度を表す評価尺度であり，引数 iou (intersection over union) が真のときには，以下のように計算する．\n\\[DICE(A,B)=\\frac{|A \\cap B|}{|A|+|B|-|A \\cap B| + 1 }\\]\n引数iouが偽（既定値）のときには，以下のように計算する．\n\\[DICE(A,B)=\\frac{2|A \\cap B|}{|A|+|B|}\\]\n例：\n    print(\"iou=False:\", dice(in_,targs,iou=False))\n    print(\"iou=True:\",dice(in_,targs,iou=True))\n\n&gt;&gt;&gt;\n    iou=False: tensor(0.5714)\n    iou=True: tensor(0.3333)\n\n誤差率 (error rate)\n\n\\(1-\\)正解率であり，上の例題では\\(1-0.4=0.6\\)となる．\n\n決定係数(coefficient of determination) \\(R^2\\)\n\n回帰モデルによって実データをどれくらい説明できているか（回帰分析の精度）を表す指標であり，1に近いほど精度が良いと解釈できる．\n\\[R^2 = 1 - {\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\over \\sum_{i=1}^n (y_i-\\bar{y})^2 }\\]\nこの定義だと（記号が\\(R^2\\)であるにもかかわらず）負になる場合もあるので，注意を要する．最大値は1で誤差が0の状態である．\\(R^2\\)が0とは，平均で予測をした場合と同じ精度という意味であり，負の場合は平均値より悪い予測を意味する．\n\n平均自乗誤差 (mean squared error)\n\n誤差の自乗の平均値であり，\\(i\\)番目のデータの正解（目標値）を\\(y_i\\)，予測値を\\(\\hat{y}_{i}\\) としたとき，以下のように定義される．\n\\[MSE = \\frac{\\sum_{i=1}^n (\\hat y_i - y_i)^2}{n}\\]\nこれの平方根をとったものがroot_mean_squared_error (RMSE) である．\n\\[RMSE =\\sqrt{\\frac{\\sum_{i=1}^n (\\hat y_i - y_i)^2}{n}}\\]\n\n平均絶対誤差 (mean absolute error)\n\n誤差の絶対値の平均値である．\n\\[MAE = \\frac{\\sum_{i=1}^n |\\hat y_i - y_i|}{n}\\]\n\n平均自乗対数誤差 (mean squared logarithmic error)\n\n予測値，正解ともに対数をとったもので評価した平均自乗誤差である．\nこれの平方根をとったものがroot mean squared logarithmic error (RMSLE) である．\n\nMAPE 平均絶対パーセント誤差 (mean absolute percentage error)\n\n\\[MAPE = \\frac{\\sum_{i=1}^n | (\\hat y_i - y_i)/y_i) |}{n}\\]\n\n適合率 (precision)：正と予測したデータのうち，実際に正であるものの割合\n\n\\[\n     \\mathrm{precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}\n\\]\n\n再現率 (recall)：実際に正であるもののうち，正であると予測されたものの割合\n\n\\[\n\\mathrm{recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\n\\]\n\n\\(f\\)ベータ\n\n適合率と再現率をパラメータ \\(\\beta\\) で調整した評価尺度であり，主に2値分類で用いられる．\n\\[f_\\beta = (1 + \\beta^2)  \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\mathrm{precision}) + \\mathrm{recall}}\\]\n\n寄与率(explained variance)\n\n「\\(1 -\\)誤差の分散/正解の分散」と定義される．\n\n自分で新しい評価尺度を作る方法\n他の評価尺度から新たに評価尺度を生成するには，標準モジュールのfunctoolsにあるpartialを使うと簡単にできる．fastaiではすでにimportした状態にあるので，以下のように呼び出せば良い．\n    acc_02 = partial(accuracy_thresh, thresh=0.2)\n    f_05 = partial(fbeta, beta=0.5)\n最初の行では，閾値付き正解率に対して，閾値を0.2に固定した評価尺度acc_02を生成し，次の行ではfベータ のパラメータ（beta) を0.5に固定した評価尺度f_05を生成している．"
  },
  {
    "objectID": "30chatgpt.html",
    "href": "30chatgpt.html",
    "title": "ChatGPTを用いたプロンプト・エンジニアリング",
    "section": "",
    "text": "openai, python-dotenv (python 3.8.1以上）をインストール\n「command」+「shift」+「.」 で、隠しファイルを表示\nhttps://platform.openai.com/account/api-keys でキーを得る． 3ヶ月で180$を無料で使用できる． .env にOPENAI_API_KEY = &lt;キー&gt; を追加\n.gitignore に .env を追加\n\nimport openai\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\n\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\n\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0, # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]\n\n\n# text = f\"\"\"\n# You should express what you want a model to do by \\ \n# providing instructions that are as clear and \\ \n# specific as you can possibly make them. \\ \n# This will guide the model towards the desired output, \\ \n# and reduce the chances of receiving irrelevant \\ \n# or incorrect responses. Don't confuse writing a \\ \n# clear prompt with writing a short prompt. \\ \n# In many cases, longer prompts provide more clarity \\ \n# and context for the model, which can lead to \\ \n# more detailed and relevant outputs.\n# \"\"\"\n# prompt = f\"\"\"\n# Summarize the text delimited by triple backticks \\ \n# into a single sentence.\n# ```{text}```\n# \"\"\"\n# response = get_completion(prompt)\n# print(response)"
  },
  {
    "objectID": "30chatgpt.html#準備",
    "href": "30chatgpt.html#準備",
    "title": "ChatGPTを用いたプロンプト・エンジニアリング",
    "section": "",
    "text": "openai, python-dotenv (python 3.8.1以上）をインストール\n「command」+「shift」+「.」 で、隠しファイルを表示\nhttps://platform.openai.com/account/api-keys でキーを得る． 3ヶ月で180$を無料で使用できる． .env にOPENAI_API_KEY = &lt;キー&gt; を追加\n.gitignore に .env を追加\n\nimport openai\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\n\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\n\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0, # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]\n\n\n# text = f\"\"\"\n# You should express what you want a model to do by \\ \n# providing instructions that are as clear and \\ \n# specific as you can possibly make them. \\ \n# This will guide the model towards the desired output, \\ \n# and reduce the chances of receiving irrelevant \\ \n# or incorrect responses. Don't confuse writing a \\ \n# clear prompt with writing a short prompt. \\ \n# In many cases, longer prompts provide more clarity \\ \n# and context for the model, which can lead to \\ \n# more detailed and relevant outputs.\n# \"\"\"\n# prompt = f\"\"\"\n# Summarize the text delimited by triple backticks \\ \n# into a single sentence.\n# ```{text}```\n# \"\"\"\n# response = get_completion(prompt)\n# print(response)"
  }
]