{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2a0b38-0459-4b4b-90d9-1187d71871a9",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorchによる深層学習\n",
    "\n",
    "> 深層学習パッケージ Pytorch の基本的な使用法を紹介する． "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9283cef-a71d-4043-9468-58585a455bc6",
   "metadata": {},
   "source": [
    "## 簡単な計算\n",
    "\n",
    "PyTorchパッケージを用いて $N$ 行 $D$ 列のランダムな行列を作り， 簡単な計算をする． 最後に， 計算された $c = \\sum x*y + z$ を $x$ に対する勾配（微分値）をbackwardで計算する． \n",
    "\n",
    "randでランダムな行列をを生成する際に， 引数 requires_grad を True に設定しておくことによって， 勾配が計算される．　\n",
    "\n",
    "変数に対しては，　dataで値が， 勾配を計算するように指定した変数に対しては，gradで勾配が得られる．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93456b5d-a2c6-4b13-a009-aca6e86b0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e94f8c-b4e5-4d14-8018-2bd4964182b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9498)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D = 3,4 #3行4列のランダムなテンソルを生成し， 勾配を計算する\n",
    "x = torch.rand( (N, D), requires_grad=True)\n",
    "y = torch.rand( (N, D), requires_grad=True)\n",
    "z = torch.rand( (N, D), requires_grad=True)\n",
    "\n",
    "a = x*y\n",
    "b = a+z\n",
    "c = torch.sum(b)\n",
    "\n",
    "c.backward()\n",
    "\n",
    "c.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbbffd-41ed-40fa-bd61-0a65f0f22cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1379, 0.2976, 0.8908, 0.8022],\n",
      "        [0.2833, 0.0333, 0.8762, 0.2876],\n",
      "        [0.3688, 0.8043, 0.9437, 0.0616]])\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d420d5-935a-45c3-96fa-189ff99ad4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7403, 0.0179, 0.0335, 0.0769],\n",
      "        [0.5766, 0.5749, 0.5347, 0.4289],\n",
      "        [0.7062, 0.0719, 0.6709, 0.1248]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448346ce-09b4-41ae-8fe5-d90caa702061",
   "metadata": {},
   "source": [
    "##  最小2乗法\n",
    "\n",
    "$y = a x$ のパラメータ $a$ の最適化を最小2乗法によって行う．\n",
    "\n",
    "from_numpyでNumPyの配列をPyTorchのテンソルに変換できる．　\n",
    "\n",
    "パラメータ $a$ をランダムに設定し，勾配を計算するように指示する．\n",
    "\n",
    "予測値 yhat を計算した後で， 損出関数 loss を誤差の2乗平均として計算し， backward で勾配を計算する．\n",
    "\n",
    "勾配の逆方向に学習率 lr (learning rate) だけ移動させたものを新しい $a$ とし， それをn_epochs回繰り返す．\n",
    "\n",
    "ここで，反復ごとに勾配を $0$ に初期化する a.grad.zero_ を呼び出すことに注意されたい．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13ef81-0530-448b-8e45-51e410bb7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd314f-106b-4344-b3eb-566e3f76b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1317.2814) tensor([2.6670])\n",
      "tensor(803.9544) tensor([4.5403])\n",
      "tensor(491.6464) tensor([6.0014])\n",
      "tensor(301.6381) tensor([7.1411])\n",
      "tensor(186.0371) tensor([8.0301])\n",
      "tensor(115.7055) tensor([8.7234])\n",
      "tensor(72.9157) tensor([9.2643])\n",
      "tensor(46.8824) tensor([9.6861])\n",
      "tensor(31.0437) tensor([10.0152])\n",
      "tensor(21.4075) tensor([10.2719])\n"
     ]
    }
   ],
   "source": [
    "x_numpy = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float32)\n",
    "y_numpy = np.array([15.0, 20.0, 34.0, 42.0, 58.0], dtype=np.float32)\n",
    "\n",
    "x = torch.from_numpy(x_numpy)\n",
    "y = torch.from_numpy(y_numpy)\n",
    "\n",
    "a = torch.rand(1, requires_grad=True) #スカラーを定義\n",
    "\n",
    "n_epochs = 10  #反復回数（エポック数）\n",
    "lr = 0.01      #学習率\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a*x \n",
    "    error = y - yhat\n",
    "    loss =(error**2).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= lr*a.grad\n",
    "\n",
    "    print(loss.data, a.data)\n",
    "    \n",
    "    a.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722f577-9694-4374-99e1-efe874d01bbf",
   "metadata": {},
   "source": [
    "## 線形回帰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f5419-e5cf-4281-85cd-a4b8696344c8",
   "metadata": {},
   "source": [
    "### クラス\n",
    "\n",
    "nn.Moduleクラスから派生させて線形回帰を行うクラス LinearRegression を作る．\n",
    "\n",
    "コンストラクタ __init__ で親クラスを呼び出した後でパラメータ $a$ を定義する．\n",
    "\n",
    "与えられた $x$ に対して予測値 $y=ax$ を計算するための関数 forward を定義する．\n",
    "\n",
    "モデルのインスタンス model のstate_dicメソッドでパラメータ $a$ の現在の値の辞書を得ることができる． "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dcd19-c470-441d-b944-92e94f364212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', tensor([0.5324]))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    #コンストラクタ\n",
    "    def __init__(self):\n",
    "        super().__init__() #親クラスのコンストラクタを呼ぶ\n",
    "        self.a = nn.Parameter(torch.rand(1, requires_grad=True)) #モデルのパラメータを準備\n",
    "        \n",
    "    #予測値の計算\n",
    "    def forward(self, x):\n",
    "        return self.a*x\n",
    "        \n",
    "model = LinearRegression()\n",
    "\n",
    "model.state_dict()  #パラメータと現在の値の辞書"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af33a6-df3e-4ca2-a05a-7367a3a9682f",
   "metadata": {},
   "source": [
    "### 訓練\n",
    "\n",
    "損出関数を計算する関数 loss_fn を最小2乗誤差 nn.MSELoss とし， 最適化は率的勾配降下法 (SGD: Stochastic Fradient Descent) optim.SDG とする．\n",
    "引数はモデルのパラメータ model.parameters() と学習率 lr である．\n",
    "\n",
    "optimizerのstepで勾配降下法の1反復を行い， 反復ごとに zero_grad で勾配を $0$ に初期化する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03461179-c555-4c6f-b8a1-01d6127cd153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1253.9412) OrderedDict([('a', tensor([2.8753]))])\n",
      "tensor(765.4183) OrderedDict([('a', tensor([4.7027]))])\n",
      "tensor(468.2010) OrderedDict([('a', tensor([6.1281]))])\n",
      "tensor(287.3740) OrderedDict([('a', tensor([7.2399]))])\n",
      "tensor(177.3588) OrderedDict([('a', tensor([8.1072]))])\n",
      "tensor(110.4256) OrderedDict([('a', tensor([8.7836]))])\n",
      "tensor(69.7034) OrderedDict([('a', tensor([9.3112]))])\n",
      "tensor(44.9280) OrderedDict([('a', tensor([9.7227]))])\n",
      "tensor(29.8547) OrderedDict([('a', tensor([10.0437]))])\n",
      "tensor(20.6841) OrderedDict([('a', tensor([10.2941]))])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()  #損出関数（最小2乗誤差）\n",
    "optimizer = optim.SGD(model.parameters(), lr)  # 最適化（確率的勾配降下法）を準備； model.parameters()はパラメータを返すジェネレータ\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()           #モデルを訓練モードにする\n",
    "    yhat = model(x)         #forwardメソッドで予測値を計算する\n",
    "    loss = loss_fn(y, yhat) #損出関数\n",
    "    loss.backward()         #誤差逆伝播で勾配を計算\n",
    "    \n",
    "    optimizer.step()        #最適化の１反復\n",
    "    optimizer.zero_grad()   #勾配を０にリセット\n",
    "    \n",
    "    print(loss.data, model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0f677-bc61-4d11-81cf-708c264936d8",
   "metadata": {},
   "source": [
    "## 線形層の追加\n",
    "\n",
    "nn.Linear(入力数, 出力数)で線形層をモデルに追加する． モデルは $y = w_0 + w_1 x$ となる． \n",
    "\n",
    "データの $x,y$ は縦ベクトル（shapeは (5,1)　）になおしておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48061cff-1158-4bcc-a8d7-bb2a3da58510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[-0.9852]])),\n",
       "             ('linear.bias', tensor([0.3170]))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    #コンストラクタ\n",
    "    def __init__(self):\n",
    "        super().__init__() #親クラスのコンストラクタを呼ぶ\n",
    "        self.linear = nn.Linear(1,1, dtype=torch.float32) #1入力・１出力の線形層\n",
    "        #self.linear =nn.Sequential(nn.Linear(1,2), nn.ReLU(), nn.Linear(2,1)) #多層のモデルもSequentialを用いて作れる\n",
    "    #予測値の計算\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "        \n",
    "model = LayerLinearRegression()\n",
    "\n",
    "model.state_dict()  #パラメータと現在の値の辞書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1364f7-b905-49dc-a0a7-e56b1d853a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef152726-3699-4602-b7f1-88a6c72716d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1611.6226) OrderedDict([('linear.weight', tensor([[1.6726]])), ('linear.bias', tensor([1.0458]))])\n",
      "tensor(942.0174) OrderedDict([('linear.weight', tensor([[3.7018]])), ('linear.bias', tensor([1.6005]))])\n",
      "tensor(551.8026) OrderedDict([('linear.weight', tensor([[5.2514]])), ('linear.bias', tensor([2.0224]))])\n",
      "tensor(324.4026) OrderedDict([('linear.weight', tensor([[6.4348]])), ('linear.bias', tensor([2.3428]))])\n",
      "tensor(191.8832) OrderedDict([('linear.weight', tensor([[7.3385]])), ('linear.bias', tensor([2.5859]))])\n",
      "tensor(114.6554) OrderedDict([('linear.weight', tensor([[8.0289]])), ('linear.bias', tensor([2.7699]))])\n",
      "tensor(69.6488) OrderedDict([('linear.weight', tensor([[8.5564]])), ('linear.bias', tensor([2.9087]))])\n",
      "tensor(43.4192) OrderedDict([('linear.weight', tensor([[8.9594]])), ('linear.bias', tensor([3.0132]))])\n",
      "tensor(28.1319) OrderedDict([('linear.weight', tensor([[9.2676]])), ('linear.bias', tensor([3.0914]))])\n",
      "tensor(19.2212) OrderedDict([('linear.weight', tensor([[9.5032]])), ('linear.bias', tensor([3.1495]))])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()  #損出関数（最小2乗誤差）\n",
    "optimizer = optim.SGD(model.parameters(), lr)  # 最適化（確率的勾配降下法）を準備； model.parameters()はパラメータを返すジェネレータ\n",
    "\n",
    "n_epochs=10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()           #モデルを訓練モードにする\n",
    "    yhat = model(x)         #forwardメソッドで予測値を計算する\n",
    "    loss = loss_fn(y, yhat) #損出関数\n",
    "    loss.backward()         #誤差逆伝播で勾配を計算\n",
    "    \n",
    "    optimizer.step()        #最適化の１反復\n",
    "    optimizer.zero_grad()   #勾配を０にリセット\n",
    "    \n",
    "    print(loss.data, model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc8d85-e60f-492d-85af-f6b518769551",
   "metadata": {},
   "source": [
    "## データセットとデータローダー\n",
    "\n",
    "データセットはTensorDatasetで生成されるサプライチェーン． \n",
    "\n",
    "1バッチずつデータを出力するデータローダーは DataLoaderクラスに，データセットを入れることによって生成される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116d38b-633f-4e96-9d10-65738fc8949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([15.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_data = TensorDataset(x,y)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e057b1-1cbe-4402-8749-da8000ac0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.],\n",
      "        [2.]]) tensor([[42.],\n",
      "        [20.]])\n",
      "tensor([[3.],\n",
      "        [5.]]) tensor([[34.],\n",
      "        [58.]])\n",
      "tensor([[1.]]) tensor([[15.]])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size=2, shuffle=True)\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2a1ea-1ad6-4b22-96d4-ecba2c49e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4948) OrderedDict([('linear.weight', tensor([[9.5969]])), ('linear.bias', tensor([3.1964]))])\n",
      "tensor(26.1062) OrderedDict([('linear.weight', tensor([[9.8901]])), ('linear.bias', tensor([3.2406]))])\n",
      "tensor(0.6415) OrderedDict([('linear.weight', tensor([[9.8260]])), ('linear.bias', tensor([3.2246]))])\n",
      "tensor(2.0398) OrderedDict([('linear.weight', tensor([[9.8243]])), ('linear.bias', tensor([3.2388]))])\n",
      "tensor(4.9985) OrderedDict([('linear.weight', tensor([[9.8052]])), ('linear.bias', tensor([3.2228]))])\n",
      "tensor(33.0736) OrderedDict([('linear.weight', tensor([[10.3803]])), ('linear.bias', tensor([3.3379]))])\n",
      "tensor(12.2090) OrderedDict([('linear.weight', tensor([[10.4364]])), ('linear.bias', tensor([3.3245]))])\n",
      "tensor(0.9685) OrderedDict([('linear.weight', tensor([[10.4298]])), ('linear.bias', tensor([3.3305]))])\n",
      "tensor(9.3001) OrderedDict([('linear.weight', tensor([[10.1858]])), ('linear.bias', tensor([3.2695]))])\n",
      "tensor(8.4185) OrderedDict([('linear.weight', tensor([[10.3913]])), ('linear.bias', tensor([3.3230]))])\n",
      "tensor(8.5516) OrderedDict([('linear.weight', tensor([[10.2943]])), ('linear.bias', tensor([3.2770]))])\n",
      "tensor(6.0230) OrderedDict([('linear.weight', tensor([[10.0980]])), ('linear.bias', tensor([3.2279]))])\n",
      "tensor(5.9756) OrderedDict([('linear.weight', tensor([[10.0438]])), ('linear.bias', tensor([3.1984]))])\n",
      "tensor(2.4884) OrderedDict([('linear.weight', tensor([[10.0065]])), ('linear.bias', tensor([3.2023]))])\n",
      "tensor(22.7092) OrderedDict([('linear.weight', tensor([[10.4830]])), ('linear.bias', tensor([3.2976]))])\n",
      "tensor(2.8948) OrderedDict([('linear.weight', tensor([[10.5750]])), ('linear.bias', tensor([3.3130]))])\n",
      "tensor(16.4854) OrderedDict([('linear.weight', tensor([[10.3412]])), ('linear.bias', tensor([3.2322]))])\n",
      "tensor(2.0351) OrderedDict([('linear.weight', tensor([[10.3697]])), ('linear.bias', tensor([3.2608]))])\n",
      "tensor(8.0694) OrderedDict([('linear.weight', tensor([[10.2786]])), ('linear.bias', tensor([3.2171]))])\n",
      "tensor(6.8768) OrderedDict([('linear.weight', tensor([[10.4632]])), ('linear.bias', tensor([3.2660]))])\n",
      "tensor(9.7260) OrderedDict([('linear.weight', tensor([[10.2137]])), ('linear.bias', tensor([3.2036]))])\n",
      "tensor(1.2645) OrderedDict([('linear.weight', tensor([[10.2342]])), ('linear.bias', tensor([3.2210]))])\n",
      "tensor(8.8373) OrderedDict([('linear.weight', tensor([[10.3283]])), ('linear.bias', tensor([3.2355]))])\n",
      "tensor(15.1480) OrderedDict([('linear.weight', tensor([[10.1726]])), ('linear.bias', tensor([3.1577]))])\n",
      "tensor(14.0528) OrderedDict([('linear.weight', tensor([[10.3015]])), ('linear.bias', tensor([3.1624]))])\n",
      "tensor(1.1820) OrderedDict([('linear.weight', tensor([[10.3148]])), ('linear.bias', tensor([3.1771]))])\n",
      "tensor(5.9366) OrderedDict([('linear.weight', tensor([[10.1199]])), ('linear.bias', tensor([3.1284]))])\n",
      "tensor(7.2067) OrderedDict([('linear.weight', tensor([[10.0701]])), ('linear.bias', tensor([3.1122]))])\n",
      "tensor(10.5234) OrderedDict([('linear.weight', tensor([[10.3173]])), ('linear.bias', tensor([3.1644]))])\n",
      "tensor(5.9218) OrderedDict([('linear.weight', tensor([[10.1226]])), ('linear.bias', tensor([3.1157]))])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()  #損出関数（最小2乗誤差）\n",
    "optimizer = optim.SGD(model.parameters(), lr)  # 最適化（確率的勾配降下法）を準備； model.parameters()はパラメータを返すジェネレータ\n",
    "\n",
    "n_epochs=10\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        model.train()           #モデルを訓練モードにする\n",
    "        yhat = model(x_batch)         #forwardメソッドで予測値を計算する\n",
    "        loss = loss_fn(y_batch, yhat) #損出関数\n",
    "        loss.backward()         #誤差逆伝播で勾配を計算\n",
    "\n",
    "        optimizer.step()        #最適化の１反復\n",
    "        optimizer.zero_grad()   #勾配を０にリセット\n",
    "\n",
    "        print(loss.data, model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bf83b-e06b-41bd-92b6-51c8be6e19b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
