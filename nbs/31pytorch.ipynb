{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2a0b38-0459-4b4b-90d9-1187d71871a9",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorchによる深層学習\n",
    "\n",
    "> 深層学習パッケージ Pytorch の基本的な使用法を紹介する． "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9283cef-a71d-4043-9468-58585a455bc6",
   "metadata": {},
   "source": [
    "## 簡単な計算\n",
    "\n",
    "PyTorchパッケージを用いて $N$ 行 $D$ 列のランダムな行列を作り， 簡単な計算をする． 最後に， 計算された $c = \\sum x*y + z$ を $x$ に対する勾配（微分値）をbackwardで計算する． \n",
    "\n",
    "randでランダムな行列をを生成する際に， 引数 requires_grad を True に設定しておくことによって， 勾配が計算される．　\n",
    "\n",
    "変数に対しては，　dataで値が， 勾配を計算するように指定した変数に対しては，gradで勾配が得られる．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93456b5d-a2c6-4b13-a009-aca6e86b0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e94f8c-b4e5-4d14-8018-2bd4964182b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.7942)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D = 3,4 #3行4列のランダムなテンソルを生成し， 勾配を計算する\n",
    "x = torch.rand( (N, D), requires_grad=True)\n",
    "y = torch.rand( (N, D), requires_grad=True)\n",
    "z = torch.rand( (N, D), requires_grad=True)\n",
    "\n",
    "a = x*y\n",
    "b = a+z\n",
    "c = torch.sum(b)\n",
    "\n",
    "c.backward()\n",
    "\n",
    "c.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbbffd-41ed-40fa-bd61-0a65f0f22cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4322, 0.4083, 0.5727, 0.7827],\n",
      "        [0.6399, 0.5336, 0.7069, 0.9943],\n",
      "        [0.9654, 0.9505, 0.1478, 0.4963]])\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d420d5-935a-45c3-96fa-189ff99ad4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5993, 0.3683, 0.9010, 0.6827],\n",
      "        [0.3640, 0.7956, 0.9216, 0.3772],\n",
      "        [0.9165, 0.4953, 0.0958, 0.9175]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448346ce-09b4-41ae-8fe5-d90caa702061",
   "metadata": {},
   "source": [
    "##  最小2乗法\n",
    "\n",
    "$y = a x$ のパラメータ $a$ の最適化を最小2乗法によって行う．\n",
    "\n",
    "from_numpyでNumPyの配列をPyTorchのテンソルに変換できる．　\n",
    "\n",
    "パラメータ $a$ をランダムに設定し，勾配を計算するように指示する．\n",
    "\n",
    "予測値 yhat を計算した後で， 損出関数 loss を誤差の2乗平均として計算し， backward で勾配を計算する．\n",
    "\n",
    "勾配の逆方向に学習率 lr (learning rate) だけ移動させたものを新しい $a$ とし， それをn_epochs回繰り返す．\n",
    "\n",
    "ここで，反復ごとに勾配を $0$ に初期化する a.grad.zero_ を呼び出すことに注意されたい．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13ef81-0530-448b-8e45-51e410bb7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd314f-106b-4344-b3eb-566e3f76b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1361.3071) tensor([2.5252])\n",
      "tensor(830.7396) tensor([4.4297])\n",
      "tensor(507.9425) tensor([5.9151])\n",
      "tensor(311.5527) tensor([7.0738])\n",
      "tensor(192.0692) tensor([7.9776])\n",
      "tensor(119.3753) tensor([8.6825])\n",
      "tensor(75.1485) tensor([9.2324])\n",
      "tensor(48.2408) tensor([9.6612])\n",
      "tensor(31.8702) tensor([9.9958])\n",
      "tensor(21.9103) tensor([10.2567])\n"
     ]
    }
   ],
   "source": [
    "x_numpy = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float32)\n",
    "y_numpy = np.array([15.0, 20.0, 34.0, 42.0, 58.0], dtype=np.float32)\n",
    "\n",
    "x = torch.from_numpy(x_numpy)\n",
    "y = torch.from_numpy(y_numpy)\n",
    "\n",
    "a = torch.rand(1, requires_grad=True) #スカラーを定義\n",
    "\n",
    "n_epochs = 10  #反復回数（エポック数）\n",
    "lr = 0.01      #学習率\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a*x \n",
    "    error = y - yhat\n",
    "    loss =(error**2).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= lr*a.grad\n",
    "\n",
    "    print(loss.data, a.data)\n",
    "    \n",
    "    a.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722f577-9694-4374-99e1-efe874d01bbf",
   "metadata": {},
   "source": [
    "## 線形回帰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f5419-e5cf-4281-85cd-a4b8696344c8",
   "metadata": {},
   "source": [
    "### クラス\n",
    "\n",
    "nn.Moduleクラスから派生させて線形回帰を行うクラス LinearRegression を作る．\n",
    "\n",
    "コンストラクタ __init__ で親クラスを呼び出した後でパラメータ $a$ を定義する．\n",
    "\n",
    "与えられた $x$ に対して予測値 $y=ax$ を計算するための関数 forward を定義する．\n",
    "\n",
    "モデルのインスタンス model のstate_dicメソッドでパラメータ $a$ の現在の値の辞書を得ることができる． "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dcd19-c470-441d-b944-92e94f364212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', tensor([0.0902]))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    #コンストラクタ\n",
    "    def __init__(self):\n",
    "        super().__init__() #親クラスのコンストラクタを呼ぶ\n",
    "        self.a = nn.Parameter(torch.rand(1, requires_grad=True)) #モデルのパラメータを準備\n",
    "        \n",
    "    #予測値の計算\n",
    "    def forward(self, x):\n",
    "        return self.a*x\n",
    "        \n",
    "model = LinearRegression()\n",
    "\n",
    "model.state_dict()  #パラメータと現在の値の辞書"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af33a6-df3e-4ca2-a05a-7367a3a9682f",
   "metadata": {},
   "source": [
    "### 訓練\n",
    "\n",
    "損出関数を計算する関数 loss_fn を最小2乗誤差 nn.MSELoss とし， 最適化は率的勾配降下法 (SGD: Stochastic Fradient Descent) optim.SDG とする．\n",
    "引数はモデルのパラメータ model.parameters() と学習率 lr である．\n",
    "\n",
    "optimizerのstepで勾配降下法の1反復を行い， 反復ごとに zero_grad で勾配を $0$ に初期化する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03461179-c555-4c6f-b8a1-01d6127cd153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.8396) OrderedDict([('a', tensor([10.4606]))])\n",
      "tensor(12.1573) OrderedDict([('a', tensor([10.6193]))])\n",
      "tensor(9.9170) OrderedDict([('a', tensor([10.7431]))])\n",
      "tensor(8.5540) OrderedDict([('a', tensor([10.8396]))])\n",
      "tensor(7.7247) OrderedDict([('a', tensor([10.9149]))])\n",
      "tensor(7.2202) OrderedDict([('a', tensor([10.9736]))])\n",
      "tensor(6.9132) OrderedDict([('a', tensor([11.0194]))])\n",
      "tensor(6.7265) OrderedDict([('a', tensor([11.0551]))])\n",
      "tensor(6.6129) OrderedDict([('a', tensor([11.0830]))])\n",
      "tensor(6.5438) OrderedDict([('a', tensor([11.1047]))])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()  #損出関数（最小2乗誤差）\n",
    "optimizer = optim.SGD(model.parameters(), lr)  # 最適化（確率的勾配降下法）を準備； model.parameters()はパラメータを返すジェネレータ\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()           #モデルを訓練モードにする\n",
    "    yhat = model(x)         #forwardメソッドで予測値を計算する\n",
    "    loss = loss_fn(y, yhat) #損出関数\n",
    "    loss.backward()         #誤差逆伝播で勾配を計算\n",
    "    \n",
    "    optimizer.step()        #最適化の１反復\n",
    "    optimizer.zero_grad()   #勾配を０にリセット\n",
    "    \n",
    "    print(loss.data, model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0f677-bc61-4d11-81cf-708c264936d8",
   "metadata": {},
   "source": [
    "## 線形層の追加\n",
    "\n",
    "nn.Linear(入力数, 出力数)で線形層をモデルに追加する． モデルは $y = w_0 + w_1 x$ となる． \n",
    "\n",
    "データの $x,y$ は縦ベクトル（shapeは (5,1)　）になおしておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48061cff-1158-4bcc-a8d7-bb2a3da58510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[-0.2438]])),\n",
       "             ('linear.bias', tensor([-0.6258]))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    #コンストラクタ\n",
    "    def __init__(self):\n",
    "        super().__init__() #親クラスのコンストラクタを呼ぶ\n",
    "        self.linear = nn.Linear(1,1, dtype=torch.float32) #1入力・１出力の線形層\n",
    "        #self.linear =nn.Sequential(nn.Linear(1,2), nn.ReLU(), nn.Linear(2,1)) #多層のモデルもSequentialを用いて作れる\n",
    "    #予測値の計算\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "        \n",
    "model = LayerLinearRegression()\n",
    "\n",
    "model.state_dict()  #パラメータと現在の値の辞書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1364f7-b905-49dc-a0a7-e56b1d853a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef152726-3699-4602-b7f1-88a6c72716d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1486.0369) OrderedDict([('linear.weight', tensor([[2.3074]])), ('linear.bias', tensor([0.0773]))])\n",
      "tensor(868.5944) OrderedDict([('linear.weight', tensor([[4.2551]])), ('linear.bias', tensor([0.6133]))])\n",
      "tensor(508.7790) OrderedDict([('linear.weight', tensor([[5.7422]])), ('linear.bias', tensor([1.0218]))])\n",
      "tensor(299.0960) OrderedDict([('linear.weight', tensor([[6.8776]])), ('linear.bias', tensor([1.3328]))])\n",
      "tensor(176.9026) OrderedDict([('linear.weight', tensor([[7.7446]])), ('linear.bias', tensor([1.5695]))])\n",
      "tensor(105.6939) OrderedDict([('linear.weight', tensor([[8.4066]])), ('linear.bias', tensor([1.7494]))])\n",
      "tensor(64.1965) OrderedDict([('linear.weight', tensor([[8.9122]])), ('linear.bias', tensor([1.8860]))])\n",
      "tensor(40.0135) OrderedDict([('linear.weight', tensor([[9.2983]])), ('linear.bias', tensor([1.9896]))])\n",
      "tensor(25.9204) OrderedDict([('linear.weight', tensor([[9.5933]])), ('linear.bias', tensor([2.0679]))])\n",
      "tensor(17.7072) OrderedDict([('linear.weight', tensor([[9.8187]])), ('linear.bias', tensor([2.1269]))])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()  #損出関数（最小2乗誤差）\n",
    "optimizer = optim.SGD(model.parameters(), lr)  # 最適化（確率的勾配降下法）を準備； model.parameters()はパラメータを返すジェネレータ\n",
    "\n",
    "n_epochs=10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()           #モデルを訓練モードにする\n",
    "    yhat = model(x)         #forwardメソッドで予測値を計算する\n",
    "    loss = loss_fn(y, yhat) #損出関数\n",
    "    loss.backward()         #誤差逆伝播で勾配を計算\n",
    "    \n",
    "    optimizer.step()        #最適化の１反復\n",
    "    optimizer.zero_grad()   #勾配を０にリセット\n",
    "    \n",
    "    print(loss.data, model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc8d85-e60f-492d-85af-f6b518769551",
   "metadata": {},
   "source": [
    "## データセットとデータローダー\n",
    "\n",
    "データセットはTensorDatasetで生成されるサプライチェーン． \n",
    "\n",
    "1バッチずつデータを出力するデータローダーは DataLoaderクラスに，データセットを入れることによって生成される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116d38b-633f-4e96-9d10-65738fc8949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([15.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_data = TensorDataset(x,y)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e057b1-1cbe-4402-8749-da8000ac0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.],\n",
      "        [2.]]) tensor([[34.],\n",
      "        [20.]])\n",
      "tensor([[4.],\n",
      "        [1.]]) tensor([[42.],\n",
      "        [15.]])\n",
      "tensor([[5.]]) tensor([[58.]])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size=2, shuffle=True)\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2a1ea-1ad6-4b22-96d4-ecba2c49e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4772) OrderedDict([('linear.weight', tensor([[9.8559]])), ('linear.bias', tensor([2.1335]))])\n",
      "tensor(26.2251) OrderedDict([('linear.weight', tensor([[10.2154]])), ('linear.bias', tensor([2.2294]))])\n",
      "tensor(1.1903) OrderedDict([('linear.weight', tensor([[10.1281]])), ('linear.bias', tensor([2.2076]))])\n",
      "tensor(4.5405) OrderedDict([('linear.weight', tensor([[10.1970]])), ('linear.bias', tensor([2.2483]))])\n",
      "tensor(4.0279) OrderedDict([('linear.weight', tensor([[10.1027]])), ('linear.bias', tensor([2.2115]))])\n",
      "tensor(27.8253) OrderedDict([('linear.weight', tensor([[10.6302]])), ('linear.bias', tensor([2.3170]))])\n",
      "tensor(9.6045) OrderedDict([('linear.weight', tensor([[10.6852]])), ('linear.bias', tensor([2.3066]))])\n",
      "tensor(2.0820) OrderedDict([('linear.weight', tensor([[10.6945]])), ('linear.bias', tensor([2.3230]))])\n",
      "tensor(9.6154) OrderedDict([('linear.weight', tensor([[10.4464]])), ('linear.bias', tensor([2.2610]))])\n",
      "tensor(5.0532) OrderedDict([('linear.weight', tensor([[10.3953]])), ('linear.bias', tensor([2.2335]))])\n",
      "tensor(8.8286) OrderedDict([('linear.weight', tensor([[10.5122]])), ('linear.bias', tensor([2.2532]))])\n",
      "tensor(4.9932) OrderedDict([('linear.weight', tensor([[10.5569]])), ('linear.bias', tensor([2.2979]))])\n",
      "tensor(3.1897) OrderedDict([('linear.weight', tensor([[10.4568]])), ('linear.bias', tensor([2.2730]))])\n",
      "tensor(8.5036) OrderedDict([('linear.weight', tensor([[10.6517]])), ('linear.bias', tensor([2.3301]))])\n",
      "tensor(13.2021) OrderedDict([('linear.weight', tensor([[10.5063]])), ('linear.bias', tensor([2.2575]))])\n",
      "tensor(2.6305) OrderedDict([('linear.weight', tensor([[10.4217]])), ('linear.bias', tensor([2.2369]))])\n",
      "tensor(11.4219) OrderedDict([('linear.weight', tensor([[10.5428]])), ('linear.bias', tensor([2.2426]))])\n",
      "tensor(4.9042) OrderedDict([('linear.weight', tensor([[10.5871]])), ('linear.bias', tensor([2.2869]))])\n",
      "tensor(3.8582) OrderedDict([('linear.weight', tensor([[10.7246]])), ('linear.bias', tensor([2.3142]))])\n",
      "tensor(7.0831) OrderedDict([('linear.weight', tensor([[10.6157]])), ('linear.bias', tensor([2.3017]))])\n",
      "tensor(12.4823) OrderedDict([('linear.weight', tensor([[10.4744]])), ('linear.bias', tensor([2.2310]))])\n",
      "tensor(8.0356) OrderedDict([('linear.weight', tensor([[10.5591]])), ('linear.bias', tensor([2.2437]))])\n",
      "tensor(2.4170) OrderedDict([('linear.weight', tensor([[10.5834]])), ('linear.bias', tensor([2.2665]))])\n",
      "tensor(11.7876) OrderedDict([('linear.weight', tensor([[10.4461]])), ('linear.bias', tensor([2.1978]))])\n",
      "tensor(6.7385) OrderedDict([('linear.weight', tensor([[10.3050]])), ('linear.bias', tensor([2.1471]))])\n",
      "tensor(12.6113) OrderedDict([('linear.weight', tensor([[10.5469]])), ('linear.bias', tensor([2.2158]))])\n",
      "tensor(0.0206) OrderedDict([('linear.weight', tensor([[10.5555]])), ('linear.bias', tensor([2.2187]))])\n",
      "tensor(2.4837) OrderedDict([('linear.weight', tensor([[10.5812]])), ('linear.bias', tensor([2.2421]))])\n",
      "tensor(7.3612) OrderedDict([('linear.weight', tensor([[10.6211]])), ('linear.bias', tensor([2.2450]))])\n",
      "tensor(12.1605) OrderedDict([('linear.weight', tensor([[10.4816]])), ('linear.bias', tensor([2.1752]))])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()  #損出関数（最小2乗誤差）\n",
    "optimizer = optim.SGD(model.parameters(), lr)  # 最適化（確率的勾配降下法）を準備； model.parameters()はパラメータを返すジェネレータ\n",
    "\n",
    "n_epochs=10\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        model.train()           #モデルを訓練モードにする\n",
    "        yhat = model(x_batch)         #forwardメソッドで予測値を計算する\n",
    "        loss = loss_fn(y_batch, yhat) #損出関数\n",
    "        loss.backward()         #誤差逆伝播で勾配を計算\n",
    "\n",
    "        optimizer.step()        #最適化の１反復\n",
    "        optimizer.zero_grad()   #勾配を０にリセット\n",
    "\n",
    "        print(loss.data, model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bf83b-e06b-41bd-92b6-51c8be6e19b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
